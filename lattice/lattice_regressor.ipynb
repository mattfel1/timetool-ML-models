{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU HAVE TO SOURCE THE lattice/lattice/bin/activate virtual env for this, not conda!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in ./lattice/lib/python3.6/site-packages (0.15.1)\n",
      "Requirement already satisfied: six>=1.0.0 in /usr/lib/python3/dist-packages (from pyarrow) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.16.2)\n",
      "['/local/ssd/home/mattfel/slac/timetool-ML-models/lattice/lattice/lib/python3.6/site-packages/pyarrow']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install pyarrow\n",
    "\n",
    "import pandas as pd\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import tensorflow_lattice as tfl\n",
    "import timeit\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import sys\n",
    "sys.path.append(\"./lattice/lib/python3.6/site-packages\")\n",
    "import pyarrow\n",
    "print(pyarrow.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 70m; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 70m; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'train' is defined twice. First from /local/ssd/home/mattfel/slac/timetool-ML-models/lattice/lattice/lib/python3.6/site-packages/ipykernel_launcher.py, Second from /local/ssd/home/mattfel/slac/timetool-ML-models/lattice/lattice/lib/python3.6/site-packages/ipykernel_launcher.py.  Description from first occurrence: Path to test file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3581ccabf9c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# test and train data set paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mproject_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Path to test file.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Path to train file.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[0;34m(name, default, help, flag_values, **args)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'train' is defined twice. First from /local/ssd/home/mattfel/slac/timetool-ML-models/lattice/lattice/lib/python3.6/site-packages/ipykernel_launcher.py, Second from /local/ssd/home/mattfel/slac/timetool-ML-models/lattice/lattice/lib/python3.6/site-packages/ipykernel_launcher.py.  Description from first occurrence: Path to test file."
     ]
    }
   ],
   "source": [
    "\n",
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Train example: python train.py --create_quantiles --output_dir=evaluations/output --quantiles_dir=evaluations/ --target=loadCycs --learning_rate=1 --train_epochs=100\n",
    "# Test example: python train.py --output_dir=evaluations/output --quantiles_dir=evaluations/ --target=loadCycs\n",
    "\n",
    "# test and train data set paths\n",
    "project_dir = \"./data/\"\n",
    "flags.DEFINE_string(\"train\", project_dir + \"train\", \"Path to test file.\")\n",
    "flags.DEFINE_string(\"test\", project_dir + \"test\", \"Path to train file.\")\n",
    "\n",
    "# Run mode of the program.\n",
    "flags.DEFINE_string(\n",
    "    \"run\", \"train\", \"One of 'train', 'evaluate', 'time' or 'save', train will \"\n",
    "    \"train on training data and also optionally evaluate; evaluate will \"\n",
    "    \"evaluate train and test data; save saves the trained model so far \"\n",
    "    \"so it can be used by TensorFlow Serving.\")\n",
    "\n",
    "# Model flags.\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"model_type\", \"calibrated_lattice\",\n",
    "    \"Types defined in this example: calibrated_linear, calibrated_lattice, \"\n",
    "    \" calibrated_lattice, calibrated_etl, calibrated_dnn\")\n",
    "flags.DEFINE_string(\"hparams\", None,\n",
    "                    \"Model hyperparameters, see hyper-parameters in Tensorflow \"\n",
    "                    \"Lattice documentation. Example: --hparams=learning_rate=\"\n",
    "                    \"0.1,lattice_size=2,num_keypoints=100\")\n",
    "\n",
    "\n",
    "# Calibration quantiles flags.\n",
    "QUANTILES_DIR='./quantiles'\n",
    "OUTPUT_DIR='./output'\n",
    "EPOCHS=50\n",
    "BATCH_SIZE=100\n",
    "L2_LAPLACIAN = 5.0e-4\n",
    "L2_TORSION = 1.0e-4\n",
    "OPTIMIZER = tf.train.AdamOptimizer\n",
    "NUM_LATTICES = 1\n",
    "NUM_KEYPOINTS = 8\n",
    "LATTICE_SIZE = 4\n",
    "LEARNING_RATE = 20\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"train_evaluate_on_train\", True,\n",
    "    \"If set, every 1/10th of the train_epochs runs an evaluation on the \"\n",
    "    \"full train data.\")\n",
    "flags.DEFINE_bool(\n",
    "    \"train_evaluate_on_test\", False,\n",
    "    \"If set, every 1/10th of the train_epochs runs an evaluation on the \"\n",
    "    \"full test data.\")\n",
    "\n",
    "\n",
    "# Extra flags: \n",
    "#   label defaults to normal (i.e. a non-malicious packet)\n",
    "#   rtl_seed is a random seed to intialize lattices   \n",
    "flags.DEFINE_string(\"target\", \"delay\", \"\")\n",
    "flags.DEFINE_integer(\"rtl_seed\", 337893, \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['fileId', 'row','rising_idx','falling_idx','volume','rising_weight',\n",
    "                'falling_weight', 'first_val', 'last_val', 'delay']\n",
    "def filterBad(dataset):\n",
    "    initial_len = len(dataset)\n",
    "    dataset = dataset[(dataset['rising_idx'] != 0)]\n",
    "    dataset = dataset[(dataset['falling_idx'] != 0)]\n",
    "    dataset = dataset[(dataset['volume'] > 500)]\n",
    "#     dataset = dataset[(dataset['first_val'] < 30)]\n",
    "#     dataset = dataset[(dataset['last_val'] < 30)]\n",
    "    final_len = len(dataset)\n",
    "    print('Rejected %d points (%f%%)' % ((initial_len-final_len), (initial_len-final_len)/initial_len))\n",
    "    return dataset\n",
    "\n",
    "raw_dataset = pd.read_csv('../preprocessing/processed.csv', names=column_names,\n",
    "                      na_values = \"?\", comment='\\t',\n",
    "                      sep=\",\", skipinitialspace=True)\n",
    "raw_dataset = filterBad(raw_dataset)\n",
    "# visualize_dataset = raw_dataset.sample(frac=0.01)\n",
    "# sns.pairplot(visualize_dataset[column_names], diag_kind=\"kde\")\n",
    "\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()\n",
    "\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset.drop(columns=['fileId', 'row'])\n",
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "# dataset.tail()\n",
    "# train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "# test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "# train_stats = train_dataset.describe()\n",
    "# train_stats.pop(\"delay\")\n",
    "# train_stats = train_stats.transpose()\n",
    "# train_stats\n",
    "# train_labels = train_dataset.pop('delay')\n",
    "# test_labels = test_dataset.pop('delay')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = dataset.iloc[:,:-1],dataset.iloc[:,-1]\n",
    "\n",
    "train_dataset, test_dataset, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "  x=train_dataset,\n",
    "  y=train_labels,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False,\n",
    "  num_epochs=EPOCHS,\n",
    "  num_threads=1)\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "  x=test_dataset,\n",
    "  y=test_labels,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False,\n",
    "  num_epochs=1,\n",
    "  num_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainable_features = ['rising_idx','falling_idx','volume','rising_weight',\n",
    "                'falling_weight', 'first_val', 'last_val']\n",
    "\n",
    "target = 'delay'\n",
    "\n",
    "# Set up test and train functions with relevant parameters\n",
    "def get_test_input_fn(batch_size, num_epochs, shuffle):\n",
    "#   return get_input_fn(FLAGS.test, batch_size, num_epochs, shuffle)\n",
    "    return test_input_fn\n",
    "\n",
    "def get_train_input_fn(batch_size, num_epochs, shuffle):\n",
    "#   return get_input_fn(FLAGS.train, batch_size, num_epochs, shuffle)\n",
    "    return train_input_fn\n",
    "\n",
    "\n",
    "\n",
    "# # Copy of data read from train/test files: keep copy to avoid re-reading\n",
    "# # it at every training/evaluation loop.\n",
    "# _df_data = {}\n",
    "# _df_data_labels = {}\n",
    "\n",
    "\n",
    "\n",
    "# # Load data from files\n",
    "# def get_input_fn(file_path, batch_size, num_epochs, shuffle):\n",
    "#   \"\"\"Returns an input_fn closure for given parameters.\"\"\"\n",
    "#   if file_path not in _df_data:\n",
    "\n",
    "#     # Load data in column_names format\n",
    "#     print(\"Loading data from \", file_path)\n",
    "#     _df_data[file_path] = pd.read_csv(\n",
    "# #         tf.gfile.Open(file_path),\n",
    "#         tf.gfile.Open('../preprocessing/processed_baby.csv'),\n",
    "#         sep=',',\n",
    "#         names=column_names,\n",
    "#         skipinitialspace=True,\n",
    "#         engine=\"python\",\n",
    "#         dtype=np.float32)\n",
    "# #     _df_data[file_path].drop(columns=list(targets - {FLAGS.target}), errors=\"ignore\", inplace=True)\n",
    "\n",
    "#     # Mark labels\n",
    "#     _df_data_labels[file_path] = (_df_data[file_path][FLAGS.target])\n",
    "\n",
    "#   # set up data with labels\n",
    "#   return tf.estimator.inputs.pandas_input_fn(\n",
    "#       x=_df_data[file_path],\n",
    "#       y=_df_data_labels[file_path],\n",
    "#       batch_size=batch_size,\n",
    "#       shuffle=shuffle,\n",
    "#       num_epochs=num_epochs,\n",
    "#       num_threads=1)\n",
    "\n",
    "\n",
    "# Create feature columns with correct categorical vocabularies\n",
    "def create_feature_columns():\n",
    "  return [tf.feature_column.numeric_column(feat) for feat in trainable_features if feat != target]\n",
    "\n",
    "# Create quantiles based on batch size\n",
    "def create_quantiles(quantiles_dir):\n",
    "  \"\"\"Creates quantiles directory if it doesn't yet exist.\"\"\"\n",
    "  batch_size = 100\n",
    "  print(\"creating quantiles...\")\n",
    "  input_fn = get_train_input_fn(\n",
    "      batch_size=batch_size, num_epochs=1, shuffle=False)\n",
    "  # Reads until input is exhausted, 50 at a time.\n",
    "  tfl.save_quantiles_for_keypoints(\n",
    "      input_fn=input_fn,\n",
    "      save_dir=quantiles_dir,\n",
    "      feature_columns=create_feature_columns(),\n",
    "      num_steps=None)\n",
    "\n",
    "\n",
    "# Print hyper parameters\n",
    "def _pprint_hparams(hparams):\n",
    "  \"\"\"Pretty-print hparams.\"\"\"\n",
    "  print(\"* hparams=[\")\n",
    "  for (key, value) in sorted(six.iteritems(hparams.values())):\n",
    "    print(\"\\t{}={}\".format(key, value))\n",
    "  print(\"]\")\n",
    "\n",
    "\n",
    "# # Create network\n",
    "# def create_network(model_fn1, model_fn2, feature_columns, config, quantiles_dir):\n",
    "#   \"\"\"Creates a calibrated Lattice estimator.\"\"\"\n",
    "#   \"\"\" Lattice 0 (Competitors) \"\"\"\n",
    "#   feature_names = [fc.name for fc in feature_columns if fc not in payloads]\n",
    "#   hparams = tfl.CalibratedLatticeHParams(\n",
    "#       feature_names=feature_names,\n",
    "#       learning_rate=FLAGS.learning_rate,\n",
    "#       lattice_l2_laplacian_reg=5.0e-4,\n",
    "#       # lattice_l2_torsion_reg=1.0e-4,\n",
    "#       interpolation_type='hypercube',\n",
    "#       num_keypoints=8,\n",
    "#       lattice_size=5,\n",
    "#       lattice_rank=len(feature_names),\n",
    "#       num_lattices=1,\n",
    "#       optimizer=tf.train.AdamOptimizer)\n",
    "#   hparams.parse(FLAGS.hparams)\n",
    "#   _pprint_hparams(hparams)\n",
    "#   (layer00, _, _, _) = tfl.calibrated_lattice_regressor(\n",
    "#     model_fn1,\n",
    "#     feature_columns=feature_columns,\n",
    "#     model_dir=config.model_dir,\n",
    "#     config=config,\n",
    "#     quantiles_dir=quantiles_dir,\n",
    "#     keypoints_initializers_fn=None,\n",
    "#     optimizer=tf.train.AdamOptimizer,\n",
    "#     hparams=hparams\n",
    "#   )\n",
    "\n",
    "#   \"\"\" Lattice 1 (Transfer info) \"\"\"\n",
    "#   feature_names = [fc.name for fc in feature_columns if fc not in competitors]\n",
    "#   hparams = tfl.CalibratedLatticeHParams(\n",
    "#       feature_names=feature_names,\n",
    "#       learning_rate=FLAGS.learning_rate,\n",
    "#       lattice_l2_laplacian_reg=5.0e-4,\n",
    "#       # lattice_l2_torsion_reg=1.0e-4,\n",
    "#       interpolation_type='hypercube',\n",
    "#       num_keypoints=8,\n",
    "#       lattice_size=16,\n",
    "#       lattice_rank=len(feature_names),\n",
    "#       num_lattices=1,\n",
    "#       optimizer=tf.train.AdamOptimizer)\n",
    "#   hparams.parse(FLAGS.hparams)\n",
    "#   _pprint_hparams(hparams)\n",
    "#   (layer01, _, _, _) = tfl.calibrated_lattice_regressor(\n",
    "#     model_fn2,\n",
    "#     feature_columns=feature_columns,\n",
    "#     model_dir=config.model_dir,\n",
    "#     config=config,\n",
    "#     quantiles_dir=quantiles_dir,\n",
    "#     keypoints_initializers_fn=None,\n",
    "#     optimizer=tf.train.AdamOptimizer,\n",
    "#     hparams=hparams\n",
    "#   )\n",
    "\n",
    "#   \"\"\" Lattice 2 (combine lattices 0 and 1) \"\"\"\n",
    "#   feature_names = ['lattice0','lattice1']\n",
    "#   hparams = tfl.CalibratedLatticeHParams(\n",
    "#       feature_names=feature_names,\n",
    "#       learning_rate=FLAGS.learning_rate,\n",
    "#       lattice_l2_laplacian_reg=5.0e-4,\n",
    "#       # lattice_l2_torsion_reg=1.0e-4,\n",
    "#       interpolation_type='hypercube',\n",
    "#       num_keypoints=8,\n",
    "#       lattice_size=16,\n",
    "#       lattice_rank=len(feature_names),\n",
    "#       num_lattices=1,\n",
    "#       optimizer=tf.train.AdamOptimizer)\n",
    "#   hparams.parse(FLAGS.hparams)\n",
    "#   _pprint_hparams(hparams)\n",
    "#   layer10 = tfl.calibrated_lattice_regressor(\n",
    "#     feature_columns=feature_columns,\n",
    "#     model_dir=config.model_dir,\n",
    "#     config=config,\n",
    "#     quantiles_dir=quantiles_dir,\n",
    "#     keypoints_initializers_fn=None,\n",
    "#     optimizer=tf.train.AdamOptimizer,\n",
    "#     hparams=hparams\n",
    "#   )\n",
    "\n",
    "#   layer10\n",
    "\n",
    "\n",
    "# Create a set of randomly initialized lattices with calibrator inputs\n",
    "def create_calibrated_lattice(feature_columns, config, quantiles_dir):\n",
    "  \"\"\"Creates a calibrated Lattice estimator.\"\"\"\n",
    "  feature_names = [fc.name for fc in feature_columns]\n",
    "  hparams = tfl.CalibratedLatticeHParams(\n",
    "      feature_names=feature_names,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      lattice_l2_laplacian_reg=L2_LAPLACIAN, \n",
    "      lattice_l2_torsion_reg=L2_TORSION,\n",
    "      interpolation_type='simplex',\n",
    "      num_keypoints=NUM_KEYPOINTS, \n",
    "#       feature__outerIters__monotonicity=1,\n",
    "#       feature__innerIters__monotonicity=1,\n",
    "      # feature__bitsPerCycle__monotonicity=-1,\n",
    "      # feature__innerIters__lattice_size=3,\n",
    "      # feature__outerIters__lattice_size=3,\n",
    "      # monotonicity=frozenset({'outerIters': 1}.items),\n",
    "      lattice_size=LATTICE_SIZE, \n",
    "      lattice_rank=len(feature_columns),\n",
    "      num_lattices=NUM_LATTICES, \n",
    "      optimizer=OPTIMIZER) \n",
    "\n",
    "  # Specific feature parameters.\n",
    "  hparams.parse(FLAGS.hparams)\n",
    "  _pprint_hparams(hparams)\n",
    "\n",
    "  return tfl.calibrated_lattice_regressor(\n",
    "    feature_columns=feature_columns,\n",
    "    model_dir=config.model_dir,\n",
    "    config=config,\n",
    "    quantiles_dir=quantiles_dir,\n",
    "    keypoints_initializers_fn=None,\n",
    "    optimizer=tf.train.AdamOptimizer,\n",
    "    hparams=hparams\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an estimator\n",
    "# TODO - Add other lattice models here\n",
    "def create_estimator(config, quantiles_dir):\n",
    "  \"\"\"Creates estimator for given configuration based on --model_type.\"\"\"\n",
    "  feature_columns = create_feature_columns()\n",
    "  #FLAGS.model_type == \"calibrated_rtl\":\n",
    "  return create_calibrated_lattice(feature_columns, config, quantiles_dir)\n",
    "\n",
    "  #raise ValueError(\"Unknown model_type={}\".format(FLAGS.model_type))\n",
    "\n",
    "\n",
    "\n",
    "# Evaluator that keeps track accuracy and loss\n",
    "def evaluate_on_data(estimator, data):\n",
    "  \"\"\"Evaluates and prints results, set data to FLAGS.test or FLAGS.train.\"\"\"\n",
    "  name = os.path.basename(data)\n",
    "  evaluation = estimator.evaluate(\n",
    "      input_fn=get_input_fn(\n",
    "          file_path=data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          num_epochs=1,\n",
    "          shuffle=False),\n",
    "      name=name)\n",
    "\n",
    "\n",
    "  metrics = [\n",
    "          \"average_loss\"\n",
    "          ]\n",
    "  print(estimator)\n",
    "  print(evaluation)\n",
    "  metric_string = \"\\t\".join(\"{}={:.8f}\".format(metric, evaluation[metric]) for metric in metrics)\n",
    "  print(metric_string)\n",
    "\n",
    "  return evaluation\n",
    "  #print(\"  Evaluation on '{}':\\taccuracy={:.4f}\\taverage_loss={:.4f}\".format(\n",
    "      #name, evaluation[\"accuracy\"], evaluation[\"average_loss\"]))\n",
    "\n",
    "# Training function\n",
    "def train(estimator):\n",
    "  \"\"\"Trains estimator and optionally intermediary evaluations.\"\"\"\n",
    "  if not FLAGS.train_evaluate_on_train and not FLAGS.train_evaluate_on_test:\n",
    "    estimator.train(input_fn=get_train_input_fn(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_epochs=EPOCHS,\n",
    "        shuffle=True))\n",
    "  else:\n",
    "    # Train 1/10th of the epochs requested per loop, but at least 1 per loop.\n",
    "    epochs_trained = 0\n",
    "    loops = 0\n",
    "    while epochs_trained < EPOCHS:\n",
    "      loops += 1\n",
    "      next_epochs_trained = int(loops * EPOCHS / 10.0)\n",
    "      epochs = max(1, next_epochs_trained - epochs_trained)\n",
    "      epochs_trained += epochs\n",
    "      inp = get_train_input_fn(batch_size=BATCH_SIZE, num_epochs=epochs, shuffle=True)\n",
    "      estimator.train(input_fn=inp)\n",
    "      print(\"Trained for {} epochs, total so far {}:\".format(\n",
    "          epochs, epochs_trained))\n",
    "      if FLAGS.train_evaluate_on_train:\n",
    "          evaluate_on_data(estimator, FLAGS.train)\n",
    "      if FLAGS.train_evaluate_on_test:\n",
    "          evaluate_on_data(estimator, FLAGS.test)\n",
    "\n",
    "\n",
    "# Train before testing\n",
    "def evaluate(estimator):\n",
    "  \"\"\"Runs straight evaluation on a currently trained model.\"\"\"\n",
    "  evaluate_on_data(estimator, FLAGS.train)\n",
    "  evaluate_on_data(estimator, FLAGS.test)\n",
    "\n",
    "def timed_evaluate(estimator):\n",
    "\n",
    "  def eval_func():\n",
    "    evaluate_on_data(estimator, FLAGS.test)\n",
    "\n",
    "\n",
    "  \"\"\" Timed evaluation over 50 trials\"\"\"\n",
    "  runtime = timeit.timeit(eval_func, number = 3)*1000000\n",
    "  sperinf = (runtime / float(10000))\n",
    "  print(\"%f us/inference (%fus / %d)\" % (sperinf, runtime, 10000))\n",
    "\n",
    "# Main function that sets up and runs program\n",
    "def main(args):\n",
    "  del args  # Not used.\n",
    "\n",
    "  # Prepare directories.\n",
    "  output_dir = OUTPUT_DIR\n",
    "  if output_dir is None:\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    tf.logging.warning(\"Using temporary folder as model directory: %s\",\n",
    "                       output_dir)\n",
    "  quantiles_dir = QUANTILES_DIR or output_dir\n",
    "\n",
    "  # Create quantiles if required.\n",
    "  if CREATE_QUANTILES:\n",
    "    if FLAGS.run != \"train\":\n",
    "      raise ValueError(\n",
    "          \"Can not create_quantiles for mode --run='{}'\".format(FLAGS.run))\n",
    "    create_quantiles(quantiles_dir)\n",
    "\n",
    "  # Create config and then model.\n",
    "  config = tf.estimator.RunConfig().replace(model_dir=output_dir)\n",
    "  estimator = create_estimator(config, quantiles_dir)\n",
    "\n",
    "  if FLAGS.run == \"train\":\n",
    "    train(estimator)\n",
    "\n",
    "  elif FLAGS.run == \"evaluate\":\n",
    "    evaluate(estimator)\n",
    "    \n",
    "  elif FLAGS.run == \"time\":\n",
    "    timed_evaluate(estimator)\n",
    "\n",
    "  else:\n",
    "    raise ValueError(\"Unknown --run={}\".format(FLAGS.run))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quantiles...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unbound (num_steps=None) materialization of input reached safety size of 1000000000.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6db52918dad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mCREATE_QUANTILES\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantiles created!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0362317976d4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    301\u001b[0m       raise ValueError(\n\u001b[1;32m    302\u001b[0m           \"Can not create_quantiles for mode --run='{}'\".format(FLAGS.run))\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mcreate_quantiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantiles_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m   \u001b[0;31m# Create config and then model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0362317976d4>\u001b[0m in \u001b[0;36mcreate_quantiles\u001b[0;34m(quantiles_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantiles_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mfeature_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_feature_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m       num_steps=None)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_lattice/python/lib/keypoints_initialization.py\u001b[0m in \u001b[0;36msave_quantiles_for_keypoints\u001b[0;34m(input_fn, save_dir, feature_columns, num_steps, override, num_quantiles, dtype)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;31m# Here a dict of feature_name to tensor is in tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_materialize_locally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m   \u001b[0mpercentiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_quantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_lattice/python/lib/keypoints_initialization.py\u001b[0m in \u001b[0;36m_materialize_locally\u001b[0;34m(tensors, num_steps, feed_dict, safety_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m               raise ValueError(\n\u001b[1;32m    114\u001b[0m                   \u001b[0;34m\"Unbound (num_steps=None) materialization of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                   \"input reached safety size of {}\".format(safety_size))\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0msplits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unbound (num_steps=None) materialization of input reached safety size of 1000000000.0"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  FLAGS.run = \"train\"\n",
    "  CREATE_QUANTILES=True\n",
    "  tf.app.run()\n",
    "  print(\"quantiles created!\")\n",
    "    \n",
    "  CREATE_QUANTILES=False\n",
    "  tf.app.run()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, preds)\n",
    "rmae = mean_absolute_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "print(\"MSE, MAE, r2: %f,%f,%f\" % (rmse, rmae, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
