{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterBad(dataset, row_limit = -1):\n",
    "    initial_len = len(dataset)\n",
    "#     dataset = dataset[(dataset['rising_idx'] != 0)]\n",
    "#     dataset = dataset[(dataset['falling_idx'] != 0)]\n",
    "    dataset = dataset[(dataset['falling_idx'] - dataset['rising_idx'] > 100)]\n",
    "    dataset = dataset[(dataset['falling_idx'] - dataset['rising_idx'] < 500)]\n",
    "#     dataset = dataset[(dataset['volume'] > 500)]\n",
    "    dataset = dataset[(dataset['first_val'] < 30) | (dataset['last_val'] < 30)]\n",
    "#     dataset = dataset[(dataset['first_val'] < 30)]\n",
    "#     dataset = dataset[(dataset['last_val'] < 30)]\n",
    "    if (row_limit > 0):\n",
    "        dataset = dataset[(dataset['row'] < row_limit)]\n",
    "    final_len = len(dataset)\n",
    "    print('Rejected %d points (%f%%)' % ((initial_len-final_len), 100.0*(initial_len-final_len)/initial_len))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dropColumns(dataset,cols):\n",
    "    import pandas as pd\n",
    "    dataset = dataset.drop(columns=cols)\n",
    "    return dataset, [x for x in dataset.columns.values.tolist() if x not in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buildDataset(filename):\n",
    "    import pandas as pd\n",
    "    column_names = ['fileId', 'row','rising_idx','falling_idx','volume','rising_weight',\n",
    "                'falling_weight', 'first_val', 'last_val', 'delay']\n",
    "    \n",
    "    # raw_dataset =  pd.read_feather('../preprocessing/processed.feather')\n",
    "#     raw_dataset =  pd.read_feather('/local/ssd/home/mattfel/slac/timetool-ML-models/preprocessing/processed.feather')\n",
    "    raw_dataset =  pd.read_feather('/scratch/mattfel/data-fs/' + filename)\n",
    "\n",
    "    dataset = raw_dataset.copy()\n",
    "    dataset.tail()\n",
    "    \n",
    "#     unmodeledColumns = ['fileId']\n",
    "    unmodeledColumns = []\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.drop(columns=unmodeledColumns)\n",
    "    return dataset, [item for item in column_names if item not in unmodeledColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buildDatasetForLattice():\n",
    "    import pandas as pd\n",
    "    column_names = ['fileId', 'row','rising_idx','falling_idx','volume','rising_weight',\n",
    "                'falling_weight', 'first_val', 'last_val', 'delay']\n",
    "    \n",
    "    # raw_dataset =  pd.read_feather('../preprocessing/processed.feather')\n",
    "#     raw_dataset =  pd.read_feather('/local/ssd/home/mattfel/slac/timetool-ML-models/preprocessing/processed.feather')\n",
    "    raw_dataset =  pd.read_feather('/scratch/mattfel/data-fs/processed_floor.feather')\n",
    "\n",
    "    dataset = raw_dataset.copy()\n",
    "    dataset.tail()\n",
    "    \n",
    "    unmodeledColumns = ['fileId', 'volume', 'rising_weight', 'falling_weight']\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.drop(columns=unmodeledColumns)\n",
    "    return dataset, [item for item in column_names if item not in unmodeledColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(dataset, split):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X, y = dataset.iloc[:,:-1],dataset.iloc[:,-1]\n",
    "    train_dataset, test_dataset, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    return train_dataset, test_dataset, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDatasetLabels(dataset, labels, split):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X, y = dataset, labels\n",
    "    train_dataset, test_dataset, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    return train_dataset, test_dataset, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeDataset(dataset, column_names, percent):\n",
    "    visualize_dataset = raw_dataset.sample(frac=percent)\n",
    "    sns.pairplot(visualize_dataset[column_names], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleVolume(dataset, factor):\n",
    "    dataset['volume'] = dataset['volume'] / factor\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normDataset(x,stats):\n",
    "  return (x - stats['mean']) / stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePerf(y, preds):\n",
    "    def flatten(l): \n",
    "        if (isinstance(l[0], list)): return [item for sublist in l for item in sublist]\n",
    "        else: return l\n",
    "    import math\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    y = flatten(y)\n",
    "    preds = flatten(preds)\n",
    "    y = [a for a,b in zip(y,preds) if not math.isnan(b)]\n",
    "    preds = [a for a in preds if not math.isnan(a)]\n",
    "    rmse = mean_squared_error(y, preds)\n",
    "    rmae = mean_absolute_error(y, preds)\n",
    "    r2 = r2_score(y, preds)\n",
    "    print(\"MSE, MAE, r2: %f,%f,%f\" % (rmse, rmae, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worstOffenders(y, preds, data, labels):\n",
    "    import numpy as np\n",
    "    mae = np.abs(y - preds)\n",
    "    mse = (y - preds) * (y - preds)\n",
    "    mae_mean, mae_std = np.mean(mae), np.std(mae)\n",
    "    mse_mean, mse_std = np.mean(mse), np.std(mse)\n",
    "    print('mae mean: %.3f, stddev: %.3f' % (mae_mean, mae_std))\n",
    "    print('mse mean: %.3f, stddev: %.3f' % (mse_mean, mse_std))\n",
    "    print('MAE Outliers:')\n",
    "    for i,x in enumerate(mae):\n",
    "        if ((x - mae_mean) > 3 * mae_std):\n",
    "            print('Error: %.1f (want %.1f, got %.1f), file %d, row %d, _idx %d %d, _val %d %d, volume %f' % (\n",
    "                                                                                            y.iloc[i] - preds[i],\n",
    "                                                                                            y.iloc[i], preds[i],\n",
    "                                                                                            data.iloc[i]['fileId'],\n",
    "                                                                                            data.iloc[i]['row'],\n",
    "                                                                                            data.iloc[i]['rising_idx'],\n",
    "                                                                                            data.iloc[i]['falling_idx'],\n",
    "                                                                                            data.iloc[i]['first_val'],\n",
    "                                                                                            data.iloc[i]['last_val'],\n",
    "                                                                                            data.iloc[i]['volume']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateCustom(model, df2, truth):\n",
    "    print(model.predict(df2))\n",
    "    print('True label = %f' % truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNNWeights(model, modelName, hDims, column_names, train_stats):\n",
    "    i = 0\n",
    "\n",
    "    print('  def %s_mean[I,T](toI: scala.Double => I, toT: scala.Double => T) = (toI(%d),toI(%f),toI(%f),toI(%f),toT(%f),toT(%f),toT(%f),toT(%f))' % (modelName, train_stats['mean']['row'],\n",
    "                                                                                                                                                            train_stats['mean']['rising_idx'],\n",
    "                                                                                                                                                            train_stats['mean']['falling_idx'],\n",
    "                                                                                                                                                            train_stats['mean']['volume'],\n",
    "                                                                                                                                                            train_stats['mean']['rising_weight'],\n",
    "                                                                                                                                                            train_stats['mean']['falling_weight'],\n",
    "                                                                                                                                                            train_stats['mean']['first_val'],\n",
    "                                                                                                                                                            train_stats['mean']['last_val']))\n",
    "    #activation(dot(input, kernel) + bias)\n",
    "    assembled = []\n",
    "    for layer in model.get_weights():\n",
    "        s = 'W' if (i % 2 == 0) else 'B'\n",
    "        vals = []\n",
    "        for j,x in enumerate(layer.flatten()):\n",
    "            if (i // 2 >= len(hDims)): dim = 1\n",
    "            else: dim = hDims[i//2]\n",
    "            if (i % 2 == 0):\n",
    "                if (i == 0):\n",
    "                    ft = column_names[:-1][int(j / dim)]\n",
    "                    scaled = x / train_stats['std'][ft]\n",
    "                else:\n",
    "                    scaled = x\n",
    "                vals.append('%f' % scaled)\n",
    "            else:\n",
    "                vals.append('%f' % x)\n",
    "        print('  private val %s_l%d%s = Seq(%s)' % (modelName, 1+i//2, s, ','.join(vals)))\n",
    "    #     print('Layer %d' % i)\n",
    "    #     print(layer)\n",
    "        assembled.append('%s_l%d%s.map(cast)' % (modelName, 1+i//2, s))\n",
    "        i =  i + 1\n",
    "\n",
    "\n",
    "    print('  def %s_layers[T](cast: scala.Double => T) = (%s)' % (modelName, ','.join(assembled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractXGWeights(xg_reg, modelName, MAX_DEPTH):\n",
    "    import pickle\n",
    "    import json\n",
    "    xg_reg.get_booster().dump_model('example_params')\n",
    "#     xg_reg.dump_model('example_params')\n",
    "    # xg_reg.get_booster().get_dump()\n",
    "    numlines = len(open('example_params').readlines(  ))\n",
    "\n",
    "    import numpy\n",
    "    import math\n",
    "    fields = {}\n",
    "    threshes = {}\n",
    "    lefts = {}\n",
    "    rights = {}\n",
    "    tree = -1\n",
    "    num_nodes = int(math.pow(2,MAX_DEPTH+1))\n",
    "\n",
    "    with open('example_params') as file:\n",
    "        for cnt, line in enumerate(file):\n",
    "            if (not 'booster' in line):\n",
    "                idx = int(line.strip().split(':')[0])\n",
    "                if ('leaf=' in line):\n",
    "                    thresh = float(line.strip().split('leaf=')[-1])\n",
    "                    fields[idx] = ''\n",
    "                    threshes[idx] = thresh\n",
    "                else:\n",
    "                    fields[idx] = line.strip().split('[')[1].split('<')[0]\n",
    "                    threshes[idx] = float(line.strip().split('[')[1].split('<')[1].split(']')[0])\n",
    "                    lefts[idx] = int(line.strip().split('yes=')[1].split(',')[0])\n",
    "                    rights[idx] = int(line.strip().split('no=')[1].split(',')[0])\n",
    "            if (('booster' in line and cnt != 0) or cnt == numlines-1):\n",
    "                tree = tree + 1\n",
    "                scala = ['(\"\", 0)' for i in range(num_nodes)]\n",
    "\n",
    "                # Decompress the tree into a complete tree\n",
    "                leftsArr = [lefts[0]]\n",
    "                rightsArr = [rights[0]]\n",
    "                scala[0] = '(\"%s\", %f)' % (fields[0], threshes[0])\n",
    "                for idx in range(1,num_nodes):\n",
    "                    if (idx % 2 == 1 and leftsArr[int(idx/2)] != None):\n",
    "                        lookup = leftsArr[int(idx/2)]\n",
    "                    elif (idx % 2 == 0 and rightsArr[int(idx/2)-1] != None):\n",
    "                        lookup = rightsArr[int(idx/2)-1]\n",
    "                    else: lookup = None\n",
    "\n",
    "                    if (lookup != None):\n",
    "                        scala[idx] = '(\"%s\", %f)' % (fields[lookup], threshes[lookup])\n",
    "                        if (lookup in lefts):\n",
    "                            leftsArr.append(lefts[lookup])\n",
    "                        else:\n",
    "                            leftsArr.append(None)\n",
    "                        if (lookup in rights):\n",
    "                            rightsArr.append(rights[lookup])\n",
    "                        else:\n",
    "                            rightsArr.append(None)\n",
    "                    else:\n",
    "                        leftsArr.append(None)\n",
    "                        rightsArr.append(None)\n",
    "\n",
    "\n",
    "                print('  private val %s_tree%d: List[(String,Double)] = List(' % (modelName,tree) + ','.join(scala).replace(' ','') + ')')\n",
    "                fields = {}\n",
    "                threshes = {}\n",
    "                lefts = {}\n",
    "                rights = {}\n",
    "\n",
    "    print('  def %s_trees[T](cast: scala.Double => T) = List(%s)' % (modelName,','.join(['%s_tree%d.map{x => (x._1, cast(x._2))}' % (modelName,d) for d in range(tree+1)])))\n",
    "#     print('  def base_score = %f' % BASE_SCORE)\n",
    "    #         print(\"Line {}: {}\".format(cnt, line))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLatticeWeights(model, modelName):\n",
    "    print('TBD :)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomImageIndices(numImages, percent):\n",
    "    import random\n",
    "    random.seed(1337)\n",
    "    opts = list(range(0,numImages))\n",
    "    selection = []\n",
    "    for i in range(0,int(numImages*percent)):\n",
    "        r=random.randint(0,len(opts)-1)\n",
    "        selection.append(opts.pop(r))\n",
    "    return selection\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowOffsets():\n",
    "#     datadir = '/scratch/mattfel/data-fs/raw_timetool/u1/coffee/2dtimetool_simulation_data/raw/'\n",
    "\n",
    "#     raw_labels = []\n",
    "#     for fileId in range(1,20): #range(1,99999)::\n",
    "#         fileType = 'fibermap'\n",
    "#         if (fileId % 100 == 0): print('at %d' % fileId)\n",
    "#         f = datadir + '/chirp-2000_interferedelay1650_photonen6.0_carriertagdiamond_nfibers109_netalon1_1.00_1.00_%s.out.%d' % (fileType,fileId)\n",
    "#         if (path.exists(f)):\n",
    "#             rawdata = pd.read_csv(f, \n",
    "#                               skiprows = 1, usecols = [6], \n",
    "#                               na_values = \"?\", comment='\\t',\n",
    "#                               sep=\"\\t|,\", skipinitialspace=True)\n",
    "#             raw_labels.append(rawdata.as_matrix(columns=['delay']).flatten())\n",
    "#     #         raw_labels = raw_labels.append(rawdata)\n",
    "#         else:\n",
    "#             print('file %s does not exist!' % f)\n",
    "\n",
    "#     deltas = []\n",
    "#     for r in raw_labels:\n",
    "#         arr = r[0] - r\n",
    "#         deltas.append(arr)\n",
    "#     deltas = np.asmatrix(deltas)\n",
    "#     print(deltas[0])\n",
    "#     return     [ 0.0, -366.63, -183.32, 183.31, 366.63, 183.31, -183.32, -733.26, -366.63, 366.63, 733.26, 366.63, -366.63, -549.95, 0.0, 549.94, 549.94, 0.0, -549.95, -1099.895, -549.95, 549.94, 1099.89, 549.94, -549.95, -916.58, -183.32, 733.26, 916.57, 183.31, -733.26, -916.58, -733.26, 183.31, 916.57, 733.26, -183.32, -1466.525, -733.26, 733.26, 1466.52, 733.26, -733.26, -1283.21, -366.63, 916.57, 1283.2, 366.63, -916.58, -1283.21, -916.58, 366.63, 1283.2, 916.57, -366.63, -1099.895, 0.0, 1099.89, 1099.89, 0.0, -1099.895, -1833.15477, -916.58, 916.57, 1833.15, 916.57, -916.58, -1649.84, -549.95, 1099.89, 1649.83, 549.94, -1099.895, -1649.84, -1099.895, 549.94, 1649.83, 1099.89, -549.95, -1283.21, 183.31, 1466.52, 1283.2, -183.32, -1466.525, -1283.21, -1466.525, -183.32, 1283.2, 1466.52, 183.31, -1649.84, 0.0, 1649.83, 1649.83, 0.0, -1649.84, -1833.15479, -366.63, 1466.52, 1833.15, 366.63, -1466.525, -1833.15475, 1466.525, 366.63, 1833.15, 1466.52, -366.63, ]\n",
    "    return [0.000,-366.630,-183.561,183.066,366.626,183.556,-183.069,-733.260,-367.122,366.136,733.256,367.126,-366.138,-550.191,-0.492,549.696,550.186,0.492,-549.699,-1099.890,-550.682,549.206,1099.886,550.686,-549.207,-916.821,-184.052,732.766,916.816,184.056,-732.768,-916.329,-733.751,182.576,916.326,733.756,-182.578,-1466.520,-734.243,732.276,1466.516,734.246,-732.277,-1283.451,-367.613,915.836,1283.446,367.616,-915.837,-1282.959,-917.312,365.646,1282.956,917.316]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictPoly(pt, rising_model, falling_model):\n",
    "    import numpy as np\n",
    "    import math\n",
    "    scale = 128.0 # divide rising/falling idx by this number so it fits in fpga precision\n",
    "\n",
    "    tyr = np.sum([m * math.pow(pt['rising_idx'] / scale,i) for i,m in enumerate(rising_model[::-1])])\n",
    "    tyf = np.sum([m * math.pow(pt['falling_idx'] / scale,i) for i,m in enumerate(falling_model[::-1])])\n",
    "    \n",
    "    if (pt['first_val'] >= 30): return tyf # only use falling\n",
    "    elif (pt['last_val'] >= 30): return tyr # only use rising\n",
    "    return (tyr + tyf) / 2 # use both rising and falling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
