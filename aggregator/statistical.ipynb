{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/tutorials/keras/regression\n",
    "\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import scipy\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_lattice as tfl\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import tensorflow_docs as tfdocs\n",
    "# import tensorflow_docs.plots\n",
    "# import tensorflow_docs.modeling\n",
    "\n",
    "# from ttictoc import TicToc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import helpers\n",
    "!#pip install import_ipynb\n",
    "import import_ipynb\n",
    "from common import buildDataset\n",
    "from common import filterBad\n",
    "from common import splitDataset\n",
    "from common import normDataset\n",
    "from common import evaluatePerf\n",
    "from common import predictPoly\n",
    "from common import randomImageIndices\n",
    "from common import evaluateCustom\n",
    "from common import extractNNWeights\n",
    "from common import rowOffsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l): \n",
    "    if (isinstance(l[0], list)): return [item for sublist in l for item in sublist]\n",
    "    else: return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect row data bundled by fileId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, columns = buildDataset('processed.feather')\n",
    "deltas = rowOffsets()\n",
    "\n",
    "test_idx = randomImageIndices(int(len(dataset)/109), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected 6174926 points (56.651264%)\n"
     ]
    }
   ],
   "source": [
    "# back calculating stats for individual rows for hypercube\n",
    "def buildDatasetForLattice(filename = 'processed_floor.feather'):\n",
    "    import pandas as pd\n",
    "    column_names = ['fileId', 'row','rising_idx','falling_idx','volume','rising_weight',\n",
    "                'falling_weight', 'first_val', 'last_val', 'delay']\n",
    "    \n",
    "    # raw_dataset =  pd.read_feather('../preprocessing/processed.feather')\n",
    "#     raw_dataset =  pd.read_feather('/local/ssd/home/mattfel/slac/timetool-ML-models/preprocessing/processed.feather')\n",
    "    raw_dataset =  pd.read_feather('/scratch/mattfel/data-fs/%s' % filename)\n",
    "\n",
    "    dataset = raw_dataset.copy()\n",
    "    dataset.tail()\n",
    "    \n",
    "    unmodeledColumns = ['fileId', 'volume', 'rising_weight', 'falling_weight']\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.drop(columns=unmodeledColumns)\n",
    "    return dataset, [item for item in column_names if item not in unmodeledColumns]\n",
    "\n",
    "stats_dataset, _ = buildDatasetForLattice('processed.feather')\n",
    "stats_dataset = filterBad(stats_dataset, 54)\n",
    "stats_train_dataset, stats_test_dataset, stats_train_labels, stats_test_labels = splitDataset(stats_dataset, 0.2)\n",
    "hcub_columns = ['row', 'rising_idx', 'falling_idx', 'first_val', 'last_val', 'delay']\n",
    "stats_features = [stats_test_dataset[col].values for col in hcub_columns[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "for idx in test_idx:\n",
    "    features = dataset.iloc[idx*109:(idx+1)*109]\n",
    "    test_set.append(features)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def garbageGate(ft, prediction):\n",
    "    if (ft['falling_idx'] - ft['rising_idx'] < 100 or\n",
    "        ft['falling_idx'] - ft['rising_idx'] > 500 or\n",
    "        ft['volume'] < 50 or\n",
    "        (ft['first_val'] > 30 and ft['last_val'] > 30)):\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polyfit general\n",
    "def predictPolyfit(pt, rising_model, falling_model):\n",
    "    scale = 128.0 # divide rising/falling idx by this number so it fits in fpga precision\n",
    "\n",
    "    tyr = np.sum([m * math.pow(pt['rising_idx'] / scale,i) for i,m in enumerate(rising_model[::-1])])\n",
    "    tyf = np.sum([m * math.pow(pt['falling_idx'] / scale,i) for i,m in enumerate(falling_model[::-1])])\n",
    "    \n",
    "    if (pt['first_val'] >= 30): return tyf # only use falling\n",
    "    elif (pt['last_val'] >= 30): return tyr # only use rising\n",
    "    return garbageGate(pt,(tyr + tyf) / 2) # use both rising and falling\n",
    "\n",
    "def polyFit2(ft):\n",
    "    rising_model = [54.93240949884357,-1196.8992955773579,1880.8014648783726]\n",
    "    falling_model = [42.17895475957633,-1117.8885336758663,3174.7423125408845]\n",
    "    return predictPolyfit(ft, rising_model, falling_model)\n",
    "\n",
    "def polyFit3(ft):\n",
    "    rising_model = [-3.453292877552903,82.28053267025867,-1257.1970620164395,1914.700283489191]\n",
    "    falling_model = [-2.4793743678901543,73.19222899717427,-1235.8065880531187,3308.967740573062]\n",
    "    return predictPolyfit(ft, rising_model, falling_model)\n",
    "    \n",
    "def polyFit4(ft):\n",
    "    rising_model = [0.2983872186312137,-6.794932480985469,94.59195292870838,-1274.0845848248348,1921.4176948170698]\n",
    "    falling_model = [0.17045218683904267,-5.383933637827994,90.48238200637373,-1277.9836931678628,3344.180658908667]\n",
    "    return predictPolyfit(ft, rising_model, falling_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCNN general\n",
    "\n",
    "def normalize(ft):\n",
    "    with open('../networks/train_stats', 'rb') as f:\n",
    "        stats = pickle.load(f)\n",
    "    todrop = (['delay'] if 'delay' in ft else []) + (['fileId'] if 'fileId' in ft else [])\n",
    "    normed = ft.copy()\n",
    "    normed['volume'] = ft['volume'] / 4096\n",
    "    normed = (normed.drop(columns=todrop) - stats['mean']) / stats['std']\n",
    "    \n",
    "    col_order = ['row',  'rising_idx',  'falling_idx',       'volume',  'rising_weight',  'falling_weight',  'first_val',  'last_val']\n",
    "    ordered = normed.reindex(columns=col_order)\n",
    "    return ordered\n",
    "    \n",
    "def buildNN(hLayers, actFuncs):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    nn = []\n",
    "    input_dim = 8\n",
    "    for i in range(len(hLayers)):\n",
    "        if (i == 0):\n",
    "            shape = [input_dim]\n",
    "        else:\n",
    "            shape = [hLayers[i-1]]\n",
    "        nn.append(layers.Dense(hLayers[i], activation=actFuncs[i], input_shape = shape))\n",
    "    nn.append(layers.Dense(1))\n",
    "    model = keras.Sequential(nn)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def predictNN(image, model):\n",
    "    nimage = normalize(image)\n",
    "    raw_pred = model.predict(nimage).flatten() #(image.to_frame().transpose())[0][0]\n",
    "    return [garbageGate(image.iloc[i],raw_pred[i]) for i in range(len(image))]\n",
    "\n",
    "def predictNN_64_RELU(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [64]\n",
    "        actFuncs = ['relu']\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_64_RELU_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "def predictNN_64_Leaky(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [64]\n",
    "        \n",
    "        actFuncs = [lambda x : tf.nn.leaky_relu(x, 0.125)]\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_64_Leaky_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "def predictNN_32_32_RELU(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [32, 32]\n",
    "        actFuncs = ['relu', 'relu']\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_32_32_RELU_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "def predictNN_32_32_Leaky(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [32, 32]\n",
    "        actFuncs = [lambda x : tf.nn.leaky_relu(x, 0.125), lambda x : tf.nn.leaky_relu(x, 0.125)]\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_32_32_Leaky_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "    \n",
    "\n",
    "def predictNN_16_16_RELU(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [16, 16]\n",
    "        actFuncs = ['relu', 'relu']\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_16_16_RELU_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "def predictNN_16_16_Leaky(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [16, 16]\n",
    "        actFuncs = [lambda x : tf.nn.leaky_relu(x, 0.125), lambda x : tf.nn.leaky_relu(x, 0.125)]\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_16_16_Leaky_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "\n",
    "\n",
    "def predictNN_8_8_RELU(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [8, 8]\n",
    "        actFuncs = ['relu', 'relu']\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_8_8_RELU_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "def predictNN_8_8_Leaky(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [8, 8]\n",
    "        actFuncs = [lambda x : tf.nn.leaky_relu(x, 0.125), lambda x : tf.nn.leaky_relu(x, 0.125)]\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_8_8_Leaky_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "\n",
    "def predictNN_4_4_RELU(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [4, 4]\n",
    "        actFuncs = ['relu', 'relu']\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_4_4_RELU_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "def predictNN_4_4_Leaky(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [4, 4]\n",
    "        actFuncs = [lambda x : tf.nn.leaky_relu(x, 0.125), lambda x : tf.nn.leaky_relu(x, 0.125)]\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_4_4_Leaky_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "def predictNN_16_4_RELU(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [16, 4]\n",
    "        actFuncs = ['relu', 'relu']\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_16_4_RELU_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "def predictNN_16_4_Leaky(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [16, 4]\n",
    "        actFuncs = [lambda x : tf.nn.leaky_relu(x, 0.125), lambda x : tf.nn.leaky_relu(x, 0.125)]\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_16_4_Leaky_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "    \n",
    "def predictNN_8_4_RELU(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [8, 4]\n",
    "        actFuncs = ['relu', 'relu']\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_8_4_RELU_strong') #load\n",
    "    return predictNN(image, model), model\n",
    "\n",
    "def predictNN_8_4_Leaky(image, model):\n",
    "    if (model == None):\n",
    "        hDims = [8, 4]\n",
    "        actFuncs = [lambda x : tf.nn.leaky_relu(x, 0.125), lambda x : tf.nn.leaky_relu(x, 0.125)]\n",
    "        model = buildNN(hDims, actFuncs)\n",
    "        model.load_weights('../networks/NN_8_4_Leaky_strong') #load\n",
    "    return predictNN(image, model), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildXG(alpha, colsampbytree, base_score, lr, lmbda, num_trees, max_depth):\n",
    "    return xgb.XGBRegressor(objective ='reg:linear', base_score = base_score, \n",
    "                              colsample_bytree = colsampbytree, learning_rate = lr,\n",
    "                              max_depth = max_depth, alpha = alpha, n_estimators = num_trees, reg_lambda = lmbda)\n",
    "\n",
    "def predictXG(image, model):\n",
    "#     nimage = normalize(image)\n",
    "    nimage = image.drop(columns=['fileId', 'delay'])\n",
    "    nimage['volume'] = nimage['volume'] / 4096\n",
    "    raw_pred = model.predict(nimage).flatten() #(image.to_frame().transpose())[0][0]\n",
    "    return [garbageGate(image.iloc[i],raw_pred[i]) for i in range(len(image))]\n",
    "\n",
    "def predictXGBoost_10_5(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 10, 5)\n",
    "        model.load_model('../networks/XGBoost_10_5_strong')\n",
    "    return predictXG(image,model), model\n",
    "def predictXGBoost_20_5(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 20, 5)\n",
    "        model.load_model('../networks/XGBoost_20_5_strong')\n",
    "    return predictXG(image,model), model\n",
    "def predictXGBoost_25_5(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 25, 5)\n",
    "        model.load_model('../networks/XGBoost_25_5_strong')\n",
    "    return predictXG(image,model), model\n",
    "def predictXGBoost_10_6(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 10, 6)\n",
    "        model.load_model('../networks/XGBoost_10_6_strong')\n",
    "    return predictXG(image,model), model\n",
    "def predictXGBoost_20_6(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 20, 6)\n",
    "        model.load_model('../networks/XGBoost_20_6_strong')\n",
    "    return predictXG(image,model), model\n",
    "def predictXGBoost_40_4(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 40, 4)\n",
    "        model.load_model('../networks/XGBoost_40_4_strong')\n",
    "    return predictXG(image,model), model\n",
    "def predictXGBoost_64_4(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 64, 4)\n",
    "        model.load_model('../networks/XGBoost_64_4_strong')\n",
    "    return predictXG(image,model), model\n",
    "def predictXGBoost_96_4(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 96, 4)\n",
    "        model.load_model('../networks/XGBoost_96_4_strong')\n",
    "    return predictXG(image,model), model\n",
    "def predictXGBoost_128_4(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 128, 4)\n",
    "        model.load_model('../networks/XGBoost_128_4_strong')\n",
    "    return predictXG(image,model), model\n",
    "def predictXGBoost_128_3(image, model):\n",
    "    if (model == None):\n",
    "        model = buildXG(10, 0.3, 0.5, 0.9, 2, 128, 3)\n",
    "        model.load_model('../networks/XGBoost_128_3_strong')\n",
    "    return predictXG(image,model), model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictHypercube(image, model):\n",
    "#     nimage = normalize(image)\n",
    "    columns = ['row', 'rising_idx', 'falling_idx', 'first_val', 'last_val', 'delay']\n",
    "    nimage = [image[col].values for col in columns[:-1]]\n",
    "    raw_pred = model.predict(nimage).flatten() \n",
    "    return [garbageGate(image.iloc[i],raw_pred[i]) for i in range(len(image))]\n",
    "\n",
    "\n",
    "\n",
    "def predictHypercube_16_2(image, model):\n",
    "    if (model == None):\n",
    "        model = keras.models.load_model('../networks/hypercube_sweep_16_2.h5', custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    return predictHypercube(image,model), model\n",
    "\n",
    "def predictHypercube_4_2(image, model):\n",
    "    if (model == None):\n",
    "        model = keras.models.load_model('../networks/hypercube_sweep_4_2.h5', custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    return predictHypercube(image,model), model\n",
    "\n",
    "def predictHypercube_4_3(image, model):\n",
    "    if (model == None):\n",
    "        model = keras.models.load_model('../networks/hypercube_sweep_4_3.h5', custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    return predictHypercube(image,model), model\n",
    "\n",
    "def predictHypercube_4_4(image, model):\n",
    "    if (model == None):\n",
    "        model = keras.models.load_model('../networks/hypercube_sweep_4_4.h5', custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    return predictHypercube(image,model), model\n",
    "    \n",
    "def predictHypercube_4_5(image, model):\n",
    "    if (model == None):\n",
    "        model = keras.models.load_model('../networks/hypercube_sweep_4_5.h5', custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    return predictHypercube(image,model), model\n",
    "\n",
    "def predictHypercube_8_2(image, model):\n",
    "    if (model == None):\n",
    "        model = keras.models.load_model('../networks/hypercube_sweep_8_2.h5', custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    return predictHypercube(image,model), model\n",
    "    \n",
    "def predictHypercube_8_3(image, model):\n",
    "    if (model == None):\n",
    "        model = keras.models.load_model('../networks/hypercube_sweep_8_3.h5', custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    return predictHypercube(image,model), model\n",
    "\n",
    "def predictHypercube_8_4(image, model):\n",
    "    if (model == None):\n",
    "        model = keras.models.load_model('../networks/hypercube_sweep_8_4.h5', custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    return predictHypercube(image,model), model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImage(model_name, image, deltas, row_thresh, model):\n",
    "    if (model_name == 'polyfit2'):\n",
    "        return [polyFit2(image.iloc[i]) for i in range(row_thresh)], None\n",
    "    elif (model_name == 'polyfit3'):\n",
    "        return [polyFit3(image.iloc[i]) for i in range(row_thresh)], None\n",
    "    elif (model_name == 'polyfit4'):\n",
    "        return [polyFit4(image.iloc[i]) for i in range(row_thresh)], None\n",
    "    elif (model_name == 'NN_64_RELU_strong'):\n",
    "        preds, model = predictNN_64_RELU(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_64_Leaky_strong'):\n",
    "        preds, model = predictNN_64_Leaky(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_32_32_RELU_strong'):\n",
    "        preds, model = predictNN_32_32_RELU(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_32_32_Leaky_strong'):\n",
    "        preds, model = predictNN_32_32_Leaky(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_16_16_RELU_strong'):\n",
    "        preds, model = predictNN_16_16_RELU(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_16_16_Leaky_strong'):\n",
    "        preds, model = predictNN_16_16_Leaky(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_8_8_RELU_strong'):\n",
    "        preds, model = predictNN_8_8_RELU(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_8_8_Leaky_strong'):\n",
    "        preds, model = predictNN_8_8_Leaky(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_4_4_RELU_strong'):\n",
    "        preds, model = predictNN_4_4_RELU(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_4_4_Leaky_strong'):\n",
    "        preds, model = predictNN_4_4_Leaky(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_16_4_RELU_strong'):\n",
    "        preds, model = predictNN_16_4_RELU(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_16_4_Leaky_strong'):\n",
    "        preds, model = predictNN_16_4_Leaky(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_8_4_RELU_strong'):\n",
    "        preds, model = predictNN_8_4_RELU(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'NN_8_4_Leaky_strong'):\n",
    "        preds, model = predictNN_8_4_Leaky(image, model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_10_5_strong'):\n",
    "            preds, model = predictXGBoost_10_5(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_20_5_strong'):\n",
    "            preds, model = predictXGBoost_20_5(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_25_5_strong'):\n",
    "            preds, model = predictXGBoost_25_5(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_10_6_strong'):\n",
    "            preds, model = predictXGBoost_10_6(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_20_6_strong'):\n",
    "            preds, model = predictXGBoost_20_6(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_40_4_strong'):\n",
    "            preds, model = predictXGBoost_40_4(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_64_4_strong'):\n",
    "            preds, model = predictXGBoost_64_4(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_96_4_strong'):\n",
    "            preds, model = predictXGBoost_96_4(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_128_4_strong'):\n",
    "            preds, model = predictXGBoost_128_4(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'XGBoost_128_3_strong'):\n",
    "            preds, model = predictXGBoost_128_3(image, model)\n",
    "            return preds[:row_thresh], model\n",
    "    elif (model_name == 'Hypercube_16_2'):\n",
    "        preds, model = predictHypercube_16_2(image,model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'Hypercube_4_2'):\n",
    "        preds, model = predictHypercube_4_2(image,model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'Hypercube_4_3'):\n",
    "        preds, model = predictHypercube_4_3(image,model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'Hypercube_4_4'):\n",
    "        preds, model = predictHypercube_4_4(image,model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'Hypercube_4_5'):\n",
    "        preds, model = predictHypercube_4_5(image,model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'Hypercube_8_2'):\n",
    "        preds, model = predictHypercube_8_2(image,model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'Hypercube_8_3'):\n",
    "        preds, model = predictHypercube_8_3(image,model)\n",
    "        return preds[:row_thresh], model\n",
    "    elif (model_name == 'Hypercube_8_4'):\n",
    "        preds, model = predictHypercube_8_4(image,model)\n",
    "        return preds[:row_thresh], model\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [#'polyfit2', 'polyfit3', 'polyfit4',\n",
    "#                'NN_64_RELU_strong', \n",
    "               #'NN_64_Leaky_strong', 'NN_32_32_RELU_strong', 'NN_32_32_Leaky_strong']\n",
    "#                 'NN_16_16_RELU_strong','NN_16_16_Leaky_strong','NN_8_8_RELU_strong','NN_8_8_Leaky_strong','NN_4_4_RELU_strong','NN_4_4_Leaky_strong','NN_16_4_RELU_strong','NN_16_4_Leaky_strong','NN_8_4_RELU_strong','NN_8_4_Leaky_strong',\n",
    "#                 'XGBoost_10_5_strong','XGBoost_20_5_strong','XGBoost_25_5_strong','XGBoost_10_6_strong','XGBoost_20_6_strong','XGBoost_40_4_strong','XGBoost_64_4_strong','XGBoost_96_4_strong','XGBoost_128_4_strong','XGBoost_128_3_strong',\n",
    "    #            'Hypercube_16_2', \n",
    "    'Hypercube_4_2', 'Hypercube_4_3', 'Hypercube_4_4', 'Hypercube_4_5', 'Hypercube_8_2', 'Hypercube_8_3', 'Hypercube_8_4', \n",
    "              ]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypercube_4_2: mse,mae,avg_var: 485.10,10.59,438159.79\n",
      "MSE, MAE, r2: 1706.696126,18.442829,0.998399\n",
      "Hypercube_4_3: mse,mae,avg_var: 451.31,10.13,439160.12\n",
      "MSE, MAE, r2: 1763.654033,19.027192,0.998345\n",
      "Hypercube_4_4: mse,mae,avg_var: 9221.38,75.34,416353.43\n",
      "MSE, MAE, r2: 12027.770572,76.253949,0.988716\n",
      "Hypercube_4_5: mse,mae,avg_var: 5961.15,58.36,415062.92\n",
      "MSE, MAE, r2: 11295.924449,75.016519,0.989402\n",
      "Hypercube_8_2: mse,mae,avg_var: 747.92,14.73,437526.86\n",
      "MSE, MAE, r2: 2346.181674,21.620354,0.997799\n",
      "Hypercube_8_3: mse,mae,avg_var: 13989.99,94.77,428276.17\n",
      "MSE, MAE, r2: 18364.189895,106.183202,0.982771\n",
      "Hypercube_8_4: mse,mae,avg_var: 13459.43,90.35,419870.68\n",
      "MSE, MAE, r2: 18256.849142,102.332042,0.982872\n"
     ]
    }
   ],
   "source": [
    "for model_name in models_list:\n",
    "    model = None\n",
    "    \n",
    "    row_thresh = 54\n",
    "    preds = []\n",
    "    img_labels = []\n",
    "    line_labels = []\n",
    "    stats = []\n",
    "\n",
    "    i = 0\n",
    "    for pt in test_set:\n",
    "        i = i + 1\n",
    "#         if (i % 500 == 0): print(i)\n",
    "        # Get top level label for row\n",
    "        img_label = pt.iloc[0]['delay']\n",
    "        img_labels.append(img_label)\n",
    "        # Get prediction per row on test point\n",
    "        image_pred, model = predictImage(model_name, pt, deltas, row_thresh, model) \n",
    "        line_labels.append(pt.iloc[:]['delay'].tolist()[:row_thresh])\n",
    "        preds.append(image_pred)\n",
    "#         print('got: %.2f, want: %.2f' % (np.nanmean([a+b for a,b in zip(image_pred,deltas)]), img_label))\n",
    "#         print([a+b for a,b in zip(image_pred,deltas)][0:5])\n",
    "#         print([a+b for a,b in zip(pt.iloc[0:5]['delay'], deltas)])\n",
    "#         print()\n",
    "        \n",
    "        stats.append({'pred_mean': np.nanmean(image_pred), \n",
    "                      'pred_var': np.nanvar(image_pred), \n",
    "                      'avg_err': abs(np.nanmean([a + b for a,b in zip(image_pred, deltas)] - img_label))})\n",
    "\n",
    "    mae = np.nansum([x['avg_err'] for x in stats]) / len(stats)\n",
    "    mse = np.nansum([x['avg_err']**2 for x in stats]) / len(stats)\n",
    "    av = np.nansum([x['pred_var'] for x in stats]) / len(stats)\n",
    "    print('%s: mse,mae,avg_var: %.2f,%.2f,%.2f' % (model_name,mse, mae,av))\n",
    "    \n",
    "    model_preds = model.predict(stats_features)\n",
    "    evaluatePerf(stats_test_labels.tolist(), model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944993\n",
      "944993\n",
      "MSE, MAE, r2: 2933.534350,21.632795,0.997248\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pt.iloc[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(evaluatePerf(flatten(line_labels)[0:1*54], flatten(preds)[0:1*54]))\n",
    "# print([a+b for a,b in zip(flatten(line_labels)[0:54], deltas[0:54])])\n",
    "# print([a+b for a,b in zip(flatten(preds)[0:54], deltas[0:54])])\n",
    "# np.nanmean([a+b for a,b in zip(flatten(preds)[0:54], deltas[0:54])])\n",
    "\n",
    "# for pt in test_set[0:1]:\n",
    "#     print('%d=%d' % (pt.iloc[0]['fileId'], pt.iloc[0]['volume']))\n",
    "\n",
    "# print(test_set[0])\n",
    "\n",
    "\n",
    "# import pickle\n",
    "# with open('/scratch/mattfel/testset', 'rb') as f:\n",
    "#     t = pickle.load(f)\n",
    "# with open('/scratch/mattfel/testlbl', 'rb') as f:\n",
    "#     l = pickle.load(f)\n",
    "\n",
    "\n",
    "# preds = [polyFit2(t.iloc[i]) for i in range(len(t))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE, MAE, r2: 1931.824895,25.172853,0.998188\n"
     ]
    }
   ],
   "source": [
    "evaluatePerf(l.tolist(),preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred_mean': 694.1585988354038, 'pred_var': 252710.77694735868, 'avg_err': 5.731842078647074}\n",
      "{'pred_mean': 1073.5753206041124, 'pred_var': 608382.3964831432, 'avg_err': 196.13332060411255}\n",
      "{'pred_mean': -1715.0747, 'pred_var': 547173.9, 'avg_err': 1.7079962836372042}\n",
      "{'pred_mean': 777.8958498636881, 'pred_var': 203391.54544868786, 'avg_err': 6.443122590960836}\n",
      "{'pred_mean': 984.3070086775155, 'pred_var': 159280.09415394586, 'avg_err': 21.67804316027422}\n",
      "{'pred_mean': -282.76038000718603, 'pred_var': 543193.4671427888, 'avg_err': 3.2012426343234157}\n",
      "{'pred_mean': -3.4089186, 'pred_var': 544476.25, 'avg_err': 82.01734807400172}\n",
      "{'pred_mean': 289.27562657407685, 'pred_var': 465237.8441664559, 'avg_err': 5.349861868194489}\n",
      "{'pred_mean': 459.11592550838697, 'pred_var': 464753.0475834671, 'avg_err': 114.48316080250467}\n",
      "{'pred_mean': -772.7942, 'pred_var': 556836.94, 'avg_err': 58.07951352380823}\n"
     ]
    }
   ],
   "source": [
    "for s in stats:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
