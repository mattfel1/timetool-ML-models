{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterBad(dataset, row_limit = -1):\n",
    "    initial_len = len(dataset)\n",
    "    dataset = dataset[(dataset['rising_idx'] != 0)]\n",
    "    dataset = dataset[(dataset['falling_idx'] != 0)]\n",
    "    dataset = dataset[(dataset['falling_idx'] - dataset['rising_idx'] > 100)]\n",
    "#     dataset = dataset[(dataset['volume'] > 500)]\n",
    "    dataset = dataset[(dataset['first_val'] < 30) | (dataset['last_val'] < 30)]\n",
    "#     dataset = dataset[(dataset['first_val'] < 30)]\n",
    "#     dataset = dataset[(dataset['last_val'] < 30)]\n",
    "    if (row_limit > 0):\n",
    "        dataset = dataset[(dataset['row'] < row_limit)]\n",
    "    final_len = len(dataset)\n",
    "    print('Rejected %d points (%f%%)' % ((initial_len-final_len), 100.0*(initial_len-final_len)/initial_len))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dropColumns(dataset,cols):\n",
    "    import pandas as pd\n",
    "    dataset = dataset.drop(columns=cols)\n",
    "    return dataset, [x for x in dataset.columns.values.tolist() if x not in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buildDataset():\n",
    "    import pandas as pd\n",
    "    column_names = ['fileId', 'row','rising_idx','falling_idx','volume','rising_weight',\n",
    "                'falling_weight', 'first_val', 'last_val', 'delay']\n",
    "    \n",
    "    # raw_dataset =  pd.read_feather('../preprocessing/processed.feather')\n",
    "#     raw_dataset =  pd.read_feather('/local/ssd/home/mattfel/slac/timetool-ML-models/preprocessing/processed.feather')\n",
    "    raw_dataset =  pd.read_feather('/scratch/mattfel/data-fs/processed.feather')\n",
    "\n",
    "    dataset = raw_dataset.copy()\n",
    "    dataset.tail()\n",
    "    \n",
    "#     unmodeledColumns = ['fileId']\n",
    "    unmodeledColumns = []\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.drop(columns=unmodeledColumns)\n",
    "    return dataset, [item for item in column_names if item not in unmodeledColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buildDatasetForLattice():\n",
    "    import pandas as pd\n",
    "    column_names = ['fileId', 'row','rising_idx','falling_idx','volume','rising_weight',\n",
    "                'falling_weight', 'first_val', 'last_val', 'delay']\n",
    "    \n",
    "    # raw_dataset =  pd.read_feather('../preprocessing/processed.feather')\n",
    "#     raw_dataset =  pd.read_feather('/local/ssd/home/mattfel/slac/timetool-ML-models/preprocessing/processed.feather')\n",
    "    raw_dataset =  pd.read_feather('/scratch/mattfel/data-fs/processed.feather')\n",
    "\n",
    "    dataset = raw_dataset.copy()\n",
    "    dataset.tail()\n",
    "    \n",
    "    unmodeledColumns = ['fileId', 'volume', 'rising_weight', 'falling_weight']\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.drop(columns=unmodeledColumns)\n",
    "    return dataset, [item for item in column_names if item not in unmodeledColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(dataset, split):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X, y = dataset.iloc[:,:-1],dataset.iloc[:,-1]\n",
    "    train_dataset, test_dataset, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    return train_dataset, test_dataset, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDatasetLabels(dataset, labels, split):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X, y = dataset, labels\n",
    "    train_dataset, test_dataset, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    return train_dataset, test_dataset, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeDataset(dataset, column_names, percent):\n",
    "    visualize_dataset = raw_dataset.sample(frac=percent)\n",
    "    sns.pairplot(visualize_dataset[column_names], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleVolume(dataset, factor):\n",
    "    dataset['volume'] = dataset['volume'] / factor\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normDataset(x,stats):\n",
    "  return (x - stats['mean']) / stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePerf(y, preds):\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    rmse = mean_squared_error(y, preds)\n",
    "    rmae = mean_absolute_error(y, preds)\n",
    "    r2 = r2_score(y, preds)\n",
    "    print(\"MSE, MAE, r2: %f,%f,%f\" % (rmse, rmae, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worstOffenders(y, preds, data, labels):\n",
    "    import numpy as np\n",
    "    mae = np.abs(y - preds)\n",
    "    mse = (y - preds) * (y - preds)\n",
    "    mae_mean, mae_std = np.mean(mae), np.std(mae)\n",
    "    mse_mean, mse_std = np.mean(mse), np.std(mse)\n",
    "    print('mae mean: %.3f, stddev: %.3f' % (mae_mean, mae_std))\n",
    "    print('mse mean: %.3f, stddev: %.3f' % (mse_mean, mse_std))\n",
    "    print('MAE Outliers:')\n",
    "    for i,x in enumerate(mae):\n",
    "        if ((x - mae_mean) > 3 * mae_std):\n",
    "            print('Error: %.1f (want %.1f, got %.1f), file %d, row %d, _idx %d %d, _val %d %d, volume %f' % (\n",
    "                                                                                            y.iloc[i] - preds[i],\n",
    "                                                                                            y.iloc[i], preds[i],\n",
    "                                                                                            data.iloc[i]['fileId'],\n",
    "                                                                                            data.iloc[i]['row'],\n",
    "                                                                                            data.iloc[i]['rising_idx'],\n",
    "                                                                                            data.iloc[i]['falling_idx'],\n",
    "                                                                                            data.iloc[i]['first_val'],\n",
    "                                                                                            data.iloc[i]['last_val'],\n",
    "                                                                                            data.iloc[i]['volume']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateCustom(model, df2, truth):\n",
    "    print(model.predict(df2))\n",
    "    print('True label = %f' % truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNNWeights(model, modelName, hDims, column_names, train_stats):\n",
    "    i = 0\n",
    "\n",
    "    print('  def %s_mean[I,T](toI: scala.Double => I, toT: scala.Double => T) = (toI(%d),toI(%f),toI(%f),toI(%f),toT(%f),toT(%f),toT(%f),toT(%f))' % (modelName, train_stats['mean']['row'],\n",
    "                                                                                                                                                            train_stats['mean']['rising_idx'],\n",
    "                                                                                                                                                            train_stats['mean']['falling_idx'],\n",
    "                                                                                                                                                            train_stats['mean']['volume'],\n",
    "                                                                                                                                                            train_stats['mean']['rising_weight'],\n",
    "                                                                                                                                                            train_stats['mean']['falling_weight'],\n",
    "                                                                                                                                                            train_stats['mean']['first_val'],\n",
    "                                                                                                                                                            train_stats['mean']['last_val']))\n",
    "    #activation(dot(input, kernel) + bias)\n",
    "    assembled = []\n",
    "    for layer in model.get_weights():\n",
    "        s = 'W' if (i % 2 == 0) else 'B'\n",
    "        vals = []\n",
    "        for j,x in enumerate(layer.flatten()):\n",
    "            if (i // 2 >= len(hDims)): dim = 1\n",
    "            else: dim = hDims[i//2]\n",
    "            if (i % 2 == 0):\n",
    "                if (i == 0):\n",
    "                    ft = column_names[:-1][int(j / dim)]\n",
    "                    scaled = x / train_stats['std'][ft]\n",
    "                else:\n",
    "                    scaled = x\n",
    "                vals.append('%f' % scaled)\n",
    "            else:\n",
    "                vals.append('%f' % x)\n",
    "        print('  private val %s_l%d%s = Seq(%s)' % (modelName, 1+i//2, s, ','.join(vals)))\n",
    "    #     print('Layer %d' % i)\n",
    "    #     print(layer)\n",
    "        assembled.append('%s_l%d%s.map(cast)' % (modelName, 1+i//2, s))\n",
    "        i =  i + 1\n",
    "\n",
    "\n",
    "    print('  def %s_layers[T](cast: scala.Double => T) = (%s)' % (modelName, ','.join(assembled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractXGWeights(xg_reg, modelName, MAX_DEPTH):\n",
    "    import pickle\n",
    "    import json\n",
    "    xg_reg.get_booster().dump_model('example_params')\n",
    "#     xg_reg.dump_model('example_params')\n",
    "    # xg_reg.get_booster().get_dump()\n",
    "    numlines = len(open('example_params').readlines(  ))\n",
    "\n",
    "    import numpy\n",
    "    import math\n",
    "    fields = {}\n",
    "    threshes = {}\n",
    "    lefts = {}\n",
    "    rights = {}\n",
    "    tree = -1\n",
    "    num_nodes = int(math.pow(2,MAX_DEPTH+1))\n",
    "\n",
    "    with open('example_params') as file:\n",
    "        for cnt, line in enumerate(file):\n",
    "            if (not 'booster' in line):\n",
    "                idx = int(line.strip().split(':')[0])\n",
    "                if ('leaf=' in line):\n",
    "                    thresh = float(line.strip().split('leaf=')[-1])\n",
    "                    fields[idx] = ''\n",
    "                    threshes[idx] = thresh\n",
    "                else:\n",
    "                    fields[idx] = line.strip().split('[')[1].split('<')[0]\n",
    "                    threshes[idx] = float(line.strip().split('[')[1].split('<')[1].split(']')[0])\n",
    "                    lefts[idx] = int(line.strip().split('yes=')[1].split(',')[0])\n",
    "                    rights[idx] = int(line.strip().split('no=')[1].split(',')[0])\n",
    "            if (('booster' in line and cnt != 0) or cnt == numlines-1):\n",
    "                tree = tree + 1\n",
    "                scala = ['(\"\", 0)' for i in range(num_nodes)]\n",
    "\n",
    "                # Decompress the tree into a complete tree\n",
    "                leftsArr = [lefts[0]]\n",
    "                rightsArr = [rights[0]]\n",
    "                scala[0] = '(\"%s\", %f)' % (fields[0], threshes[0])\n",
    "                for idx in range(1,num_nodes):\n",
    "                    if (idx % 2 == 1 and leftsArr[int(idx/2)] != None):\n",
    "                        lookup = leftsArr[int(idx/2)]\n",
    "                    elif (idx % 2 == 0 and rightsArr[int(idx/2)-1] != None):\n",
    "                        lookup = rightsArr[int(idx/2)-1]\n",
    "                    else: lookup = None\n",
    "\n",
    "                    if (lookup != None):\n",
    "                        scala[idx] = '(\"%s\", %f)' % (fields[lookup], threshes[lookup])\n",
    "                        if (lookup in lefts):\n",
    "                            leftsArr.append(lefts[lookup])\n",
    "                        else:\n",
    "                            leftsArr.append(None)\n",
    "                        if (lookup in rights):\n",
    "                            rightsArr.append(rights[lookup])\n",
    "                        else:\n",
    "                            rightsArr.append(None)\n",
    "                    else:\n",
    "                        leftsArr.append(None)\n",
    "                        rightsArr.append(None)\n",
    "\n",
    "\n",
    "                print('  private val %s_tree%d: List[(String,Double)] = List(' % (modelName,tree) + ','.join(scala).replace(' ','') + ')')\n",
    "                fields = {}\n",
    "                threshes = {}\n",
    "                lefts = {}\n",
    "                rights = {}\n",
    "\n",
    "    print('  def %s_trees[T](cast: scala.Double => T) = List(%s)' % (modelName,','.join(['%s_tree%d.map{x => (x._1, cast(x._2))}' % (modelName,d) for d in range(tree+1)])))\n",
    "#     print('  def base_score = %f' % BASE_SCORE)\n",
    "    #         print(\"Line {}: {}\".format(cnt, line))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLatticeWeights(model, modelName):\n",
    "    print('TBD :)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
