{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_lattice as tfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import helpers\n",
    "import import_ipynb\n",
    "from common import buildDatasetForLattice\n",
    "from common import scaleVolume\n",
    "from common import extractLatticeWeights\n",
    "from common import dropColumns\n",
    "from common import filterBad\n",
    "from common import splitDataset\n",
    "from common import normDataset\n",
    "from common import evaluatePerf\n",
    "from common import evaluateCustom\n",
    "from common import extractXGWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected 6196746 points (56.847699%)\n"
     ]
    }
   ],
   "source": [
    "only_use_strong = 54\n",
    "\n",
    "dataset, columns = buildDatasetForLattice()\n",
    "dataset = filterBad(dataset, only_use_strong)\n",
    "train_dataset, test_dataset, train_labels, test_labels = splitDataset(dataset, 0.2)\n",
    "train_stats = train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['row'], axis=1)\n",
    "columns = [x for x in columns if x != 'row']\n",
    "train_dataset, test_dataset, train_labels, test_labels = splitDataset(dataset, 0.2)\n",
    "train_stats = train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 1.00000000e+00 2.00000000e+00 3.00000000e+00\n",
      " 4.00000000e+00 5.00000000e+00 6.00000000e+00 7.00000000e+00\n",
      " 8.00000000e+00 9.00000000e+00 1.00000000e+01 1.10000000e+01\n",
      " 1.20000000e+01 1.30000000e+01 1.40000000e+01 1.50000000e+01\n",
      " 1.60000000e+01 1.70000000e+01 1.80000000e+01 1.90000000e+01\n",
      " 2.00000000e+01 2.10000000e+01 2.20000000e+01 2.30000000e+01\n",
      " 2.40000000e+01 2.60000000e+01 2.90000000e+01 3.30000000e+01\n",
      " 3.70000000e+01 4.20000000e+01 4.70000000e+01 5.30000000e+01\n",
      " 5.90000000e+01 6.60000000e+01 7.40000000e+01 8.30000000e+01\n",
      " 9.40000000e+01 1.05000000e+02 1.18000000e+02 1.33000000e+02\n",
      " 1.50000000e+02 1.70000000e+02 1.92000000e+02 2.17000000e+02\n",
      " 2.47000000e+02 2.81000000e+02 3.20000000e+02 3.68000000e+02\n",
      " 4.25000000e+02 4.93800000e+02 5.74000000e+02 6.72000000e+02\n",
      " 7.88000000e+02 9.47000000e+02 1.13100000e+03 1.37800000e+03\n",
      " 1.70200000e+03 2.13000000e+03 2.77600000e+03 3.74868571e+03\n",
      " 5.21771429e+03 7.82782857e+03 1.29910286e+04 2.66382000e+04]\n"
     ]
    }
   ],
   "source": [
    "kpts = np.percentile(dataset['first_val'], np.linspace(0,99,64))\n",
    "for i in range(1,len(kpts)):\n",
    "    while kpts[i] <= kpts[i-1]:\n",
    "        kpts[i] = kpts[i] + 1\n",
    "    \n",
    "print(kpts)\n",
    "# quartiles = np.percentile(dataset['first_val'], np.linspace(0,99,8))\n",
    "# print(quartiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLattice(num_keypoints, lattice_size, columns):\n",
    "    # Use ParallelCombination helper layer to group togehter calibration layers\n",
    "    # which have to be executed in paralel in order to be able to use Sequential\n",
    "    # model. Alternatively you can use functional API.\n",
    "    combined_calibrators = tfl.layers.ParallelCombination()\n",
    "\n",
    "    # Configure calibration layers for every feature:\n",
    "\n",
    "    for ft in columns:\n",
    "        kpts = np.percentile(dataset[ft], np.linspace(0,99,num_keypoints))\n",
    "        for i in range(1,len(kpts)):\n",
    "            while kpts[i] <= kpts[i-1]:\n",
    "                kpts[i] = kpts[i] + 1\n",
    "        calibrator = tfl.layers.PWLCalibration(\n",
    "            # Every PWLCalibration layer must have keypoints of piecewise linear\n",
    "            # function specified. Easiest way to specify them is to uniformly cover\n",
    "            # entire input range by using numpy.linspace().\n",
    "            input_keypoints=kpts,\n",
    "#             input_keypoints=np.linspace(dataset[ft].min(),\n",
    "#                                       dataset[ft].max(),\n",
    "#                                       num=num_keypoints),\n",
    "        # You need to ensure that input keypoints have same dtype as layer input.\n",
    "        # You can do it by setting dtype here or by providing keypoints in such\n",
    "        # format which will be converted to deisred tf.dtype by default.\n",
    "        dtype=tf.float32,\n",
    "        # Output range must correspond to expected lattice input range.\n",
    "        output_min=0.0,\n",
    "        output_max=lattice_size - 1.0,\n",
    "        monotonicity='increasing')\n",
    "        combined_calibrators.append(calibrator)\n",
    "\n",
    "    # Create Lattice layer to nonlineary fuse output of calibrators. Don't forget\n",
    "    # to specify monotonicity 'increasing' for any dimension which calibrator is\n",
    "    # monotonic regardless of monotonicity direction of calibrator. This includes\n",
    "    # partial monotonicity of CategoricalCalibration layer.\n",
    "    lattice = tfl.layers.Lattice(\n",
    "      lattice_sizes=[lattice_size for _ in columns],\n",
    "      monotonicities=['increasing' if (ft == 'rising_idx' or ft == 'falling_idx') else 'none' for x in columns],\n",
    "      output_min=dataset['delay'].min(),\n",
    "      output_max=dataset['delay'].max())\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    # We have just 2 layer as far as Sequential model is concerned.\n",
    "    # PWLConcatenate layer takes care of grouping calibrators.\n",
    "    model.add(combined_calibrators)\n",
    "    model.add(lattice)\n",
    "    model.compile(loss=keras.losses.mean_squared_error,\n",
    "                optimizer=keras.optimizers.Adagrad(learning_rate=80.0))\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLattice(model, modelName, cols, epochs):\n",
    "    \n",
    "    features = train_dataset[cols].values.astype(np.float32)\n",
    "    target = train_labels.values.astype(np.float32)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('./' + modelName, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "    model.fit(features,\n",
    "            target,\n",
    "            batch_size=32,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.05,\n",
    "            shuffle=False,\n",
    "            callbacks=[checkpoint])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3574936 samples, validate on 188155 samples\n",
      "Epoch 1/30\n",
      "3574624/3574936 [============================>.] - ETA: 0s - loss: 353079.8081WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574936/3574936 [==============================] - 707s 198us/sample - loss: 353062.2878 - val_loss: 49978.1500\n",
      "Epoch 2/30\n",
      "3574752/3574936 [============================>.] - ETA: 0s - loss: 190614.1429WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574936/3574936 [==============================] - 706s 198us/sample - loss: 190616.6614 - val_loss: 199389.4668\n",
      "Epoch 3/30\n",
      "3574912/3574936 [============================>.] - ETA: 0s - loss: 167776.4935WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574936/3574936 [==============================] - 707s 198us/sample - loss: 167775.6241 - val_loss: 40367.2047\n",
      "Epoch 4/30\n",
      "3574784/3574936 [============================>.] - ETA: 0s - loss: 159023.2429WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574936/3574936 [==============================] - 776s 217us/sample - loss: 159023.0054 - val_loss: 44495.0090\n",
      "Epoch 5/30\n",
      "3574720/3574936 [============================>.] - ETA: 0s - loss: 146773.4204WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574936/3574936 [==============================] - 700s 196us/sample - loss: 146775.6966 - val_loss: 246840.2953\n",
      "Epoch 6/30\n",
      "1258624/3574936 [=========>....................] - ETA: 7:24 - loss: 160693.1215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-0b7ca9199af5>\", line 11, in <module>\n",
      "    Hypercube_8_2 = trainLattice(Hypercube_8_2,'Hypercube_8_2' + strong_data_sfx,columns[:-1], 30) #train\n",
      "  File \"<ipython-input-7-2d57d983f7db>\", line 13, in trainLattice\n",
      "    callbacks=[checkpoint])\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 780, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 363, in model_iteration\n",
      "    batch_outs = f(ins_batch)\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/mattfel/anaconda3/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "show_perf = True\n",
    "load_prior_model = False\n",
    "train_model = True\n",
    "strong_data_sfx = \"_strong\" if (only_use_strong > 0) else \"\"\n",
    "\n",
    "Hypercube_8_2 = buildLattice(8,2,columns[:-1])\n",
    "if (load_prior_model): \n",
    "    Hypercube_8_2.build((None,4,))\n",
    "    Hypercube_8_2.load_weights('./Hypercube_8_2' + strong_data_sfx) #load\n",
    "if (train_model): \n",
    "    Hypercube_8_2 = trainLattice(Hypercube_8_2,'Hypercube_8_2' + strong_data_sfx,columns[:-1], 30) #train\n",
    "    Hypercube_8_2.save('./Hypercube_8_2' + strong_data_sfx)\n",
    "preds = Hypercube_8_2.predict(test_dataset.values.astype(np.float32))\n",
    "if (show_perf): evaluatePerf(test_labels, preds)\n",
    "print('--------------------------------')\n",
    "extractLatticeWeights(Hypercube_8_2, 'Hypercube_8_2' + strong_data_sfx)\n",
    "print('--------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "Hypercube_4_3 = buildLattice(4,3,columns[:-1])\n",
    "if (load_prior_model): \n",
    "    Hypercube_4_3.build((None,4,))\n",
    "    Hypercube_4_3.load_weights('./Hypercube_4_3' + strong_data_sfx) #load\n",
    "if (train_model): \n",
    "    Hypercube_4_3 = trainLattice(Hypercube_4_3,'Hypercube_4_3' + strong_data_sfx,columns[:-1], 30) #train\n",
    "    Hypercube_4_3.save('./Hypercube_4_3' + strong_data_sfx)\n",
    "preds = Hypercube_4_3.predict(test_dataset.values.astype(np.float32))\n",
    "if (show_perf): evaluatePerf(test_labels, preds)\n",
    "print('--------------------------------')\n",
    "extractLatticeWeights(Hypercube_4_3, 'Hypercube_4_3' + strong_data_sfx)\n",
    "print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7ff5ce5da668>\n"
     ]
    }
   ],
   "source": [
    "# print(Hypercube_8_2)\n",
    "# print(train_dataset.values)\n",
    "# print(train_labels.values)\n",
    "test = buildLattice(2,8,columns[:-1])\n",
    "print(test)\n",
    "# print(Hypercube_8_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.200e+01 1.900e+02 3.790e+02 1.300e+01 0.000e+00]\n",
      " [3.600e+01 2.480e+02 4.710e+02 1.943e+03 0.000e+00]\n",
      " [2.500e+01 4.000e+01 2.010e+02 2.500e+01 0.000e+00]\n",
      " [9.600e+01 1.390e+02 3.180e+02 1.000e+00 0.000e+00]\n",
      " [6.600e+01 4.000e+01 1.470e+02 0.000e+00 0.000e+00]\n",
      " [2.400e+01 1.470e+02 3.270e+02 1.850e+02 0.000e+00]\n",
      " [1.400e+01 1.550e+02 3.370e+02 4.890e+02 0.000e+00]\n",
      " [1.050e+02 1.850e+02 3.730e+02 1.200e+01 0.000e+00]\n",
      " [4.900e+01 7.000e+01 8.300e+01 3.660e+03 0.000e+00]\n",
      " [2.600e+01 5.200e+01 2.150e+02 5.900e+01 0.000e+00]]\n",
      "Train on 9 samples, validate on 1 samples\n",
      "9/9 [==============================] - 1s 137ms/sample - loss: 1308289.0000 - val_loss: 2232724.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff5f64755c0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = train_dataset[columns[:-1]].values.astype(np.float32)[0:10]\n",
    "print(features)\n",
    "target = train_labels.values.astype(np.float32)[0:10]\n",
    "\n",
    "test.fit(features,\n",
    "        target,\n",
    "        batch_size=32,\n",
    "        epochs=1,\n",
    "        validation_split=0.05,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "parallel_combination_21 (Par multiple                  40        \n",
      "_________________________________________________________________\n",
      "lattice_21 (Lattice)         multiple                  32        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test.summary())\n",
    "preds = test.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -84.66176]\n",
      " [ 204.66684]\n",
      " [ -94.94145]\n",
      " [-149.71718]\n",
      " [-133.84915]\n",
      " [ -71.80316]\n",
      " [ -34.44699]\n",
      " [-163.96371]\n",
      " [ 572.02185]\n",
      " [ -91.2105 ]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.save('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = buildLattice(2,8,columns[:-1])\n",
    "test2.build((None,5,))\n",
    "test2.load_weights('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -84.66176]\n",
      " [ 204.66684]\n",
      " [ -94.94145]\n",
      " [-149.71718]\n",
      " [-133.84915]\n",
      " [ -71.80316]\n",
      " [ -34.44699]\n",
      " [-163.96371]\n",
      " [ 572.02185]\n",
      " [ -91.2105 ]]\n"
     ]
    }
   ],
   "source": [
    "print(test2.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-5911d59d27fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHypercube_8_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshow_perf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluatePerf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[0;31m# Get step function and loop type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0muse_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_dataset\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m    530\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2280\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2282\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2283\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_predict_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2270\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict_function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2272\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   2273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2274\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   3477\u001b[0m                'backend') % key\n\u001b[1;32m   3478\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mGraphExecutionFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m     \u001b[0;31m# dependencies in call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     \u001b[0;31m# Index 0 = total loss or model output for `predict`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3143\u001b[0m       \u001b[0mupdates_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "preds = Hypercube_8_2.predict(test_dataset.values.astype(np.float32))\n",
    "if (show_perf): evaluatePerf(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
