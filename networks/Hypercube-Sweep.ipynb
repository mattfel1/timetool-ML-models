{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_lattice as tfl\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import helpers\n",
    "import import_ipynb\n",
    "from common import buildDatasetForLattice\n",
    "from common import scaleVolume\n",
    "from common import extractLatticeWeights\n",
    "from common import dropColumns\n",
    "from common import filterBad\n",
    "from common import splitDataset\n",
    "from common import normDataset\n",
    "from common import evaluatePerf\n",
    "from common import evaluateCustom\n",
    "from common import extractXGWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected 6196563 points (56.849770%)\n"
     ]
    }
   ],
   "source": [
    "dataset, columns = buildDatasetForLattice()\n",
    "dataset = filterBad(dataset, 54)\n",
    "train_dataset, test_dataset, train_labels, test_labels = splitDataset(dataset, 0.2)\n",
    "train_stats = train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = dataset.copy()\n",
    "col = preprocessed_dataset.first_val\n",
    "preprocessed_dataset.first_val = np.log(col + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_initializers = {\n",
    "    'row': \"quantile\", 'rising_idx': \"quantile\", 'falling_idx': \"quantile\", 'first_val': \"uniform\", 'last_val': \"uniform\"\n",
    "}\n",
    "monotonicities = {\n",
    "    \"rising_idx\": -1,\n",
    "    \"falling_idx\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLattice(num_keypoints, lattice_size, columns):\n",
    "    inputs = [keras.layers.Input(shape=[1]) for _ in columns]\n",
    "    combined_calibrators = []\n",
    "    for inpt, ft in zip(inputs, columns):\n",
    "        if ft != \"row\":\n",
    "            if kp_initializers[ft] == \"quantile\":\n",
    "                quantile_vals = [i/(num_keypoints - 1.0) for i in range(num_keypoints)]\n",
    "                keypoints = dataset[ft].quantile(quantile_vals).values\n",
    "            if kp_initializers[ft] == \"uniform\":\n",
    "                keypoints = np.linspace(preprocessed_dataset[ft].min(), preprocessed_dataset[ft].max(), num=num_keypoints)\n",
    "\n",
    "            calibrator = tfl.layers.PWLCalibration(\n",
    "                input_keypoints=keypoints, dtype=tf.float32, output_min=0.0, output_max=lattice_size - 1.0,\n",
    "                kernel_regularizer = [(\"wrinkle\", 1e-4, 1e-5)],\n",
    "                monotonicity=monotonicities.get(ft, 0),\n",
    "            )(inpt)\n",
    "        else:\n",
    "            # row is categorical\n",
    "            calibrator = tfl.layers.CategoricalCalibration(\n",
    "                num_buckets=preprocessed_dataset[ft].nunique(),\n",
    "                output_min = 0.0,\n",
    "                output_max = lattice_size - 1.0,\n",
    "            )(inpt)\n",
    "        combined_calibrators.append(calibrator)\n",
    "    lattice = tfl.layers.Lattice(\n",
    "        lattice_sizes=[lattice_size for _ in columns],\n",
    "        monotonicities=['increasing' if (ft == 'rising_idx' or ft == 'falling_idx') else 'none' for x in columns],\n",
    "        output_min=dataset['delay'].min(),\n",
    "        output_max=dataset['delay'].max())(keras.layers.concatenate(combined_calibrators))\n",
    "    \n",
    "    model = keras.models.Model(inputs=inputs, outputs=lattice)\n",
    "    model.compile(loss=keras.losses.mean_absolute_error,\n",
    "                optimizer=keras.optimizers.Adam(), metrics=[\"mse\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLattice(model, cols, epochs):\n",
    "    features = [preprocessed_dataset[col].values for col in cols]\n",
    "    target = preprocessed_dataset[\"delay\"]\n",
    "\n",
    "    model.fit(features,\n",
    "            target,\n",
    "            batch_size=32,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.2,\n",
    "            shuffle=False, workers=32, use_multiprocessing=True, callbacks=[\n",
    "                keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', restore_best_weights=True)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning: 4 5\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/40\n",
      "3762662/3762662 [==============================] - 384s 102us/sample - loss: 374.7888 - mse: 275606.8750 - val_loss: 374.3428 - val_mse: 256742.0312\n",
      "Epoch 2/40\n",
      "3762662/3762662 [==============================] - 380s 101us/sample - loss: 347.2587 - mse: 232136.2812 - val_loss: 318.3040 - val_mse: 205969.5781\n",
      "Epoch 3/40\n",
      "3762662/3762662 [==============================] - 381s 101us/sample - loss: 291.9383 - mse: 181858.6875 - val_loss: 267.1119 - val_mse: 164587.1562\n",
      "Epoch 4/40\n",
      "3762662/3762662 [==============================] - 378s 100us/sample - loss: 245.7229 - mse: 147546.0469 - val_loss: 227.4076 - val_mse: 138077.5312\n",
      "Epoch 5/40\n",
      "3762662/3762662 [==============================] - 380s 101us/sample - loss: 212.0789 - mse: 127144.4062 - val_loss: 199.6695 - val_mse: 121214.0156\n",
      "Epoch 6/40\n",
      "3762662/3762662 [==============================] - 380s 101us/sample - loss: 189.0234 - mse: 112286.2109 - val_loss: 181.1647 - val_mse: 106856.1875\n",
      "Epoch 7/40\n",
      "3762662/3762662 [==============================] - 375s 100us/sample - loss: 173.5546 - mse: 99225.7578 - val_loss: 167.8457 - val_mse: 93576.2266\n",
      "Epoch 8/40\n",
      "3762662/3762662 [==============================] - 372s 99us/sample - loss: 162.1119 - mse: 88288.1719 - val_loss: 158.3317 - val_mse: 85189.1016\n",
      "Epoch 9/40\n",
      "3762662/3762662 [==============================] - 383s 102us/sample - loss: 153.0581 - mse: 79930.0781 - val_loss: 149.2805 - val_mse: 76213.7109\n",
      "Epoch 10/40\n",
      "3762662/3762662 [==============================] - 381s 101us/sample - loss: 143.8697 - mse: 70969.9219 - val_loss: 139.9360 - val_mse: 67289.5156\n",
      "Epoch 11/40\n",
      "3762662/3762662 [==============================] - 374s 99us/sample - loss: 134.2723 - mse: 61870.5625 - val_loss: 130.0970 - val_mse: 58030.7188\n",
      "Epoch 12/40\n",
      "3762662/3762662 [==============================] - 380s 101us/sample - loss: 124.2454 - mse: 52830.0547 - val_loss: 119.8210 - val_mse: 49031.1406\n",
      "Epoch 13/40\n",
      "3762662/3762662 [==============================] - 375s 100us/sample - loss: 113.7968 - mse: 44048.4922 - val_loss: 109.1289 - val_mse: 40341.6328\n",
      "Epoch 14/40\n",
      "3762662/3762662 [==============================] - 375s 100us/sample - loss: 103.0108 - mse: 35753.6094 - val_loss: 98.0397 - val_mse: 32232.4180\n",
      "Epoch 15/40\n",
      "3762662/3762662 [==============================] - 375s 100us/sample - loss: 91.9771 - mse: 28163.0312 - val_loss: 86.8863 - val_mse: 24991.4023\n",
      "Epoch 16/40\n",
      "3762662/3762662 [==============================] - 385s 102us/sample - loss: 80.7996 - mse: 21391.5078 - val_loss: 75.6541 - val_mse: 18639.1367\n",
      "Epoch 17/40\n",
      "3762662/3762662 [==============================] - 382s 101us/sample - loss: 69.5952 - mse: 15531.1504 - val_loss: 64.5434 - val_mse: 13210.1143\n",
      "Epoch 18/40\n",
      "3762662/3762662 [==============================] - 378s 101us/sample - loss: 58.5680 - mse: 10679.8730 - val_loss: 53.5747 - val_mse: 8790.3486\n",
      "Epoch 19/40\n",
      "3762662/3762662 [==============================] - 376s 100us/sample - loss: 47.9279 - mse: 6884.6924 - val_loss: 43.2635 - val_mse: 5493.1401\n",
      "Epoch 20/40\n",
      "3762662/3762662 [==============================] - 391s 104us/sample - loss: 38.0158 - mse: 4156.8940 - val_loss: 33.7950 - val_mse: 3215.5908\n",
      "Epoch 21/40\n",
      "3762662/3762662 [==============================] - 388s 103us/sample - loss: 29.3166 - mse: 2503.1680 - val_loss: 25.6570 - val_mse: 2022.9242\n",
      "Epoch 22/40\n",
      "3762662/3762662 [==============================] - 383s 102us/sample - loss: 23.3374 - mse: 1828.4214 - val_loss: 21.7779 - val_mse: 1721.6812\n",
      "Epoch 23/40\n",
      "3762662/3762662 [==============================] - 384s 102us/sample - loss: 21.2388 - mse: 1674.1604 - val_loss: 20.8982 - val_mse: 1628.9733\n",
      "Epoch 24/40\n",
      "3762662/3762662 [==============================] - 373s 99us/sample - loss: 20.5366 - mse: 1577.6284 - val_loss: 20.2471 - val_mse: 1533.8462\n",
      "Epoch 25/40\n",
      "3762662/3762662 [==============================] - 379s 101us/sample - loss: 19.9829 - mse: 1497.3376 - val_loss: 19.8515 - val_mse: 1470.1097\n",
      "Epoch 26/40\n",
      "3762662/3762662 [==============================] - 385s 102us/sample - loss: 19.5585 - mse: 1436.4734 - val_loss: 19.4715 - val_mse: 1416.6924\n",
      "Epoch 27/40\n",
      "3762662/3762662 [==============================] - 383s 102us/sample - loss: 19.2340 - mse: 1390.0105 - val_loss: 19.4079 - val_mse: 1391.2310\n",
      "Epoch 28/40\n",
      "3762662/3762662 [==============================] - 382s 101us/sample - loss: 18.9717 - mse: 1354.3634 - val_loss: 19.5122 - val_mse: 1372.9569\n",
      "Epoch 29/40\n",
      "3762662/3762662 [==============================] - 376s 100us/sample - loss: 18.7540 - mse: 1326.3564 - val_loss: 18.9734 - val_mse: 1331.4292\n",
      "Epoch 30/40\n",
      "3762662/3762662 [==============================] - 382s 102us/sample - loss: 18.5661 - mse: 1303.6240 - val_loss: 18.8478 - val_mse: 1311.4559\n",
      "Epoch 31/40\n",
      "3762662/3762662 [==============================] - 376s 100us/sample - loss: 18.4044 - mse: 1284.2482 - val_loss: 18.7451 - val_mse: 1294.0997\n",
      "Epoch 32/40\n",
      "3762662/3762662 [==============================] - 379s 101us/sample - loss: 18.2632 - mse: 1268.0295 - val_loss: 18.6440 - val_mse: 1279.6290\n",
      "Epoch 33/40\n",
      "3762662/3762662 [==============================] - 377s 100us/sample - loss: 18.1383 - mse: 1253.9224 - val_loss: 18.6630 - val_mse: 1274.5934\n",
      "Epoch 34/40\n",
      "3762662/3762662 [==============================] - 378s 100us/sample - loss: 18.0256 - mse: 1242.1074 - val_loss: 18.4970 - val_mse: 1256.5656\n",
      "Epoch 35/40\n",
      "3762662/3762662 [==============================] - 384s 102us/sample - loss: 17.9195 - mse: 1231.3376 - val_loss: 18.5567 - val_mse: 1257.0747\n",
      "Epoch 36/40\n",
      "3762662/3762662 [==============================] - 376s 100us/sample - loss: 17.8142 - mse: 1221.3486 - val_loss: 17.9981 - val_mse: 1223.8885\n",
      "Epoch 37/40\n",
      "3762662/3762662 [==============================] - 372s 99us/sample - loss: 17.7203 - mse: 1212.4950 - val_loss: 18.2164 - val_mse: 1229.5568\n",
      "Epoch 38/40\n",
      "3762662/3762662 [==============================] - 375s 100us/sample - loss: 17.6257 - mse: 1204.3505 - val_loss: 18.1152 - val_mse: 1220.1968\n",
      "Epoch 39/40\n",
      "3762662/3762662 [==============================] - 378s 100us/sample - loss: 17.5434 - mse: 1196.8816 - val_loss: 17.8094 - val_mse: 1201.9698\n",
      "Epoch 40/40\n",
      "3762662/3762662 [==============================] - 382s 101us/sample - loss: 17.4580 - mse: 1189.4298 - val_loss: 18.1049 - val_mse: 1210.9974\n",
      "Beginning: 8 3\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/40\n",
      "3762662/3762662 [==============================] - 317s 84us/sample - loss: 189.3167 - mse: 113279.0859 - val_loss: 172.9035 - val_mse: 94234.9375\n",
      "Epoch 2/40\n",
      "3762662/3762662 [==============================] - 322s 86us/sample - loss: 173.2137 - mse: 92642.2969 - val_loss: 170.3740 - val_mse: 93114.2734\n",
      "Epoch 3/40\n",
      "3762662/3762662 [==============================] - 317s 84us/sample - loss: 170.3574 - mse: 91184.8594 - val_loss: 167.5447 - val_mse: 91720.4375\n",
      "Epoch 4/40\n",
      "3762662/3762662 [==============================] - 315s 84us/sample - loss: 167.5391 - mse: 90136.8359 - val_loss: 164.7404 - val_mse: 90912.3906\n",
      "Epoch 5/40\n",
      "3762662/3762662 [==============================] - 322s 86us/sample - loss: 164.2398 - mse: 89151.9219 - val_loss: 160.9770 - val_mse: 89448.7031\n",
      "Epoch 6/40\n",
      "3762662/3762662 [==============================] - 311s 83us/sample - loss: 160.1220 - mse: 87683.8828 - val_loss: 156.8482 - val_mse: 88187.1641\n",
      "Epoch 7/40\n",
      "3762662/3762662 [==============================] - 309s 82us/sample - loss: 154.3105 - mse: 84947.9219 - val_loss: 150.0192 - val_mse: 83677.5625\n",
      "Epoch 8/40\n",
      "3762662/3762662 [==============================] - 315s 84us/sample - loss: 143.6063 - mse: 76524.7812 - val_loss: 133.9182 - val_mse: 69081.6094\n",
      "Epoch 9/40\n",
      "3762662/3762662 [==============================] - 321s 85us/sample - loss: 127.2197 - mse: 61546.8789 - val_loss: 118.3715 - val_mse: 55603.5078\n",
      "Epoch 10/40\n",
      "3762662/3762662 [==============================] - 319s 85us/sample - loss: 110.5511 - mse: 48250.7734 - val_loss: 100.2189 - val_mse: 41811.7461\n",
      "Epoch 11/40\n",
      "3762662/3762662 [==============================] - 312s 83us/sample - loss: 91.6477 - mse: 35512.2539 - val_loss: 81.3022 - val_mse: 30398.9180\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3762662/3762662 [==============================] - 303s 81us/sample - loss: 74.8091 - mse: 26246.2539 - val_loss: 66.2958 - val_mse: 22674.0195\n",
      "Epoch 13/40\n",
      "3762662/3762662 [==============================] - 310s 82us/sample - loss: 61.6754 - mse: 20256.5723 - val_loss: 59.3089 - val_mse: 19132.7734\n",
      "Epoch 14/40\n",
      "3762662/3762662 [==============================] - 313s 83us/sample - loss: 55.9959 - mse: 17277.8477 - val_loss: 54.3523 - val_mse: 16304.6982\n",
      "Epoch 15/40\n",
      "3762662/3762662 [==============================] - 313s 83us/sample - loss: 51.7485 - mse: 14144.4219 - val_loss: 49.9132 - val_mse: 13022.0225\n",
      "Epoch 16/40\n",
      "3762662/3762662 [==============================] - 312s 83us/sample - loss: 47.0988 - mse: 11100.9385 - val_loss: 45.4832 - val_mse: 10176.3584\n",
      "Epoch 17/40\n",
      "3762662/3762662 [==============================] - 308s 82us/sample - loss: 42.1368 - mse: 8287.3750 - val_loss: 40.0774 - val_mse: 7153.0288\n",
      "Epoch 18/40\n",
      "3762662/3762662 [==============================] - 308s 82us/sample - loss: 33.8598 - mse: 4655.7627 - val_loss: 30.6897 - val_mse: 3564.7080\n",
      "Epoch 19/40\n",
      "3762662/3762662 [==============================] - 311s 83us/sample - loss: 28.7334 - mse: 3051.9614 - val_loss: 27.2923 - val_mse: 2726.3125\n",
      "Epoch 20/40\n",
      "3762662/3762662 [==============================] - 307s 82us/sample - loss: 25.3645 - mse: 2286.9421 - val_loss: 24.1892 - val_mse: 2038.9635\n",
      "Epoch 21/40\n",
      "3762662/3762662 [==============================] - 304s 81us/sample - loss: 22.2357 - mse: 1705.0424 - val_loss: 21.2344 - val_mse: 1528.3722\n",
      "Epoch 22/40\n",
      "3762662/3762662 [==============================] - 301s 80us/sample - loss: 19.5192 - mse: 1333.7238 - val_loss: 19.2893 - val_mse: 1254.3419\n",
      "Epoch 23/40\n",
      "3762662/3762662 [==============================] - 308s 82us/sample - loss: 17.6018 - mse: 1160.0957 - val_loss: 17.7592 - val_mse: 1143.2048\n",
      "Epoch 24/40\n",
      "3762662/3762662 [==============================] - 306s 81us/sample - loss: 16.8314 - mse: 1118.3074 - val_loss: 17.7746 - val_mse: 1143.4528\n",
      "Epoch 25/40\n",
      "3762662/3762662 [==============================] - 307s 82us/sample - loss: 16.8683 - mse: 1122.3414 - val_loss: 17.7809 - val_mse: 1148.1366\n",
      "Epoch 26/40\n",
      "3762662/3762662 [==============================] - 309s 82us/sample - loss: 16.8741 - mse: 1117.5693 - val_loss: 18.2725 - val_mse: 1158.8865\n",
      "Beginning: 8 4\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/40\n",
      "3762662/3762662 [==============================] - 347s 92us/sample - loss: 192.6595 - mse: 119724.5078 - val_loss: 170.6805 - val_mse: 93252.9062\n",
      "Epoch 2/40\n",
      "3762662/3762662 [==============================] - 357s 95us/sample - loss: 169.5601 - mse: 91219.3984 - val_loss: 166.4085 - val_mse: 91741.6484\n",
      "Epoch 3/40\n",
      "3762662/3762662 [==============================] - 358s 95us/sample - loss: 164.1360 - mse: 88550.7500 - val_loss: 159.4809 - val_mse: 87219.2969\n",
      "Epoch 4/40\n",
      "3762662/3762662 [==============================] - 353s 94us/sample - loss: 157.7917 - mse: 84524.1953 - val_loss: 153.8813 - val_mse: 83953.8672\n",
      "Epoch 5/40\n",
      "3762662/3762662 [==============================] - 351s 93us/sample - loss: 153.0651 - mse: 81926.6328 - val_loss: 150.9282 - val_mse: 82679.9688\n",
      "Epoch 6/40\n",
      "3762662/3762662 [==============================] - 355s 94us/sample - loss: 149.4297 - mse: 80131.1484 - val_loss: 144.9720 - val_mse: 78411.5391\n",
      "Epoch 7/40\n",
      "3762662/3762662 [==============================] - 353s 94us/sample - loss: 137.5211 - mse: 69757.8203 - val_loss: 126.1759 - val_mse: 60877.0820\n",
      "Epoch 8/40\n",
      "3762662/3762662 [==============================] - 348s 93us/sample - loss: 117.0634 - mse: 52295.3008 - val_loss: 105.5110 - val_mse: 44486.5391\n",
      "Epoch 9/40\n",
      "3762662/3762662 [==============================] - 347s 92us/sample - loss: 96.7636 - mse: 38000.9805 - val_loss: 85.4589 - val_mse: 31760.4199\n",
      "Epoch 10/40\n",
      "3762662/3762662 [==============================] - 361s 96us/sample - loss: 78.3198 - mse: 27605.5664 - val_loss: 69.9122 - val_mse: 23872.4219\n",
      "Epoch 11/40\n",
      "3762662/3762662 [==============================] - 354s 94us/sample - loss: 65.6561 - mse: 21888.0137 - val_loss: 61.3500 - val_mse: 20423.9043\n",
      "Epoch 12/40\n",
      "3762662/3762662 [==============================] - 359s 96us/sample - loss: 58.9166 - mse: 18978.2617 - val_loss: 57.1795 - val_mse: 18008.7773\n",
      "Epoch 13/40\n",
      "3762662/3762662 [==============================] - 362s 96us/sample - loss: 55.7225 - mse: 16605.9316 - val_loss: 55.1316 - val_mse: 15693.6494\n",
      "Epoch 14/40\n",
      "3762662/3762662 [==============================] - 362s 96us/sample - loss: 54.3429 - mse: 14585.9951 - val_loss: 52.5703 - val_mse: 13362.4199\n",
      "Epoch 15/40\n",
      "3762662/3762662 [==============================] - 352s 94us/sample - loss: 50.5228 - mse: 12032.9658 - val_loss: 48.2769 - val_mse: 10818.1279\n",
      "Epoch 16/40\n",
      "3762662/3762662 [==============================] - 349s 93us/sample - loss: 46.0569 - mse: 9591.1025 - val_loss: 43.7089 - val_mse: 8438.8799\n",
      "Epoch 17/40\n",
      "3762662/3762662 [==============================] - 352s 94us/sample - loss: 41.6238 - mse: 7418.4141 - val_loss: 39.4003 - val_mse: 6464.6982\n",
      "Epoch 18/40\n",
      "3762662/3762662 [==============================] - 356s 94us/sample - loss: 37.2721 - mse: 5565.4546 - val_loss: 35.2530 - val_mse: 4785.6235\n",
      "Epoch 19/40\n",
      "3762662/3762662 [==============================] - 354s 94us/sample - loss: 33.0682 - mse: 4062.0754 - val_loss: 31.5081 - val_mse: 3533.5879\n",
      "Epoch 20/40\n",
      "3762662/3762662 [==============================] - 353s 94us/sample - loss: 29.1523 - mse: 2917.9248 - val_loss: 27.3916 - val_mse: 2479.7610\n",
      "Epoch 21/40\n",
      "3762662/3762662 [==============================] - 352s 93us/sample - loss: 25.3559 - mse: 2095.4500 - val_loss: 23.8654 - val_mse: 1833.6761\n",
      "Epoch 22/40\n",
      "3762662/3762662 [==============================] - 349s 93us/sample - loss: 22.1771 - mse: 1598.6960 - val_loss: 21.1505 - val_mse: 1452.7889\n",
      "Epoch 23/40\n",
      "3762662/3762662 [==============================] - 354s 94us/sample - loss: 19.8697 - mse: 1345.0626 - val_loss: 19.0804 - val_mse: 1264.8866\n",
      "Epoch 24/40\n",
      "3762662/3762662 [==============================] - 361s 96us/sample - loss: 18.3065 - mse: 1212.8073 - val_loss: 18.4526 - val_mse: 1192.6497\n",
      "Epoch 25/40\n",
      "3762662/3762662 [==============================] - 357s 95us/sample - loss: 17.4771 - mse: 1143.2024 - val_loss: 17.5200 - val_mse: 1123.3958\n",
      "Epoch 26/40\n",
      "3762662/3762662 [==============================] - 351s 93us/sample - loss: 17.1253 - mse: 1112.3857 - val_loss: 17.5561 - val_mse: 1119.6343\n",
      "Epoch 27/40\n",
      "3762662/3762662 [==============================] - 348s 92us/sample - loss: 17.1262 - mse: 1102.2445 - val_loss: 17.6613 - val_mse: 1111.7684\n",
      "Epoch 28/40\n",
      "3762662/3762662 [==============================] - 350s 93us/sample - loss: 17.1489 - mse: 1089.2089 - val_loss: 17.9654 - val_mse: 1109.4790\n"
     ]
    }
   ],
   "source": [
    "show_perf = True\n",
    "train_model = True\n",
    "load_model=False\n",
    "\n",
    "for num_kps, lattice_size in [(4, 2), (8, 2), (16, 2), (4, 3), (4, 4), (4, 5), (8, 3), (8, 4)][5:]:\n",
    "    model_path = f\"./hypercube_sweep_{num_kps}_{lattice_size}.h5\"\n",
    "    print(\"Beginning:\", num_kps, lattice_size)\n",
    "\n",
    "    if load_model:\n",
    "        model = keras.models.load_model(model_path, custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    else:\n",
    "        model = buildLattice(num_kps, lattice_size, columns[:-1])\n",
    "    history = trainLattice(model, columns[:-1], 40)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
