{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mattfel/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 242 samples, validate on 61 samples\n",
      "WARNING:tensorflow:From /home/mattfel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/mattfel/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/200\n",
      "242/242 [==============================] - 2s 7ms/sample - loss: 0.2068 - val_loss: 0.1691\n",
      "Epoch 2/200\n",
      "242/242 [==============================] - 0s 475us/sample - loss: 0.1783 - val_loss: 0.1540\n",
      "Epoch 3/200\n",
      "242/242 [==============================] - 0s 494us/sample - loss: 0.1705 - val_loss: 0.1569\n",
      "Epoch 4/200\n",
      "242/242 [==============================] - 0s 380us/sample - loss: 0.1710 - val_loss: 0.1492\n",
      "Epoch 5/200\n",
      "242/242 [==============================] - 0s 389us/sample - loss: 0.1642 - val_loss: 0.1464\n",
      "Epoch 6/200\n",
      "242/242 [==============================] - 0s 397us/sample - loss: 0.1626 - val_loss: 0.1508\n",
      "Epoch 7/200\n",
      "242/242 [==============================] - 0s 498us/sample - loss: 0.1658 - val_loss: 0.1475\n",
      "Epoch 8/200\n",
      "242/242 [==============================] - 0s 473us/sample - loss: 0.1596 - val_loss: 0.1464\n",
      "Epoch 9/200\n",
      "242/242 [==============================] - 0s 457us/sample - loss: 0.1597 - val_loss: 0.1464\n",
      "Epoch 10/200\n",
      "242/242 [==============================] - 0s 377us/sample - loss: 0.1588 - val_loss: 0.1453\n",
      "Epoch 11/200\n",
      "242/242 [==============================] - 0s 367us/sample - loss: 0.1621 - val_loss: 0.1541\n",
      "Epoch 12/200\n",
      "242/242 [==============================] - 0s 442us/sample - loss: 0.1601 - val_loss: 0.1455\n",
      "Epoch 13/200\n",
      "242/242 [==============================] - 0s 432us/sample - loss: 0.1552 - val_loss: 0.1470\n",
      "Epoch 14/200\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.1568 - val_loss: 0.1482\n",
      "Epoch 15/200\n",
      "242/242 [==============================] - 0s 437us/sample - loss: 0.1562 - val_loss: 0.1470\n",
      "Epoch 16/200\n",
      "242/242 [==============================] - 0s 435us/sample - loss: 0.1576 - val_loss: 0.1481\n",
      "Epoch 17/200\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.1603 - val_loss: 0.1530\n",
      "Epoch 18/200\n",
      "242/242 [==============================] - 0s 460us/sample - loss: 0.1582 - val_loss: 0.1472\n",
      "Epoch 19/200\n",
      "242/242 [==============================] - 0s 461us/sample - loss: 0.1556 - val_loss: 0.1495\n",
      "Epoch 20/200\n",
      "242/242 [==============================] - 0s 457us/sample - loss: 0.1540 - val_loss: 0.1473\n",
      "Epoch 21/200\n",
      "242/242 [==============================] - 0s 474us/sample - loss: 0.1541 - val_loss: 0.1499\n",
      "Epoch 22/200\n",
      "242/242 [==============================] - 0s 478us/sample - loss: 0.1550 - val_loss: 0.1479\n",
      "Epoch 23/200\n",
      "242/242 [==============================] - 0s 715us/sample - loss: 0.1541 - val_loss: 0.1496\n",
      "Epoch 24/200\n",
      "242/242 [==============================] - 0s 484us/sample - loss: 0.1535 - val_loss: 0.1478\n",
      "Epoch 25/200\n",
      "242/242 [==============================] - 0s 650us/sample - loss: 0.1541 - val_loss: 0.1484\n",
      "Epoch 26/200\n",
      "242/242 [==============================] - 0s 443us/sample - loss: 0.1578 - val_loss: 0.1516\n",
      "Epoch 27/200\n",
      "242/242 [==============================] - 0s 478us/sample - loss: 0.1563 - val_loss: 0.1503\n",
      "Epoch 28/200\n",
      "242/242 [==============================] - 0s 527us/sample - loss: 0.1565 - val_loss: 0.1489\n",
      "Epoch 29/200\n",
      "242/242 [==============================] - 0s 450us/sample - loss: 0.1528 - val_loss: 0.1485\n",
      "Epoch 30/200\n",
      "242/242 [==============================] - 0s 464us/sample - loss: 0.1536 - val_loss: 0.1481\n",
      "Epoch 31/200\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1525 - val_loss: 0.1485\n",
      "Epoch 32/200\n",
      "242/242 [==============================] - 0s 448us/sample - loss: 0.1532 - val_loss: 0.1489\n",
      "Epoch 33/200\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.1535 - val_loss: 0.1488\n",
      "Epoch 34/200\n",
      "242/242 [==============================] - 0s 460us/sample - loss: 0.1527 - val_loss: 0.1502\n",
      "Epoch 35/200\n",
      "242/242 [==============================] - 0s 456us/sample - loss: 0.1532 - val_loss: 0.1504\n",
      "Epoch 36/200\n",
      "242/242 [==============================] - 0s 463us/sample - loss: 0.1543 - val_loss: 0.1507\n",
      "Epoch 37/200\n",
      "242/242 [==============================] - 0s 471us/sample - loss: 0.1531 - val_loss: 0.1497\n",
      "Epoch 38/200\n",
      "242/242 [==============================] - 0s 452us/sample - loss: 0.1533 - val_loss: 0.1505\n",
      "Epoch 39/200\n",
      "242/242 [==============================] - 0s 473us/sample - loss: 0.1533 - val_loss: 0.1508\n",
      "Epoch 40/200\n",
      "242/242 [==============================] - 0s 460us/sample - loss: 0.1523 - val_loss: 0.1487\n",
      "Epoch 41/200\n",
      "242/242 [==============================] - 0s 466us/sample - loss: 0.1529 - val_loss: 0.1501\n",
      "Epoch 42/200\n",
      "242/242 [==============================] - 0s 467us/sample - loss: 0.1555 - val_loss: 0.1508\n",
      "Epoch 43/200\n",
      "242/242 [==============================] - 0s 456us/sample - loss: 0.1527 - val_loss: 0.1507\n",
      "Epoch 44/200\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1518 - val_loss: 0.1489\n",
      "Epoch 45/200\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.1553 - val_loss: 0.1507\n",
      "Epoch 46/200\n",
      "242/242 [==============================] - 0s 451us/sample - loss: 0.1517 - val_loss: 0.1494\n",
      "Epoch 47/200\n",
      "242/242 [==============================] - 0s 497us/sample - loss: 0.1523 - val_loss: 0.1485\n",
      "Epoch 48/200\n",
      "242/242 [==============================] - 0s 658us/sample - loss: 0.1543 - val_loss: 0.1496\n",
      "Epoch 49/200\n",
      "242/242 [==============================] - 0s 441us/sample - loss: 0.1526 - val_loss: 0.1495\n",
      "Epoch 50/200\n",
      "242/242 [==============================] - 0s 564us/sample - loss: 0.1523 - val_loss: 0.1491\n",
      "Epoch 51/200\n",
      "242/242 [==============================] - 0s 492us/sample - loss: 0.1540 - val_loss: 0.1499\n",
      "Epoch 52/200\n",
      "242/242 [==============================] - 0s 464us/sample - loss: 0.1520 - val_loss: 0.1496\n",
      "Epoch 53/200\n",
      "242/242 [==============================] - 0s 485us/sample - loss: 0.1516 - val_loss: 0.1492\n",
      "Epoch 54/200\n",
      "242/242 [==============================] - 0s 460us/sample - loss: 0.1519 - val_loss: 0.1492\n",
      "Epoch 55/200\n",
      "242/242 [==============================] - 0s 463us/sample - loss: 0.1514 - val_loss: 0.1494\n",
      "Epoch 56/200\n",
      "242/242 [==============================] - 0s 473us/sample - loss: 0.1518 - val_loss: 0.1498\n",
      "Epoch 57/200\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1520 - val_loss: 0.1497\n",
      "Epoch 58/200\n",
      "242/242 [==============================] - 0s 452us/sample - loss: 0.1517 - val_loss: 0.1501\n",
      "Epoch 59/200\n",
      "242/242 [==============================] - 0s 457us/sample - loss: 0.1518 - val_loss: 0.1499\n",
      "Epoch 60/200\n",
      "242/242 [==============================] - 0s 445us/sample - loss: 0.1535 - val_loss: 0.1510\n",
      "Epoch 61/200\n",
      "242/242 [==============================] - 0s 450us/sample - loss: 0.1515 - val_loss: 0.1499\n",
      "Epoch 62/200\n",
      "242/242 [==============================] - 0s 476us/sample - loss: 0.1513 - val_loss: 0.1498\n",
      "Epoch 63/200\n",
      "242/242 [==============================] - 0s 438us/sample - loss: 0.1537 - val_loss: 0.1509\n",
      "Epoch 64/200\n",
      "242/242 [==============================] - 0s 452us/sample - loss: 0.1537 - val_loss: 0.1494\n",
      "Epoch 65/200\n",
      "242/242 [==============================] - 0s 460us/sample - loss: 0.1512 - val_loss: 0.1495\n",
      "Epoch 66/200\n",
      "242/242 [==============================] - 0s 453us/sample - loss: 0.1511 - val_loss: 0.1496\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 459us/sample - loss: 0.1509 - val_loss: 0.1494\n",
      "Epoch 68/200\n",
      "242/242 [==============================] - 0s 457us/sample - loss: 0.1511 - val_loss: 0.1496\n",
      "Epoch 69/200\n",
      "242/242 [==============================] - 0s 464us/sample - loss: 0.1512 - val_loss: 0.1496\n",
      "Epoch 70/200\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1511 - val_loss: 0.1499\n",
      "Epoch 71/200\n",
      "242/242 [==============================] - 0s 480us/sample - loss: 0.1514 - val_loss: 0.1496\n",
      "Epoch 72/200\n",
      "242/242 [==============================] - 0s 487us/sample - loss: 0.1511 - val_loss: 0.1499\n",
      "Epoch 73/200\n",
      "242/242 [==============================] - 0s 601us/sample - loss: 0.1512 - val_loss: 0.1500\n",
      "Epoch 74/200\n",
      "242/242 [==============================] - 0s 465us/sample - loss: 0.1509 - val_loss: 0.1501\n",
      "Epoch 75/200\n",
      "242/242 [==============================] - 0s 664us/sample - loss: 0.1511 - val_loss: 0.1501\n",
      "Epoch 76/200\n",
      "242/242 [==============================] - 0s 427us/sample - loss: 0.1521 - val_loss: 0.1496\n",
      "Epoch 77/200\n",
      "242/242 [==============================] - 0s 454us/sample - loss: 0.1523 - val_loss: 0.1496\n",
      "Epoch 78/200\n",
      "242/242 [==============================] - 0s 453us/sample - loss: 0.1506 - val_loss: 0.1501\n",
      "Epoch 79/200\n",
      "242/242 [==============================] - 0s 458us/sample - loss: 0.1535 - val_loss: 0.1512\n",
      "Epoch 80/200\n",
      "242/242 [==============================] - 0s 444us/sample - loss: 0.1504 - val_loss: 0.1494\n",
      "Epoch 81/200\n",
      "242/242 [==============================] - 0s 454us/sample - loss: 0.1508 - val_loss: 0.1497\n",
      "Epoch 82/200\n",
      "242/242 [==============================] - 0s 455us/sample - loss: 0.1511 - val_loss: 0.1501\n",
      "Epoch 83/200\n",
      "242/242 [==============================] - 0s 447us/sample - loss: 0.1505 - val_loss: 0.1501\n",
      "Epoch 84/200\n",
      "242/242 [==============================] - 0s 427us/sample - loss: 0.1508 - val_loss: 0.1495\n",
      "Epoch 85/200\n",
      "242/242 [==============================] - 0s 456us/sample - loss: 0.1520 - val_loss: 0.1508\n",
      "Epoch 86/200\n",
      "242/242 [==============================] - 0s 459us/sample - loss: 0.1501 - val_loss: 0.1495\n",
      "Epoch 87/200\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.1502 - val_loss: 0.1500\n",
      "Epoch 88/200\n",
      "242/242 [==============================] - 0s 459us/sample - loss: 0.1504 - val_loss: 0.1503\n",
      "Epoch 89/200\n",
      "242/242 [==============================] - 0s 455us/sample - loss: 0.1522 - val_loss: 0.1498\n",
      "Epoch 90/200\n",
      "242/242 [==============================] - 0s 439us/sample - loss: 0.1502 - val_loss: 0.1495\n",
      "Epoch 91/200\n",
      "242/242 [==============================] - 0s 447us/sample - loss: 0.1517 - val_loss: 0.1498\n",
      "Epoch 92/200\n",
      "242/242 [==============================] - 0s 448us/sample - loss: 0.1502 - val_loss: 0.1495\n",
      "Epoch 93/200\n",
      "242/242 [==============================] - 0s 448us/sample - loss: 0.1520 - val_loss: 0.1508\n",
      "Epoch 94/200\n",
      "242/242 [==============================] - 0s 448us/sample - loss: 0.1501 - val_loss: 0.1495\n",
      "Epoch 95/200\n",
      "242/242 [==============================] - 0s 451us/sample - loss: 0.1500 - val_loss: 0.1495\n",
      "Epoch 96/200\n",
      "242/242 [==============================] - 0s 436us/sample - loss: 0.1503 - val_loss: 0.1500\n",
      "Epoch 97/200\n",
      "242/242 [==============================] - 0s 480us/sample - loss: 0.1504 - val_loss: 0.1499\n",
      "Epoch 98/200\n",
      "242/242 [==============================] - 0s 710us/sample - loss: 0.1502 - val_loss: 0.1504\n",
      "Epoch 99/200\n",
      "242/242 [==============================] - 0s 443us/sample - loss: 0.1515 - val_loss: 0.1500\n",
      "Epoch 100/200\n",
      "242/242 [==============================] - 0s 512us/sample - loss: 0.1500 - val_loss: 0.1502\n",
      "Epoch 101/200\n",
      "242/242 [==============================] - 0s 564us/sample - loss: 0.1513 - val_loss: 0.1501\n",
      "Epoch 102/200\n",
      "242/242 [==============================] - 0s 466us/sample - loss: 0.1500 - val_loss: 0.1501\n",
      "Epoch 103/200\n",
      "242/242 [==============================] - 0s 435us/sample - loss: 0.1502 - val_loss: 0.1500\n",
      "Epoch 104/200\n",
      "242/242 [==============================] - 0s 465us/sample - loss: 0.1502 - val_loss: 0.1505\n",
      "Epoch 105/200\n",
      "242/242 [==============================] - 0s 457us/sample - loss: 0.1502 - val_loss: 0.1502\n",
      "Epoch 106/200\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1511 - val_loss: 0.1504\n",
      "Epoch 107/200\n",
      "242/242 [==============================] - 0s 434us/sample - loss: 0.1500 - val_loss: 0.1502\n",
      "Epoch 108/200\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1498 - val_loss: 0.1507\n",
      "Epoch 109/200\n",
      "242/242 [==============================] - 0s 457us/sample - loss: 0.1503 - val_loss: 0.1510\n",
      "Epoch 110/200\n",
      "242/242 [==============================] - 0s 454us/sample - loss: 0.1516 - val_loss: 0.1503\n",
      "Epoch 111/200\n",
      "242/242 [==============================] - 0s 451us/sample - loss: 0.1516 - val_loss: 0.1505\n",
      "Epoch 112/200\n",
      "242/242 [==============================] - 0s 448us/sample - loss: 0.1517 - val_loss: 0.1502\n",
      "Epoch 113/200\n",
      "242/242 [==============================] - 0s 445us/sample - loss: 0.1513 - val_loss: 0.1504\n",
      "Epoch 114/200\n",
      "242/242 [==============================] - 0s 433us/sample - loss: 0.1518 - val_loss: 0.1515\n",
      "Epoch 115/200\n",
      "242/242 [==============================] - 0s 442us/sample - loss: 0.1518 - val_loss: 0.1516\n",
      "Epoch 116/200\n",
      "242/242 [==============================] - 0s 441us/sample - loss: 0.1515 - val_loss: 0.1502\n",
      "Epoch 117/200\n",
      "242/242 [==============================] - 0s 441us/sample - loss: 0.1516 - val_loss: 0.1503\n",
      "Epoch 118/200\n",
      "242/242 [==============================] - 0s 439us/sample - loss: 0.1499 - val_loss: 0.1504\n",
      "Epoch 119/200\n",
      "242/242 [==============================] - 0s 436us/sample - loss: 0.1510 - val_loss: 0.1501\n",
      "Epoch 120/200\n",
      "242/242 [==============================] - 0s 438us/sample - loss: 0.1513 - val_loss: 0.1503\n",
      "Epoch 121/200\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.1497 - val_loss: 0.1500\n",
      "Epoch 122/200\n",
      "242/242 [==============================] - 0s 470us/sample - loss: 0.1497 - val_loss: 0.1502\n",
      "Epoch 123/200\n",
      "242/242 [==============================] - 0s 452us/sample - loss: 0.1499 - val_loss: 0.1500\n",
      "Epoch 124/200\n",
      "242/242 [==============================] - 0s 705us/sample - loss: 0.1498 - val_loss: 0.1503\n",
      "Epoch 125/200\n",
      "242/242 [==============================] - 0s 456us/sample - loss: 0.1498 - val_loss: 0.1506\n",
      "Epoch 126/200\n",
      "242/242 [==============================] - 0s 522us/sample - loss: 0.1500 - val_loss: 0.1506\n",
      "Epoch 127/200\n",
      "242/242 [==============================] - 0s 546us/sample - loss: 0.1496 - val_loss: 0.1507\n",
      "Epoch 128/200\n",
      "242/242 [==============================] - 0s 455us/sample - loss: 0.1498 - val_loss: 0.1508\n",
      "Epoch 129/200\n",
      "242/242 [==============================] - 0s 458us/sample - loss: 0.1508 - val_loss: 0.1510\n",
      "Epoch 130/200\n",
      "242/242 [==============================] - 0s 461us/sample - loss: 0.1495 - val_loss: 0.1506\n",
      "Epoch 131/200\n",
      "242/242 [==============================] - 0s 447us/sample - loss: 0.1499 - val_loss: 0.1509\n",
      "Epoch 132/200\n",
      "242/242 [==============================] - 0s 451us/sample - loss: 0.1498 - val_loss: 0.1507\n",
      "Epoch 133/200\n",
      "242/242 [==============================] - 0s 452us/sample - loss: 0.1495 - val_loss: 0.1508\n",
      "Epoch 134/200\n",
      "242/242 [==============================] - 0s 448us/sample - loss: 0.1495 - val_loss: 0.1512\n",
      "Epoch 135/200\n",
      "242/242 [==============================] - 0s 450us/sample - loss: 0.1496 - val_loss: 0.1511\n",
      "Epoch 136/200\n",
      "242/242 [==============================] - 0s 455us/sample - loss: 0.1498 - val_loss: 0.1512\n",
      "Epoch 137/200\n",
      "242/242 [==============================] - 0s 461us/sample - loss: 0.1504 - val_loss: 0.1510\n",
      "Epoch 138/200\n",
      "242/242 [==============================] - 0s 444us/sample - loss: 0.1495 - val_loss: 0.1514\n",
      "Epoch 139/200\n",
      "242/242 [==============================] - 0s 477us/sample - loss: 0.1498 - val_loss: 0.1512\n",
      "Epoch 140/200\n",
      "242/242 [==============================] - 0s 457us/sample - loss: 0.1497 - val_loss: 0.1511\n",
      "Epoch 141/200\n",
      "242/242 [==============================] - 0s 441us/sample - loss: 0.1505 - val_loss: 0.1509\n",
      "Epoch 142/200\n",
      "242/242 [==============================] - 0s 433us/sample - loss: 0.1495 - val_loss: 0.1509\n",
      "Epoch 143/200\n",
      "242/242 [==============================] - 0s 462us/sample - loss: 0.1506 - val_loss: 0.1510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.1506 - val_loss: 0.1507\n",
      "Epoch 145/200\n",
      "242/242 [==============================] - 0s 456us/sample - loss: 0.1496 - val_loss: 0.1507\n",
      "Epoch 146/200\n",
      "242/242 [==============================] - 0s 438us/sample - loss: 0.1493 - val_loss: 0.1509\n",
      "Epoch 147/200\n",
      "242/242 [==============================] - 0s 465us/sample - loss: 0.1494 - val_loss: 0.1506\n",
      "Epoch 148/200\n",
      "242/242 [==============================] - 0s 484us/sample - loss: 0.1493 - val_loss: 0.1508\n",
      "Epoch 149/200\n",
      "242/242 [==============================] - 0s 682us/sample - loss: 0.1501 - val_loss: 0.1510\n",
      "Epoch 150/200\n",
      "242/242 [==============================] - 0s 459us/sample - loss: 0.1505 - val_loss: 0.1509\n",
      "Epoch 151/200\n",
      "242/242 [==============================] - 0s 489us/sample - loss: 0.1493 - val_loss: 0.1516\n",
      "Epoch 152/200\n",
      "242/242 [==============================] - 0s 586us/sample - loss: 0.1491 - val_loss: 0.1509\n",
      "Epoch 153/200\n",
      "242/242 [==============================] - 0s 459us/sample - loss: 0.1495 - val_loss: 0.1512\n",
      "Epoch 154/200\n",
      "242/242 [==============================] - 0s 469us/sample - loss: 0.1492 - val_loss: 0.1509\n",
      "Epoch 155/200\n",
      "242/242 [==============================] - 0s 454us/sample - loss: 0.1492 - val_loss: 0.1506\n",
      "Epoch 156/200\n",
      "242/242 [==============================] - 0s 452us/sample - loss: 0.1504 - val_loss: 0.1510\n",
      "Epoch 157/200\n",
      "242/242 [==============================] - 0s 450us/sample - loss: 0.1492 - val_loss: 0.1507\n",
      "Epoch 158/200\n",
      "242/242 [==============================] - 0s 454us/sample - loss: 0.1494 - val_loss: 0.1509\n",
      "Epoch 159/200\n",
      "242/242 [==============================] - 0s 463us/sample - loss: 0.1501 - val_loss: 0.1509\n",
      "Epoch 160/200\n",
      "242/242 [==============================] - 0s 438us/sample - loss: 0.1491 - val_loss: 0.1510\n",
      "Epoch 161/200\n",
      "242/242 [==============================] - 0s 452us/sample - loss: 0.1492 - val_loss: 0.1508\n",
      "Epoch 162/200\n",
      "242/242 [==============================] - 0s 439us/sample - loss: 0.1491 - val_loss: 0.1510\n",
      "Epoch 163/200\n",
      "242/242 [==============================] - 0s 453us/sample - loss: 0.1492 - val_loss: 0.1507\n",
      "Epoch 164/200\n",
      "242/242 [==============================] - 0s 457us/sample - loss: 0.1498 - val_loss: 0.1510\n",
      "Epoch 165/200\n",
      "242/242 [==============================] - 0s 437us/sample - loss: 0.1492 - val_loss: 0.1515\n",
      "Epoch 166/200\n",
      "242/242 [==============================] - 0s 458us/sample - loss: 0.1489 - val_loss: 0.1510\n",
      "Epoch 167/200\n",
      "242/242 [==============================] - 0s 438us/sample - loss: 0.1491 - val_loss: 0.1509\n",
      "Epoch 168/200\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1499 - val_loss: 0.1509\n",
      "Epoch 169/200\n",
      "242/242 [==============================] - 0s 437us/sample - loss: 0.1503 - val_loss: 0.1509\n",
      "Epoch 170/200\n",
      "242/242 [==============================] - 0s 442us/sample - loss: 0.1494 - val_loss: 0.1507\n",
      "Epoch 171/200\n",
      "242/242 [==============================] - 0s 442us/sample - loss: 0.1492 - val_loss: 0.1508\n",
      "Epoch 172/200\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.1499 - val_loss: 0.1515\n",
      "Epoch 173/200\n",
      "242/242 [==============================] - 0s 443us/sample - loss: 0.1488 - val_loss: 0.1513\n",
      "Epoch 174/200\n",
      "242/242 [==============================] - 0s 586us/sample - loss: 0.1491 - val_loss: 0.1516\n",
      "Epoch 175/200\n",
      "242/242 [==============================] - 0s 546us/sample - loss: 0.1487 - val_loss: 0.1515\n",
      "Epoch 176/200\n",
      "242/242 [==============================] - 0s 462us/sample - loss: 0.1499 - val_loss: 0.1510\n",
      "Epoch 177/200\n",
      "242/242 [==============================] - 0s 680us/sample - loss: 0.1502 - val_loss: 0.1508\n",
      "Epoch 178/200\n",
      "242/242 [==============================] - 0s 430us/sample - loss: 0.1491 - val_loss: 0.1508\n",
      "Epoch 179/200\n",
      "242/242 [==============================] - 0s 463us/sample - loss: 0.1500 - val_loss: 0.1505\n",
      "Epoch 180/200\n",
      "242/242 [==============================] - 0s 461us/sample - loss: 0.1488 - val_loss: 0.1507\n",
      "Epoch 181/200\n",
      "242/242 [==============================] - 0s 444us/sample - loss: 0.1490 - val_loss: 0.1506\n",
      "Epoch 182/200\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.1492 - val_loss: 0.1508\n",
      "Epoch 183/200\n",
      "242/242 [==============================] - 0s 451us/sample - loss: 0.1499 - val_loss: 0.1508\n",
      "Epoch 184/200\n",
      "242/242 [==============================] - 0s 438us/sample - loss: 0.1489 - val_loss: 0.1507\n",
      "Epoch 185/200\n",
      "242/242 [==============================] - 0s 451us/sample - loss: 0.1491 - val_loss: 0.1507\n",
      "Epoch 186/200\n",
      "242/242 [==============================] - 0s 444us/sample - loss: 0.1489 - val_loss: 0.1507\n",
      "Epoch 187/200\n",
      "242/242 [==============================] - 0s 450us/sample - loss: 0.1497 - val_loss: 0.1508\n",
      "Epoch 188/200\n",
      "242/242 [==============================] - 0s 452us/sample - loss: 0.1498 - val_loss: 0.1513\n",
      "Epoch 189/200\n",
      "242/242 [==============================] - 0s 438us/sample - loss: 0.1486 - val_loss: 0.1511\n",
      "Epoch 190/200\n",
      "242/242 [==============================] - 0s 463us/sample - loss: 0.1486 - val_loss: 0.1511\n",
      "Epoch 191/200\n",
      "242/242 [==============================] - 0s 439us/sample - loss: 0.1495 - val_loss: 0.1504\n",
      "Epoch 192/200\n",
      "242/242 [==============================] - 0s 433us/sample - loss: 0.1494 - val_loss: 0.1510\n",
      "Epoch 193/200\n",
      "242/242 [==============================] - 0s 440us/sample - loss: 0.1485 - val_loss: 0.1510\n",
      "Epoch 194/200\n",
      "242/242 [==============================] - 0s 443us/sample - loss: 0.1496 - val_loss: 0.1507\n",
      "Epoch 195/200\n",
      "242/242 [==============================] - 0s 440us/sample - loss: 0.1496 - val_loss: 0.1504\n",
      "Epoch 196/200\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1499 - val_loss: 0.1505\n",
      "Epoch 197/200\n",
      "242/242 [==============================] - 0s 440us/sample - loss: 0.1488 - val_loss: 0.1506\n",
      "Epoch 198/200\n",
      "242/242 [==============================] - 0s 457us/sample - loss: 0.1498 - val_loss: 0.1511\n",
      "Epoch 199/200\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1484 - val_loss: 0.1504\n",
      "Epoch 200/200\n",
      "242/242 [==============================] - 0s 453us/sample - loss: 0.1489 - val_loss: 0.1503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc9d778d860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python3\n",
    "\"\"\"Example usage of TFL within Keras models.\n",
    "This example builds and trains a calibrated lattice model for the UCI heart\n",
    "dataset.\n",
    "\"Calibrated lattice\" is a commonly used architecture for datasets where number\n",
    "of input features does not exceed ~15.\n",
    "\"Calibrated lattice\" assumes every feature being transformed by PWLCalibration\n",
    "or CategoricalCalibration layers before nonlineary fusing result of calibration\n",
    "within a lattice layer.\n",
    "Generally when you manually combine TFL layers you should keep track of:\n",
    "1) Ensuring that inputs to TFL layers are within expected range.\n",
    "  - Input range for PWLCalibration layer is defined by smallest and largest of\n",
    "    provided keypoints.\n",
    "  - Input range for Lattice layer is [0.0, lattice_sizes[d] - 1.0] for any\n",
    "    dimension d.\n",
    "  TFL layers can constraint their output to be within desired range. Feeding\n",
    "  output of other layers into TFL layers you might want to ensure that something\n",
    "  like sigmoid is used to constraint their output range.\n",
    "2) Properly configure monotonicity. If your calibration layer is monotonic then\n",
    "  corresponding dimension of lattice layer should also be monotonic.\n",
    "This example creates a Sequential Keras model and only uses TFL layers. For an\n",
    "example of functional model construction that also use embedding layers see\n",
    "keras_functional_uci_heart.py.\n",
    "In order to see how better generalization can be achieved with a properly\n",
    "constrained PWLCalibration layer compared to a vanila embedding layer, compare\n",
    "training and validation losses of this model with one defined in\n",
    "keras_functional_uci_heart.py\n",
    "Note that the specifics of layer configurations are for demonstration purposes\n",
    "and might not result in optimal performance.\n",
    "Example usage:\n",
    "keras_sequential_uci_heart\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_lattice as tfl\n",
    "\n",
    "# UCI Statlog (Heart) dataset.\n",
    "csv_file = tf.keras.utils.get_file(\n",
    "  'heart.csv', 'http://storage.googleapis.com/applied-dl/heart.csv')\n",
    "training_data_df = pd.read_csv(csv_file).sample(\n",
    "  frac=1.0, random_state=41).reset_index(drop=True)\n",
    "\n",
    "# Feature columns.\n",
    "# 0  age\n",
    "# 1  sex\n",
    "# 2  cp        chest pain type (4 values)\n",
    "# 3  trestbps  resting blood pressure\n",
    "# 4  chol      serum cholestoral in mg/dl\n",
    "# 5  fbs       fasting blood sugar > 120 mg/dl\n",
    "# 6  restecg   resting electrocardiographic results (values 0,1,2)\n",
    "# 7  thalach   maximum heart rate achieved\n",
    "# 8  exang     exercise induced angina\n",
    "# 9  oldpeak   ST depression induced by exercise relative to rest\n",
    "# 10 slope     the slope of the peak exercise ST segment\n",
    "# 11 ca        number of major vessels (0-3) colored by flourosopy\n",
    "# 12 thal      3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "# Example slice of training data:\n",
    "#     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak\n",
    "# 0   63    1   1       145   233    1        2      150      0      2.3\n",
    "# 1   67    1   4       160   286    0        2      108      1      1.5\n",
    "# 2   67    1   4       120   229    0        2      129      1      2.6\n",
    "# 3   37    1   3       130   250    0        0      187      0      3.5\n",
    "# 4   41    0   2       130   204    0        2      172      0      1.4\n",
    "# 5   56    1   2       120   236    0        0      178      0      0.8\n",
    "# 6   62    0   4       140   268    0        2      160      0      3.6\n",
    "# 7   57    0   4       120   354    0        0      163      1      0.6\n",
    "# 8   63    1   4       130   254    0        2      147      0      1.4\n",
    "# 9   53    1   4       140   203    1        2      155      1      3.1\n",
    "\n",
    "# Lattice sizes per dimension for Lattice layer.\n",
    "# Lattice layer expects input[i] to be within [0, lattice_sizes[i] - 1.0], so\n",
    "# we need to define lattice sizes ahead of calibration layers so we can\n",
    "# properly specify output range of calibration layers.\n",
    "lattice_sizes = [3, 2, 2, 2, 2, 2, 2]\n",
    "\n",
    "# Use ParallelCombination helper layer to group togehter calibration layers\n",
    "# which have to be executed in paralel in order to be able to use Sequential\n",
    "# model. Alternatively you can use functional API.\n",
    "combined_calibrators = tfl.layers.ParallelCombination()\n",
    "\n",
    "# Configure calibration layers for every feature:\n",
    "\n",
    "# ############### age ###############\n",
    "\n",
    "calibrator = tfl.layers.PWLCalibration(\n",
    "  # Every PWLCalibration layer must have keypoints of piecewise linear\n",
    "  # function specified. Easiest way to specify them is to uniformly cover\n",
    "  # entire input range by using numpy.linspace().\n",
    "  input_keypoints=np.linspace(training_data_df['age'].min(),\n",
    "                              training_data_df['age'].max(),\n",
    "                              num=5),\n",
    "  # You need to ensure that input keypoints have same dtype as layer input.\n",
    "  # You can do it by setting dtype here or by providing keypoints in such\n",
    "  # format which will be converted to deisred tf.dtype by default.\n",
    "  dtype=tf.float32,\n",
    "  # Output range must correspond to expected lattice input range.\n",
    "  output_min=0.0,\n",
    "  output_max=lattice_sizes[0] - 1.0,\n",
    "  monotonicity='increasing')\n",
    "combined_calibrators.append(calibrator)\n",
    "\n",
    "# ############### sex ###############\n",
    "\n",
    "# For boolean features simply specify CategoricalCalibration layer with 2\n",
    "# buckets.\n",
    "calibrator = tfl.layers.CategoricalCalibration(\n",
    "  num_buckets=2,\n",
    "  output_min=0.0,\n",
    "  output_max=lattice_sizes[1] - 1.0,\n",
    "  # Initializes all outputs to (output_min + output_max) / 2.0.\n",
    "  kernel_initializer='constant')\n",
    "combined_calibrators.append(calibrator)\n",
    "\n",
    "# ############### cp ###############\n",
    "\n",
    "calibrator = tfl.layers.PWLCalibration(\n",
    "  # Here instead of specifying dtype of layer we convert keypoints into\n",
    "  # np.float32.\n",
    "  input_keypoints=np.linspace(1, 4, num=4, dtype=np.float32),\n",
    "  output_min=0.0,\n",
    "  output_max=lattice_sizes[2] - 1.0,\n",
    "  monotonicity='increasing',\n",
    "  # You can specify TFL regularizers as tuple ('regularizer name', l1, l2).\n",
    "  kernel_regularizer=('hessian', 0.0, 1e-4))\n",
    "combined_calibrators.append(calibrator)\n",
    "\n",
    "# ############### trestbps ###############\n",
    "\n",
    "calibrator = tfl.layers.PWLCalibration(\n",
    "  # Alternatively to uniform keypoints you might want to use quantiles as\n",
    "  # keypoints.\n",
    "  input_keypoints=np.quantile(\n",
    "      training_data_df['trestbps'], np.linspace(0.0, 1.0, num=5)),\n",
    "  dtype=tf.float32,\n",
    "  # Together with quantile keypoints you might want to initialize piecewise\n",
    "  # linear function to have 'equal_slopes' in order for output of layer\n",
    "  # after initialization to preserve original distribution.\n",
    "  kernel_initializer='equal_slopes',\n",
    "  output_min=0.0,\n",
    "  output_max=lattice_sizes[3] - 1.0,\n",
    "  # You might consider clamping extreme inputs of the calibrator to output\n",
    "  # bounds.\n",
    "  clamp_min=True,\n",
    "  clamp_max=True,\n",
    "  monotonicity='increasing')\n",
    "combined_calibrators.append(calibrator)\n",
    "\n",
    "# ############### chol ###############\n",
    "\n",
    "calibrator = tfl.layers.PWLCalibration(\n",
    "  # Explicit input keypoint initialization.\n",
    "  input_keypoints=[126.0, 210.0, 247.0, 286.0, 564.0],\n",
    "  dtype=tf.float32,\n",
    "  output_min=0.0,\n",
    "  output_max=lattice_sizes[4] - 1.0,\n",
    "  # Monotonicity of calibrator can be 'decreasing'. Note that corresponding\n",
    "  # lattice dimension must have 'increasing' monotonicity regardless of\n",
    "  # monotonicity direction of calibrator.\n",
    "  # Its not some weird configuration hack. Its just how math works :)\n",
    "  monotonicity='decreasing',\n",
    "  # Convexity together with decreasing monotonicity result in diminishing\n",
    "  # return constraint.\n",
    "  convexity='convex',\n",
    "  # You can specify list of regularizers. You are not limited to TFL\n",
    "  # regularizrs. Feel free to use any :)\n",
    "  kernel_regularizer=[('laplacian', 0.0, 1e-4),\n",
    "                      keras.regularizers.l1_l2(l1=0.001)])\n",
    "combined_calibrators.append(calibrator)\n",
    "\n",
    "# ############### fbs ###############\n",
    "\n",
    "calibrator = tfl.layers.CategoricalCalibration(\n",
    "  num_buckets=2,\n",
    "  output_min=0.0,\n",
    "  output_max=lattice_sizes[5] - 1.0,\n",
    "  # For categorical calibration layer monotonicity is specified for pairs\n",
    "  # of indices of categories. Output for first category in pair will be\n",
    "  # smaller than output for second category.\n",
    "  #\n",
    "  # Don't forget to set monotonicity of corresponding dimension of Lattice\n",
    "  # layer to 'increasing'.\n",
    "  monotonicities=[(0, 1)],\n",
    "  # This initializer is identical to default one('uniform'), but has fixed\n",
    "  # seed in order to simplify experimentation.\n",
    "  kernel_initializer=keras.initializers.RandomUniform(\n",
    "      minval=0.0, maxval=lattice_sizes[5] - 1.0, seed=1))\n",
    "combined_calibrators.append(calibrator)\n",
    "\n",
    "# ############### restecg ###############\n",
    "\n",
    "calibrator = tfl.layers.CategoricalCalibration(\n",
    "  num_buckets=3,\n",
    "  output_min=0.0,\n",
    "  output_max=lattice_sizes[6] - 1.0,\n",
    "  # Categorical monotonicity can be partial order.\n",
    "  monotonicities=[(0, 1), (0, 2)],\n",
    "  # Categorical calibration layer supports standard Keras regularizers.\n",
    "  kernel_regularizer=keras.regularizers.l1_l2(l1=0.001),\n",
    "  kernel_initializer='constant')\n",
    "combined_calibrators.append(calibrator)\n",
    "\n",
    "# Create Lattice layer to nonlineary fuse output of calibrators. Don't forget\n",
    "# to specify monotonicity 'increasing' for any dimension which calibrator is\n",
    "# monotonic regardless of monotonicity direction of calibrator. This includes\n",
    "# partial monotonicity of CategoricalCalibration layer.\n",
    "lattice = tfl.layers.Lattice(\n",
    "  lattice_sizes=lattice_sizes,\n",
    "  monotonicities=['increasing', 'none', 'increasing', 'increasing',\n",
    "                  'increasing', 'increasing', 'increasing'],\n",
    "  output_min=0.0,\n",
    "  output_max=1.0)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "# We have just 2 layer as far as Sequential model is concerned.\n",
    "# PWLConcatenate layer takes care of grouping calibrators.\n",
    "model.add(combined_calibrators)\n",
    "model.add(lattice)\n",
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "            optimizer=keras.optimizers.Adagrad(learning_rate=1.0))\n",
    "\n",
    "features = training_data_df[\n",
    "  ['age', 'sex', 'cp',\n",
    "   'trestbps', 'chol', 'fbs', 'restecg']].values.astype(np.float32)\n",
    "target = training_data_df[['target']].values.astype(np.float32)\n",
    "\n",
    "model.fit(features,\n",
    "        target,\n",
    "        batch_size=32,\n",
    "        epochs=200,\n",
    "        validation_split=0.2,\n",
    "        shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
