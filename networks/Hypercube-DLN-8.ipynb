{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_lattice as tfl\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import helpers\n",
    "import import_ipynb\n",
    "from common import buildDatasetForLattice\n",
    "from common import scaleVolume\n",
    "from common import extractLatticeWeights\n",
    "from common import dropColumns\n",
    "from common import filterBad\n",
    "from common import splitDataset\n",
    "from common import normDataset\n",
    "from common import evaluatePerf\n",
    "from common import evaluateCustom\n",
    "from common import extractXGWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected 2017140 points (18.504836%)\n"
     ]
    }
   ],
   "source": [
    "dataset, columns = buildDatasetForLattice()\n",
    "dataset = filterBad(dataset)\n",
    "train_dataset, test_dataset, train_labels, test_labels = splitDataset(dataset, 0.2)\n",
    "train_stats = train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = dataset.copy()\n",
    "col = preprocessed_dataset.first_val\n",
    "preprocessed_dataset.first_val = np.log(col + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_initializers = {\n",
    "    'row': \"quantile\", 'rising_idx': \"quantile\", 'falling_idx': \"quantile\", 'first_val': \"uniform\", 'last_val': \"uniform\"\n",
    "}\n",
    "monotonicities = {\n",
    "    \"rising_idx\": -1,\n",
    "    \"falling_idx\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLattice(num_keypoints, lattice_size, columns, num_lattices):\n",
    "    inputs = [keras.layers.Input(shape=[1]) for _ in columns]\n",
    "    combined_calibrators = []\n",
    "    for inpt, ft in zip(inputs, columns):\n",
    "        if ft != \"row\":\n",
    "            if kp_initializers[ft] == \"quantile\":\n",
    "                quantile_vals = [i/(num_keypoints - 1.0) for i in range(num_keypoints)]\n",
    "                keypoints = dataset[ft].quantile(quantile_vals).values\n",
    "            if kp_initializers[ft] == \"uniform\":\n",
    "                keypoints = np.linspace(preprocessed_dataset[ft].min(), preprocessed_dataset[ft].max(), num=num_keypoints)\n",
    "\n",
    "            calibrator = tfl.layers.PWLCalibration(\n",
    "                input_keypoints=keypoints, dtype=tf.float32, output_min=0.0, output_max=lattice_size - 1.0,\n",
    "                kernel_regularizer = [(\"wrinkle\", 1e-4, 1e-5)],\n",
    "                monotonicity=monotonicities.get(ft, 0),\n",
    "                units = num_lattices\n",
    "            )(inpt)\n",
    "        else:\n",
    "            # row is categorical\n",
    "            calibrator = tfl.layers.CategoricalCalibration(\n",
    "                num_buckets=preprocessed_dataset[ft].nunique(),\n",
    "                output_min = 0.0,\n",
    "                output_max = lattice_size - 1.0,\n",
    "                units = num_lattices\n",
    "            )(inpt)\n",
    "        combined_calibrators.append(calibrator)\n",
    "    lattices = []\n",
    "    for i in range(num_lattices):\n",
    "        lattice_inputs = []\n",
    "        for col in range(len(columns)):\n",
    "            lattice_inputs.append(\n",
    "                keras.backend.transpose(\n",
    "                    keras.backend.gather(\n",
    "                        keras.backend.transpose(combined_calibrators[col]), [i])))\n",
    "        lattice = tfl.layers.Lattice(\n",
    "            lattice_sizes=[lattice_size for _ in columns],\n",
    "            monotonicities=['increasing' if (ft == 'rising_idx' or ft == 'falling_idx') else 'none' for x in columns],\n",
    "            output_min=dataset['delay'].min(),\n",
    "            output_max=dataset['delay'].max())(keras.layers.concatenate(lattice_inputs))\n",
    "        lattices.append(lattice)\n",
    "    \n",
    "    if num_lattices == 1:\n",
    "        model = keras.models.Model(inputs=inputs, outputs=lattices[0])\n",
    "    else:\n",
    "        model = keras.models.Model(inputs=inputs, outputs=tfl.layers.Linear(\n",
    "            num_input_dims=num_lattices, normalization_order=1,\n",
    "            kernel_regularizer=keras.regularizers.l1(0.01))(keras.layers.concatenate(lattices)))\n",
    "    model.compile(loss=keras.losses.mean_absolute_error,\n",
    "                optimizer=keras.optimizers.Adam(), metrics=[\"mse\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLattice(model, cols, epochs):\n",
    "    features = [preprocessed_dataset[col].values for col in cols]\n",
    "    target = preprocessed_dataset[\"delay\"]\n",
    "\n",
    "    model.fit(features,\n",
    "            target,\n",
    "            batch_size=32,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.2,\n",
    "            shuffle=True, workers=32, use_multiprocessing=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7106776 samples, validate on 1776694 samples\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanfurd/lattice_experiments/lattice_env/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/stanfurd/lattice_experiments/lattice_env/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145728/7106776 [====================>.........] - ETA: 9:51 - loss: 119.9670 - mse: 54221.2422"
     ]
    }
   ],
   "source": [
    "show_perf = True\n",
    "train_model = True\n",
    "\n",
    "num_kps = 16\n",
    "num_lattices = 8\n",
    "model_path = f\"./Hypercube_{num_kps}_{num_lattices}\"\n",
    "\n",
    "Hypercube_8_2 = buildLattice(num_kps, 2, columns[:-1], num_lattices)\n",
    "if (train_model):\n",
    "    trainLattice(Hypercube_8_2,columns[:-1], 40) #train\n",
    "else: \n",
    "    Hypercube_8_2.build((None,5,))\n",
    "    Hypercube_8_2.load_weights(model_path) #load\n",
    "Hypercube_8_2.save(model_path)\n",
    "preds = Hypercube_8_2.predict(train_dataset)\n",
    "if (show_perf): evaluatePerf(test_labels, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
