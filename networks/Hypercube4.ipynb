{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_lattice as tfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import helpers\n",
    "import import_ipynb\n",
    "from common import buildDatasetForLattice\n",
    "from common import scaleVolume\n",
    "from common import extractLatticeWeights\n",
    "from common import dropColumns\n",
    "from common import filterBad\n",
    "from common import splitDataset\n",
    "from common import normDataset\n",
    "from common import evaluatePerf\n",
    "from common import evaluateCustom\n",
    "from common import extractXGWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected 6196746 points (56.847699%)\n"
     ]
    }
   ],
   "source": [
    "only_use_strong = 54\n",
    "\n",
    "dataset, columns = buildDatasetForLattice()\n",
    "dataset = filterBad(dataset, only_use_strong)\n",
    "train_dataset, test_dataset, train_labels, test_labels = splitDataset(dataset, 0.2)\n",
    "train_stats = train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['row'], axis=1)\n",
    "columns = [x for x in columns if x != 'row']\n",
    "train_dataset, test_dataset, train_labels, test_labels = splitDataset(dataset, 0.2)\n",
    "train_stats = train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLattice(num_keypoints, lattice_size, columns):\n",
    "    # Use ParallelCombination helper layer to group togehter calibration layers\n",
    "    # which have to be executed in paralel in order to be able to use Sequential\n",
    "    # model. Alternatively you can use functional API.\n",
    "    combined_calibrators = tfl.layers.ParallelCombination()\n",
    "\n",
    "    # Configure calibration layers for every feature:\n",
    "\n",
    "    for ft in columns:\n",
    "        kpts = np.percentile(dataset[ft], np.linspace(0,99,num_keypoints))\n",
    "        for i in range(1,len(kpts)):\n",
    "            while kpts[i] <= kpts[i-1]:\n",
    "                kpts[i] = kpts[i] + 1\n",
    "        calibrator = tfl.layers.PWLCalibration(\n",
    "            # Every PWLCalibration layer must have keypoints of piecewise linear\n",
    "            # function specified. Easiest way to specify them is to uniformly cover\n",
    "            # entire input range by using numpy.linspace().\n",
    "            input_keypoints=kpts,\n",
    "#             input_keypoints=np.linspace(dataset[ft].min(),\n",
    "#                                       dataset[ft].max(),\n",
    "#                                       num=num_keypoints),\n",
    "        # You need to ensure that input keypoints have same dtype as layer input.\n",
    "        # You can do it by setting dtype here or by providing keypoints in such\n",
    "        # format which will be converted to deisred tf.dtype by default.\n",
    "        dtype=tf.float32,\n",
    "        # Output range must correspond to expected lattice input range.\n",
    "        output_min=0.0,\n",
    "        output_max=lattice_size - 1.0,\n",
    "        monotonicity='increasing')\n",
    "        combined_calibrators.append(calibrator)\n",
    "\n",
    "    # Create Lattice layer to nonlineary fuse output of calibrators. Don't forget\n",
    "    # to specify monotonicity 'increasing' for any dimension which calibrator is\n",
    "    # monotonic regardless of monotonicity direction of calibrator. This includes\n",
    "    # partial monotonicity of CategoricalCalibration layer.\n",
    "    lattice = tfl.layers.Lattice(\n",
    "      lattice_sizes=[lattice_size for _ in columns],\n",
    "      monotonicities=['increasing' if (ft == 'rising_idx' or ft == 'falling_idx') else 'none' for x in columns],\n",
    "      output_min=dataset['delay'].min(),\n",
    "      output_max=dataset['delay'].max())\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    # We have just 2 layer as far as Sequential model is concerned.\n",
    "    # PWLConcatenate layer takes care of grouping calibrators.\n",
    "    model.add(combined_calibrators)\n",
    "    model.add(lattice)\n",
    "    model.compile(loss=keras.losses.mean_squared_error,\n",
    "                optimizer=keras.optimizers.Adagrad(learning_rate=80.0))\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLattice(model, modelName, cols, epochs):\n",
    "    \n",
    "    features = train_dataset[cols].values.astype(np.float32)\n",
    "    target = train_labels.values.astype(np.float32)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('./' + modelName, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "    model.fit(features,\n",
    "            target,\n",
    "            batch_size=32,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.05,\n",
    "            shuffle=False,\n",
    "            callbacks=[checkpoint])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3574936 samples, validate on 188155 samples\n",
      "WARNING:tensorflow:From /home/mattfel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/mattfel/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/30\n",
      "3574912/3574936 [============================>.] - ETA: 0s - loss: 184336.1377WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 999s 280us/sample - loss: 184335.0286 - val_loss: 333961.7230\n",
      "Epoch 2/30\n",
      "3574656/3574936 [============================>.] - ETA: 0s - loss: 66569.3832WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 920s 257us/sample - loss: 66570.6110 - val_loss: 148608.1475\n",
      "Epoch 3/30\n",
      "3574816/3574936 [============================>.] - ETA: 0s - loss: 64445.6784WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 798s 223us/sample - loss: 64445.9225 - val_loss: 35954.9167\n",
      "Epoch 4/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 58787.3263WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 961s 269us/sample - loss: 58786.6541 - val_loss: 19518.0023\n",
      "Epoch 5/30\n",
      "3574752/3574936 [============================>.] - ETA: 0s - loss: 39984.6857WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 972s 272us/sample - loss: 39986.8977 - val_loss: 32861.9629\n",
      "Epoch 6/30\n",
      "3574912/3574936 [============================>.] - ETA: 0s - loss: 36209.1974WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 948s 265us/sample - loss: 36209.0206 - val_loss: 27317.4193\n",
      "Epoch 7/30\n",
      "3574848/3574936 [============================>.] - ETA: 0s - loss: 36789.8749WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 949s 266us/sample - loss: 36789.7598 - val_loss: 7177.7051\n",
      "Epoch 8/30\n",
      "3574912/3574936 [============================>.] - ETA: 0s - loss: 23993.7823WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 957s 268us/sample - loss: 23993.6646 - val_loss: 10087.9008\n",
      "Epoch 9/30\n",
      "3574848/3574936 [============================>.] - ETA: 0s - loss: 25170.1871WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 942s 263us/sample - loss: 25170.9442 - val_loss: 18133.0503\n",
      "Epoch 10/30\n",
      "3574848/3574936 [============================>.] - ETA: 0s - loss: 21943.7605WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 917s 256us/sample - loss: 21943.5844 - val_loss: 16753.0554\n",
      "Epoch 11/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 14511.1112WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 928s 260us/sample - loss: 14511.0489 - val_loss: 8060.4763\n",
      "Epoch 12/30\n",
      "3574848/3574936 [============================>.] - ETA: 0s - loss: 14431.7743WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 916s 256us/sample - loss: 14431.6854 - val_loss: 7815.5789\n",
      "Epoch 13/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 12547.0889WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 924s 259us/sample - loss: 12547.0173 - val_loss: 12235.5089\n",
      "Epoch 14/30\n",
      "3574784/3574936 [============================>.] - ETA: 0s - loss: 11988.2478WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 930s 260us/sample - loss: 11987.9956 - val_loss: 5423.0804\n",
      "Epoch 15/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 12305.8106WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 914s 256us/sample - loss: 12305.8279 - val_loss: 13102.8803\n",
      "Epoch 16/30\n",
      "3574624/3574936 [============================>.] - ETA: 0s - loss: 13165.1749WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 876s 245us/sample - loss: 13164.6101 - val_loss: 25988.7843\n",
      "Epoch 17/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 13273.7215WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 639s 179us/sample - loss: 13273.6305 - val_loss: 6114.3221\n",
      "Epoch 18/30\n",
      "3574752/3574936 [============================>.] - ETA: 0s - loss: 13640.2943WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 640s 179us/sample - loss: 13639.9492 - val_loss: 7613.5581\n",
      "Epoch 19/30\n",
      "3574688/3574936 [============================>.] - ETA: 0s - loss: 12974.2585WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 636s 178us/sample - loss: 12973.7456 - val_loss: 5606.4769\n",
      "Epoch 20/30\n",
      "3574816/3574936 [============================>.] - ETA: 0s - loss: 12213.5040WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 631s 176us/sample - loss: 12213.3285 - val_loss: 7685.9276\n",
      "Epoch 21/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 11649.3319WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 626s 175us/sample - loss: 11649.2853 - val_loss: 6735.0516\n",
      "Epoch 22/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 11167.7325WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 621s 174us/sample - loss: 11167.6926 - val_loss: 6457.8846\n",
      "Epoch 23/30\n",
      "3574592/3574936 [============================>.] - ETA: 0s - loss: 10432.6944WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 615s 172us/sample - loss: 10432.1131 - val_loss: 5950.9520\n",
      "Epoch 24/30\n",
      "3574688/3574936 [============================>.] - ETA: 0s - loss: 9831.6148WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 594s 166us/sample - loss: 9831.3767 - val_loss: 4235.9829\n",
      "Epoch 25/30\n",
      "3574688/3574936 [============================>.] - ETA: 0s - loss: 9426.2787WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 592s 165us/sample - loss: 9425.9567 - val_loss: 4918.3182\n",
      "Epoch 26/30\n",
      "3574656/3574936 [============================>.] - ETA: 0s - loss: 8929.9011WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 591s 165us/sample - loss: 8929.5486 - val_loss: 5262.2947\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574624/3574936 [============================>.] - ETA: 0s - loss: 8684.7333WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 591s 165us/sample - loss: 8684.1765 - val_loss: 1928.0091\n",
      "Epoch 28/30\n",
      "3574560/3574936 [============================>.] - ETA: 0s - loss: 8157.1679WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 591s 165us/sample - loss: 8156.7809 - val_loss: 7105.8565\n",
      "Epoch 29/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 8050.1500WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 591s 165us/sample - loss: 8050.1412 - val_loss: 6081.2197\n",
      "Epoch 30/30\n",
      "3574816/3574936 [============================>.] - ETA: 0s - loss: 7765.4551WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 590s 165us/sample - loss: 7765.3506 - val_loss: 6354.7730\n",
      "MSE, MAE, r2: 6437.035350,68.816056,0.993856\n",
      "--------------------------------\n",
      "TBD :)\n",
      "--------------------------------\n",
      "Train on 3574936 samples, validate on 188155 samples\n",
      "Epoch 1/30\n",
      "3574720/3574936 [============================>.] - ETA: 0s - loss: 122765.3890WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 751s 210us/sample - loss: 122758.0816 - val_loss: 3628.0072\n",
      "Epoch 2/30\n",
      "3574912/3574936 [============================>.] - ETA: 0s - loss: 2462.9029WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 747s 209us/sample - loss: 2462.8993 - val_loss: 1445.5760\n",
      "Epoch 3/30\n",
      "3574912/3574936 [============================>.] - ETA: 0s - loss: 1857.3371WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 735s 205us/sample - loss: 1857.3399 - val_loss: 1390.2071\n",
      "Epoch 4/30\n",
      "3574752/3574936 [============================>.] - ETA: 0s - loss: 1738.1711WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 728s 204us/sample - loss: 1738.1267 - val_loss: 1350.4712\n",
      "Epoch 5/30\n",
      "3574848/3574936 [============================>.] - ETA: 0s - loss: 1639.0771WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 728s 204us/sample - loss: 1639.0660 - val_loss: 1342.9861\n",
      "Epoch 6/30\n",
      "3574752/3574936 [============================>.] - ETA: 0s - loss: 1561.9898WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 705s 197us/sample - loss: 1561.9529 - val_loss: 1293.4625\n",
      "Epoch 7/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 1506.6605WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 704s 197us/sample - loss: 1506.6616 - val_loss: 1258.7668\n",
      "Epoch 8/30\n",
      "3574912/3574936 [============================>.] - ETA: 0s - loss: 1449.8125WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 704s 197us/sample - loss: 1449.8210 - val_loss: 1244.4600\n",
      "Epoch 9/30\n",
      "3574624/3574936 [============================>.] - ETA: 0s - loss: 1387.2096WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 704s 197us/sample - loss: 1387.2013 - val_loss: 1218.6378\n",
      "Epoch 10/30\n",
      "3574624/3574936 [============================>.] - ETA: 0s - loss: 1357.8225WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 704s 197us/sample - loss: 1357.8157 - val_loss: 1214.7170\n",
      "Epoch 11/30\n",
      "3574656/3574936 [============================>.] - ETA: 0s - loss: 1311.0612WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 704s 197us/sample - loss: 1311.0372 - val_loss: 1202.1148\n",
      "Epoch 12/30\n",
      "3574688/3574936 [============================>.] - ETA: 0s - loss: 1288.5933WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 703s 197us/sample - loss: 1288.5721 - val_loss: 1183.9067\n",
      "Epoch 13/30\n",
      "3574880/3574936 [============================>.] - ETA: 0s - loss: 1257.2786WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 703s 197us/sample - loss: 1257.2861 - val_loss: 1172.2915\n",
      "Epoch 14/30\n",
      "3574656/3574936 [============================>.] - ETA: 0s - loss: 1251.7817WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 703s 197us/sample - loss: 1251.7595 - val_loss: 1138.7741\n",
      "Epoch 15/30\n",
      "3574816/3574936 [============================>.] - ETA: 0s - loss: 1306.7496WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 703s 197us/sample - loss: 1306.7476 - val_loss: 1087.2631\n",
      "Epoch 16/30\n",
      "3574912/3574936 [============================>.] - ETA: 0s - loss: 1402.8855WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 703s 197us/sample - loss: 1402.8922 - val_loss: 1034.4858\n",
      "Epoch 18/30\n",
      "3574624/3574936 [============================>.] - ETA: 0s - loss: 1253.2188WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 698s 195us/sample - loss: 1253.2121 - val_loss: 1087.3781\n",
      "Epoch 19/30\n",
      "3148896/3574936 [=========================>....] - ETA: 1:21 - loss: 1203.0415"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 928992/3574936 [======>.......................] - ETA: 8:23 - loss: 1308.0957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2314048/3574936 [==================>...........] - ETA: 4:00 - loss: 1218.0789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574624/3574936 [============================>.] - ETA: 0s - loss: 1189.7928WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 696s 195us/sample - loss: 1189.7891 - val_loss: 1052.9082\n",
      "Epoch 21/30\n",
      " 254240/3574936 [=>............................] - ETA: 10:30 - loss: 1367.3495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672672/3574936 [=============>................] - ETA: 6:01 - loss: 1230.9670"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2952640/3574936 [=======================>......] - ETA: 1:58 - loss: 1183.9680"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 875712/3574936 [======>.......................] - ETA: 8:35 - loss: 1166.4998"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3323296/3574936 [==========================>...] - ETA: 47s - loss: 1126.3450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574784/3574936 [============================>.] - ETA: 0s - loss: 1119.7799WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 695s 195us/sample - loss: 1119.7799 - val_loss: 1050.0454\n",
      "Epoch 23/30\n",
      " 825760/3574936 [=====>........................] - ETA: 8:42 - loss: 1120.1369"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2699744/3574936 [=====================>........] - ETA: 2:46 - loss: 1099.8181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574688/3574936 [============================>.] - ETA: 0s - loss: 1094.1972WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 695s 194us/sample - loss: 1094.1867 - val_loss: 1050.5199\n",
      "Epoch 24/30\n",
      " 188288/3574936 [>.............................] - ETA: 10:51 - loss: 1192.4616"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1449152/3574936 [===========>..................] - ETA: 6:46 - loss: 1095.0246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574816/3574936 [============================>.] - ETA: 0s - loss: 1078.6909WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "3574936/3574936 [==============================] - 703s 197us/sample - loss: 1078.7014 - val_loss: 1054.5994\n",
      "Epoch 25/30\n",
      "1238592/3574936 [=========>....................] - ETA: 7:28 - loss: 1081.4934"
     ]
    }
   ],
   "source": [
    "show_perf = True\n",
    "load_prior_model = False\n",
    "train_model = True\n",
    "strong_data_sfx = \"_strong\" if (only_use_strong > 0) else \"\"\n",
    "\n",
    "Hypercube_8_3 = buildLattice(8,3,columns[:-1])\n",
    "if (load_prior_model): \n",
    "    Hypercube_8_3.build((None,4,))\n",
    "    Hypercube_8_3.load_weights('./Hypercube_8_3' + strong_data_sfx) #load\n",
    "if (train_model): \n",
    "    Hypercube_8_3 = trainLattice(Hypercube_8_3,'Hypercube_8_3' + strong_data_sfx,columns[:-1], 30) #train\n",
    "    Hypercube_8_3.save('./Hypercube_8_3' + strong_data_sfx)\n",
    "preds = Hypercube_8_3.predict(test_dataset.values.astype(np.float32))\n",
    "if (show_perf): evaluatePerf(test_labels, preds)\n",
    "print('--------------------------------')\n",
    "extractLatticeWeights(Hypercube_8_3, 'Hypercube_8_3' + strong_data_sfx)\n",
    "print('--------------------------------')\n",
    "\n",
    "\n",
    "Hypercube_4_5 = buildLattice(4,5,columns[:-1])\n",
    "if (load_prior_model): \n",
    "    Hypercube_4_5.build((None,4,))\n",
    "    Hypercube_4_5.load_weights('./Hypercube_4_5' + strong_data_sfx) #load\n",
    "if (train_model): \n",
    "    Hypercube_4_5 = trainLattice(Hypercube_4_5,'Hypercube_4_5' + strong_data_sfx,columns[:-1], 30) #train\n",
    "    Hypercube_4_5.save('./Hypercube_4_5' + strong_data_sfx)\n",
    "preds = Hypercube_4_5.predict(test_dataset.values.astype(np.float32))\n",
    "if (show_perf): evaluatePerf(test_labels, preds)\n",
    "print('--------------------------------')\n",
    "extractLatticeWeights(Hypercube_4_5, 'Hypercube_4_5' + strong_data_sfx)\n",
    "print('--------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = Hypercube_8_4.predict(test_dataset.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE, MAE, r2: 19563.272052 51.622984 0.988916\n"
     ]
    }
   ],
   "source": [
    "if (show_perf): evaluatePerf(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7ff5ce5da668>\n"
     ]
    }
   ],
   "source": [
    "# print(Hypercube_8_2)\n",
    "# print(train_dataset.values)\n",
    "# print(train_labels.values)\n",
    "test = buildLattice(2,8,columns[:-1])\n",
    "print(test)\n",
    "# print(Hypercube_8_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.200e+01 1.900e+02 3.790e+02 1.300e+01 0.000e+00]\n",
      " [3.600e+01 2.480e+02 4.710e+02 1.943e+03 0.000e+00]\n",
      " [2.500e+01 4.000e+01 2.010e+02 2.500e+01 0.000e+00]\n",
      " [9.600e+01 1.390e+02 3.180e+02 1.000e+00 0.000e+00]\n",
      " [6.600e+01 4.000e+01 1.470e+02 0.000e+00 0.000e+00]\n",
      " [2.400e+01 1.470e+02 3.270e+02 1.850e+02 0.000e+00]\n",
      " [1.400e+01 1.550e+02 3.370e+02 4.890e+02 0.000e+00]\n",
      " [1.050e+02 1.850e+02 3.730e+02 1.200e+01 0.000e+00]\n",
      " [4.900e+01 7.000e+01 8.300e+01 3.660e+03 0.000e+00]\n",
      " [2.600e+01 5.200e+01 2.150e+02 5.900e+01 0.000e+00]]\n",
      "Train on 9 samples, validate on 1 samples\n",
      "9/9 [==============================] - 1s 137ms/sample - loss: 1308289.0000 - val_loss: 2232724.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff5f64755c0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = train_dataset[columns[:-1]].values.astype(np.float32)[0:10]\n",
    "print(features)\n",
    "target = train_labels.values.astype(np.float32)[0:10]\n",
    "\n",
    "test.fit(features,\n",
    "        target,\n",
    "        batch_size=32,\n",
    "        epochs=1,\n",
    "        validation_split=0.05,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "parallel_combination_21 (Par multiple                  40        \n",
      "_________________________________________________________________\n",
      "lattice_21 (Lattice)         multiple                  32        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test.summary())\n",
    "preds = test.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -84.66176]\n",
      " [ 204.66684]\n",
      " [ -94.94145]\n",
      " [-149.71718]\n",
      " [-133.84915]\n",
      " [ -71.80316]\n",
      " [ -34.44699]\n",
      " [-163.96371]\n",
      " [ 572.02185]\n",
      " [ -91.2105 ]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.save('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = buildLattice(2,8,columns[:-1])\n",
    "test2.build((None,5,))\n",
    "test2.load_weights('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -84.66176]\n",
      " [ 204.66684]\n",
      " [ -94.94145]\n",
      " [-149.71718]\n",
      " [-133.84915]\n",
      " [ -71.80316]\n",
      " [ -34.44699]\n",
      " [-163.96371]\n",
      " [ 572.02185]\n",
      " [ -91.2105 ]]\n"
     ]
    }
   ],
   "source": [
    "print(test2.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-5911d59d27fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHypercube_8_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshow_perf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluatePerf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[0;31m# Get step function and loop type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0muse_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_dataset\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m    530\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2280\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2282\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2283\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_predict_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2270\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict_function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2272\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   2273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2274\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   3477\u001b[0m                'backend') % key\n\u001b[1;32m   3478\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mGraphExecutionFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m     \u001b[0;31m# dependencies in call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     \u001b[0;31m# Index 0 = total loss or model output for `predict`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3143\u001b[0m       \u001b[0mupdates_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "preds = Hypercube_8_2.predict(test_dataset.values.astype(np.float32))\n",
    "if (show_perf): evaluatePerf(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
