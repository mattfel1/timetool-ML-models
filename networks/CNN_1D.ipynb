{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from os import path\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "import functools \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from common import splitDataset\n",
    "from common import splitDatasetLabels\n",
    "from common import normDataset\n",
    "from common import evaluatePerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCNN(strides, kernels, layers):\n",
    "    n_timesteps, n_features, n_outputs = 1024, 1, 1\n",
    "    model = Sequential()\n",
    "    layer = 0\n",
    "    for s,k,l in zip(strides,kernels,layers):\n",
    "        if (layer == 0): model.add(Conv1D(filters=l, kernel_size=k, strides=s, activation='relu', input_shape=(n_timesteps,1)))\n",
    "        else: model.add(Conv1D(filters=l, kernel_size=k, strides=s, activation='relu'))\n",
    "        \n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_outputs, activation='relu'))\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer='adam', \n",
    "#                   optimizer=keras.optimizers.Adagrad(learning_rate=100.0),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCNN(model, trainX, trainY, epochs):\n",
    "    # fit network\n",
    "    model.fit(trainX, trainY, epochs=epochs, steps_per_epoch=trainY.shape[0], verbose=1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattfel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at 1100\n",
      "at 1200\n",
      "at 1300\n",
      "at 1400\n",
      "at 1500\n",
      "at 1600\n",
      "at 1700\n",
      "at 1800\n",
      "at 1900\n"
     ]
    }
   ],
   "source": [
    "# Handle raw data\n",
    "\n",
    "datadir = '/scratch/mattfel/raw_timetool/u1/coffee/2dtimetool_simulation_data/raw/'\n",
    "\n",
    "raw_dataset = pd.DataFrame()\n",
    "for fileId in range(1000,2000):\n",
    "    fileType = 'interference'\n",
    "    if (fileId % 100 == 0): print('at %d' % fileId)\n",
    "    f = datadir + '/chirp-2000_interferedelay1650_photonen6.0_carriertagdiamond_nfibers109_netalon1_1.00_1.00_%s.out.%d' % (fileType,fileId)\n",
    "    if (path.exists(f)):\n",
    "        rawdata = pd.read_csv(f, \n",
    "                          skiprows = 7, usecols = [i for i in range(1024)], header=None,\n",
    "                          na_values = \"?\", comment='\\t',\n",
    "                          sep=\"\\t|,\", skipinitialspace=True)\n",
    "        raw_dataset = raw_dataset.append(rawdata)\n",
    "    else:\n",
    "        print('file %d does not exist!' % fileId)\n",
    "    \n",
    "# raw_dataset = tf.stack([raw_dataset], axis=2)\n",
    "\n",
    "\n",
    "raw_labels = pd.DataFrame()\n",
    "for fileId in range(1000,2000):\n",
    "    fileType = 'fibermap'\n",
    "    if (fileId % 100 == 0): print('at %d' % fileId)\n",
    "    f = datadir + '/chirp-2000_interferedelay1650_photonen6.0_carriertagdiamond_nfibers109_netalon1_1.00_1.00_%s.out.%d' % (fileType,fileId)\n",
    "    if (path.exists(f)):\n",
    "        rawdata = pd.read_csv(f, \n",
    "                          skiprows = 1, usecols = [6], \n",
    "                          na_values = \"?\", comment='\\t',\n",
    "                          sep=\"\\t|,\", skipinitialspace=True)\n",
    "        raw_labels = raw_labels.append(rawdata)\n",
    "    else:\n",
    "        print('file %d does not exist!' % fileId)\n",
    "\n",
    "\n",
    "# print(raw_labels)\n",
    "# print(raw_dataset.iloc[109])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109000, 1)\n",
      "(109000, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(raw_labels.shape)\n",
    "print(raw_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, train_labels, test_labels = splitDatasetLabels(raw_dataset, raw_labels, 0.2)\n",
    "train_dataset = tf.stack([train_dataset], axis=2)\n",
    "test_dataset = tf.stack([test_dataset], axis=2)\n",
    "# train_labels = tf.stack([train_labels], axis=2)\n",
    "# test_labels = tf.stack([test_labels], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 1009, 8)           136       \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 8072)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 8073      \n",
      "=================================================================\n",
      "Total params: 8,209\n",
      "Trainable params: 8,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      " 1446/87200 [..............................] - ETA: 140:00:14 - loss: 2004840.6874 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "show_perf = True\n",
    "load_prior_model = False\n",
    "train_model = True\n",
    "strong_data_sfx = \"_strong\"\n",
    "\n",
    "strides=[1]\n",
    "kernels=[16]\n",
    "chans=[8]\n",
    "model = buildCNN(strides, kernels, chans)\n",
    "model.summary()\n",
    "model = trainCNN(model, train_dataset, train_labels, 1000)\n",
    "if (show_perf):\n",
    "    preds = model.predict(test_dataset, steps=1)\n",
    "    if (show_perf): evaluatePerf(test_labels, preds)\n",
    "\n",
    "        \n",
    "strides=[1]\n",
    "kernels=[64]\n",
    "chans=[8]\n",
    "model = buildCNN(strides, kernels, chans)\n",
    "model.summary()\n",
    "model = trainCNN(model, train_dataset, train_labels, 1000)\n",
    "if (show_perf):\n",
    "    preds = model.predict(test_dataset, steps=1)\n",
    "    if (show_perf): evaluatePerf(test_labels, preds)\n",
    "        \n",
    "        \n",
    "strides=[2,2,2]\n",
    "kernels=[16,16,16]\n",
    "chans=[4,4,4]\n",
    "model = buildCNN(strides, kernels, chans)\n",
    "model.summary()\n",
    "model = trainCNN(model, train_dataset, train_labels, 1000)\n",
    "if (show_perf):\n",
    "    preds = model.predict(test_dataset, steps=1)\n",
    "    if (show_perf): evaluatePerf(test_labels, preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE, MAE, r2: 1634321.932250 955.822019 -0.229121\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_dataset, steps=1)\n",
    "if (show_perf): evaluatePerf(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
