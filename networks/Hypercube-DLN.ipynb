{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_lattice as tfl\n",
    "from concurrent import futures\n",
    "\n",
    "import itertools\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import helpers\n",
    "import import_ipynb\n",
    "from common import buildDatasetForLattice\n",
    "from common import scaleVolume\n",
    "from common import extractLatticeWeights\n",
    "from common import dropColumns\n",
    "from common import filterBad\n",
    "from common import splitDataset\n",
    "from common import normDataset\n",
    "from common import evaluatePerf\n",
    "from common import evaluateCustom\n",
    "from common import extractXGWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected 6196563 points (56.849770%)\n"
     ]
    }
   ],
   "source": [
    "dataset, columns = buildDatasetForLattice()\n",
    "dataset = filterBad(dataset, 54)\n",
    "train_dataset, test_dataset, train_labels, test_labels = splitDataset(dataset, 0.2)\n",
    "train_stats = train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = dataset.copy()\n",
    "col = preprocessed_dataset.first_val\n",
    "preprocessed_dataset.first_val = np.log(col + 1)\n",
    "# for col_name in preprocessed_dataset.columns[:-1]:\n",
    "#     col = preprocessed_dataset[col_name]\n",
    "#     preprocessed_dataset[col_name] = (col - col.mean())/col.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>rising_idx</th>\n",
       "      <th>falling_idx</th>\n",
       "      <th>first_val</th>\n",
       "      <th>last_val</th>\n",
       "      <th>delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>315</td>\n",
       "      <td>6.444131</td>\n",
       "      <td>0</td>\n",
       "      <td>694.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>263</td>\n",
       "      <td>6.483107</td>\n",
       "      <td>0</td>\n",
       "      <td>1061.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>284</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>0</td>\n",
       "      <td>879.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>155</td>\n",
       "      <td>338</td>\n",
       "      <td>5.993961</td>\n",
       "      <td>0</td>\n",
       "      <td>512.428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>175</td>\n",
       "      <td>362</td>\n",
       "      <td>6.142037</td>\n",
       "      <td>0</td>\n",
       "      <td>328.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10899831</th>\n",
       "      <td>49</td>\n",
       "      <td>131</td>\n",
       "      <td>310</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0</td>\n",
       "      <td>708.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10899832</th>\n",
       "      <td>50</td>\n",
       "      <td>173</td>\n",
       "      <td>358</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0</td>\n",
       "      <td>342.287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10899833</th>\n",
       "      <td>51</td>\n",
       "      <td>343</td>\n",
       "      <td>562</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0</td>\n",
       "      <td>-940.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10899834</th>\n",
       "      <td>52</td>\n",
       "      <td>487</td>\n",
       "      <td>739</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0</td>\n",
       "      <td>-1857.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10899835</th>\n",
       "      <td>53</td>\n",
       "      <td>425</td>\n",
       "      <td>661</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0</td>\n",
       "      <td>-1491.540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4703328 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row  rising_idx  falling_idx  first_val  last_val     delay\n",
       "0           0         131          315   6.444131         0   694.991\n",
       "1           1          91          263   6.483107         0  1061.620\n",
       "2           2         110          284   6.163315         0   879.057\n",
       "3           3         155          338   5.993961         0   512.428\n",
       "4           4         175          362   6.142037         0   328.362\n",
       "...       ...         ...          ...        ...       ...       ...\n",
       "10899831   49         131          310   3.044522         0   708.468\n",
       "10899832   50         173          358   3.044522         0   342.287\n",
       "10899833   51         343          562   2.639057         0  -940.805\n",
       "10899834   52         487          739   2.302585         0 -1857.720\n",
       "10899835   53         425          661   2.302585         0 -1491.540\n",
       "\n",
       "[4703328 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_initializers = {\n",
    "    'row': \"quantile\", 'rising_idx': \"quantile\", 'falling_idx': \"quantile\", 'first_val': \"uniform\", 'last_val': \"uniform\"\n",
    "}\n",
    "monotonicities = {\n",
    "    \"rising_idx\": -1,\n",
    "    \"falling_idx\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLattice(model, cols, epochs, log_path):\n",
    "    features = [preprocessed_dataset[col].values for col in cols]\n",
    "    target = preprocessed_dataset[\"delay\"]\n",
    "    csv_logger = keras.callbacks.CSVLogger(log_path, append=True, separator=';')\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    \n",
    "\n",
    "    history = model.fit(features,\n",
    "            target,\n",
    "            batch_size=256,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.2,\n",
    "            shuffle=False, workers=32, use_multiprocessing=True, callbacks=[csv_logger, early_stop], verbose=2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLattice(num_keypoints, lattice_size, columns, num_lattices_per_layer, depth):\n",
    "    base_inputs = [keras.layers.Input(shape=[1]) for _ in columns]\n",
    "    for dp in range(depth):\n",
    "        if dp == 0:\n",
    "            tmp = []\n",
    "            # handle initial layer\n",
    "            for inpt, ft in zip(base_inputs, columns):\n",
    "                if ft != \"row\":\n",
    "                    if kp_initializers[ft] == \"quantile\":\n",
    "                        quantile_vals = [i/(num_keypoints - 1.0) for i in range(num_keypoints)]\n",
    "                        keypoints = dataset[ft].quantile(quantile_vals).values\n",
    "                    if kp_initializers[ft] == \"uniform\":\n",
    "                        keypoints = np.linspace(preprocessed_dataset[ft].min(), preprocessed_dataset[ft].max(), num=num_keypoints)\n",
    "\n",
    "                    calibrator = tfl.layers.PWLCalibration(\n",
    "                        input_keypoints=keypoints, dtype=tf.float32, output_min=0.0, output_max=lattice_size - 1.0,\n",
    "                        units = num_lattices_per_layer,\n",
    "                        monotonicity=monotonicities.get(ft, 0)\n",
    "                    )(inpt)\n",
    "                else:\n",
    "                    # row is categorical\n",
    "                    calibrator = tfl.layers.CategoricalCalibration(\n",
    "                        num_buckets=preprocessed_dataset[ft].nunique(),\n",
    "                        output_min = 0.0,\n",
    "                        output_max = lattice_size - 1.0,\n",
    "                        units = num_lattices_per_layer\n",
    "                    )(inpt)\n",
    "                tmp.append(calibrator)\n",
    "            if num_lattices_per_layer != 1:\n",
    "                combined_calibrators = keras.backend.stack(tmp, axis=2)\n",
    "            else:\n",
    "                combined_calibrators = keras.layers.concatenate(tmp)\n",
    "            num_inputs = len(base_inputs)\n",
    "        else:\n",
    "            tmp = tfl.layers.PWLCalibration(input_keypoints=np.linspace(0, 1, num=num_keypoints), output_min=0.0,\n",
    "                                                             output_max=lattice_size - 1.0,\n",
    "                                                             units = num_lattices_per_layer)(inputs)\n",
    "            # now need to stack copies of tmp\n",
    "            combined_calibrators = keras.backend.stack([tmp]*num_lattices_per_layer, axis=1)\n",
    "            num_inputs = num_lattices_per_layer\n",
    "        \n",
    "        # construct a multi-unit lattice\n",
    "        print(\"Combined Calibrators:\", combined_calibrators.shape)\n",
    "        inputs = tfl.layers.Lattice(\n",
    "            lattice_sizes=[lattice_size for _ in range(num_inputs)], output_min = 0.0, output_max = 1.0,\n",
    "            units = num_lattices_per_layer\n",
    "        )(combined_calibrators)\n",
    "        print(\"Inputs:\", inputs.shape)\n",
    "\n",
    "    if num_lattices_per_layer != 1:\n",
    "        inputs = tfl.layers.Linear(\n",
    "            num_input_dims=num_lattices_per_layer, normalization_order=1,\n",
    "            kernel_regularizer=keras.regularizers.l1(0.01))(inputs)\n",
    "    calibrated_output = tfl.layers.PWLCalibration(\n",
    "            input_keypoints=np.linspace(0, 1, num=num_keypoints), output_min=preprocessed_dataset['delay'].min(),\n",
    "            output_max=preprocessed_dataset['delay'].max(), monotonicity=\"increasing\")(inputs)\n",
    "    model = keras.models.Model(inputs=base_inputs, outputs=calibrated_output)\n",
    "    model.compile(loss=keras.losses.mean_absolute_error,\n",
    "                optimizer=keras.optimizers.Adam(), metrics=[\"mse\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1 1\n",
      "Training: 2 1\n",
      "Training: 2 2\n",
      "Training: 2 3\n",
      "Training: 2 4\n",
      "Training: 2 6\n",
      "Training: 2 8\n",
      "Training: 4 3\n",
      "Training: 4 4\n",
      "Training: 4 5\n",
      "Training: 4 6\n",
      "Training: 4 7\n",
      "Training: 4 8\n",
      "Training: 6 1\n",
      "Training: 6 2\n",
      "Training: 6 4\n",
      "Training: 6 5\n",
      "Training: 6 7\n",
      "Training: 6 8\n",
      "Training: 8 1\n",
      "Training: 8 2\n",
      "Training: 8 3\n",
      "Training: 8 4\n",
      "Training: 8 5\n",
      "Training: 8 6\n",
      "Training: 8 7\n",
      "Training: 8 8\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "3762662/3762662 - 334s - loss: 11.6882 - mse: 787.5555 - val_loss: 11.6136 - val_mse: 793.2952\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 371s - loss: 12.1749 - mse: 1034.4908 - val_loss: 12.0177 - val_mse: 1033.1465\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 515s - loss: 7.2440 - mse: 571.7936 - val_loss: 7.0724 - val_mse: 558.5798\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 417s - loss: 841.1711 - mse: 1052335.1250 - val_loss: 839.4393 - val_mse: 1047797.4375\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 255s - loss: 11.5834 - mse: 784.5213 - val_loss: 11.5797 - val_mse: 794.5612\n",
      "Epoch 3/16\n",
      "3762662/3762662 - 325s - loss: 841.1711 - mse: 1052335.1250 - val_loss: 839.4393 - val_mse: 1047797.4375\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 521s - loss: 10.2290 - mse: 1062.8507 - val_loss: 10.0277 - val_mse: 1053.1411\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 279s - loss: 11.9846 - mse: 1022.4717 - val_loss: 11.9886 - val_mse: 1029.7799\n",
      "Epoch 3/16\n",
      "3762662/3762662 - 268s - loss: 11.5586 - mse: 785.3433 - val_loss: 11.5685 - val_mse: 796.0463\n",
      "Epoch 4/16\n",
      "3762662/3762662 - 765s - loss: 5.2170 - mse: 318.7251 - val_loss: 5.1071 - val_mse: 320.9221\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 818s - loss: 5.5010 - mse: 395.6333 - val_loss: 5.3796 - val_mse: 391.5097\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 325s - loss: 841.1711 - mse: 1052335.1250 - val_loss: 839.4393 - val_mse: 1047797.3750\n",
      "Epoch 3/16\n",
      "3762662/3762662 - 306s - loss: 841.1711 - mse: 1052335.1250 - val_loss: 839.4393 - val_mse: 1047797.3750\n",
      "Epoch 3/16\n",
      "3762662/3762662 - 465s - loss: 7.0454 - mse: 564.4116 - val_loss: 7.0108 - val_mse: 554.4409\n",
      "Epoch 3/16\n",
      "3762662/3762662 - 286s - loss: 11.9530 - mse: 1020.1031 - val_loss: 11.9720 - val_mse: 1028.7283\n",
      "Epoch 4/16\n",
      "3762662/3762662 - 420s - loss: 9.9312 - mse: 1051.3138 - val_loss: 9.9387 - val_mse: 1051.4495\n",
      "Epoch 3/16\n",
      "3762662/3762662 - 944s - loss: 5.0168 - mse: 320.9441 - val_loss: 4.9032 - val_mse: 317.3670\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 265s - loss: 11.5416 - mse: 785.9135 - val_loss: 11.5494 - val_mse: 796.5826\n",
      "Epoch 5/16\n",
      "3762662/3762662 - 324s - loss: 841.1711 - mse: 1052335.0000 - val_loss: 839.4393 - val_mse: 1047797.2500\n",
      "Epoch 4/16\n",
      "3762662/3762662 - 1055s - loss: 8.5919 - mse: 966.5652 - val_loss: 8.3626 - val_mse: 962.3195\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 312s - loss: 841.1711 - mse: 1052335.0000 - val_loss: 839.4393 - val_mse: 1047797.2500\n",
      "Epoch 4/16\n",
      "3762662/3762662 - 285s - loss: 11.9302 - mse: 1018.6768 - val_loss: 11.9604 - val_mse: 1027.5999\n",
      "Epoch 5/16\n",
      "3762662/3762662 - 1151s - loss: 8.5814 - mse: 934.5209 - val_loss: 8.1143 - val_mse: 928.0574\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 284s - loss: 11.5279 - mse: 786.1437 - val_loss: 11.5371 - val_mse: 796.7490\n",
      "Epoch 6/16\n",
      "3762662/3762662 - 486s - loss: 7.0000 - mse: 561.1603 - val_loss: 6.9751 - val_mse: 551.4026\n",
      "Epoch 4/16\n",
      "3762662/3762662 - 453s - loss: 9.8616 - mse: 1049.8822 - val_loss: 9.8816 - val_mse: 1050.3062\n",
      "Epoch 4/16\n",
      "3762662/3762662 - 684s - loss: 5.0828 - mse: 314.7673 - val_loss: 5.0792 - val_mse: 318.8093\n",
      "Epoch 3/16\n",
      "3762662/3762662 - 370s - loss: 841.1711 - mse: 1052334.5000 - val_loss: 839.4393 - val_mse: 1047797.0625\n",
      "Epoch 5/16\n",
      "3762662/3762662 - 366s - loss: 841.1711 - mse: 1052334.5000 - val_loss: 839.4393 - val_mse: 1047797.0625\n",
      "Epoch 5/16\n",
      "3762662/3762662 - 755s - loss: 5.3651 - mse: 391.5641 - val_loss: 5.3383 - val_mse: 389.4846\n",
      "Epoch 3/16\n",
      "3762662/3762662 - 348s - loss: 11.9121 - mse: 1017.2364 - val_loss: 11.9440 - val_mse: 1026.5341\n",
      "Epoch 6/16\n",
      "3762662/3762662 - 320s - loss: 11.5165 - mse: 786.2332 - val_loss: 11.5233 - val_mse: 797.0018\n",
      "Epoch 7/16\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "def train_model(lattices_per_layer, depth, load_model=True):\n",
    "    model_path = f'./Hypercube_8_2_{lattices_per_layer}_{depth}'\n",
    "    if load_model:\n",
    "        lattice = keras.models.load_model(model_path+\".h5\", custom_objects={\n",
    "            \"CategoricalCalibration\": tfl.layers.CategoricalCalibration,\n",
    "            \"PWLCalibration\": tfl.layers.PWLCalibration,\n",
    "            \"Lattice\": tfl.layers.Lattice,\n",
    "            \"Linear\": tfl.layers.Linear\n",
    "        })\n",
    "    else:\n",
    "        lattice = buildLattice(8, 2, columns[:-1], lattices_per_layer, depth)\n",
    "    lattice.compile(loss=keras.losses.mean_absolute_error,\n",
    "            optimizer=keras.optimizers.Adagrad(), metrics=[\"mse\"])\n",
    "    history = trainLattice(lattice,columns[:-1], 16, model_path + \".csv\")\n",
    "    lattice.save(model_path + \".h5\")\n",
    "\n",
    "\n",
    "\n",
    "while reruns:\n",
    "    with futures.ThreadPoolExecutor() as pool:\n",
    "    #     combinations = itertools.product([1, 2, 4, 6, 8], [1, 2, 3, 4, 5, 6, 7, 8])\n",
    "        combinations = reruns\n",
    "        for args in combinations:\n",
    "            print(\"Training:\", *args)\n",
    "            pool.submit(train_model, *args)\n",
    "#             train_model(*args)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "reruns = []\n",
    "files = !ls *.csv\n",
    "for fn in files:\n",
    "    with open(fn) as f:\n",
    "        for line in f:\n",
    "            pass\n",
    "        split = fn.split(\"_\")\n",
    "        lpl = int(split[-2])\n",
    "        depth = int(split[-1].split(\".\")[0])\n",
    "        reruns.append((lpl, depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (2, 6),\n",
       " (2, 8),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (4, 5),\n",
       " (4, 6),\n",
       " (4, 7),\n",
       " (4, 8),\n",
       " (6, 1),\n",
       " (6, 2),\n",
       " (6, 4),\n",
       " (6, 5),\n",
       " (6, 7),\n",
       " (6, 8),\n",
       " (8, 1),\n",
       " (8, 2),\n",
       " (8, 3),\n",
       " (8, 4),\n",
       " (8, 5),\n",
       " (8, 6),\n",
       " (8, 7),\n",
       " (8, 8)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3762662 samples, validate on 940666 samples\n",
      "Epoch 1/16\n",
      "3762662/3762662 - 364s - loss: 5.4337 - mse: 331.3935 - val_loss: 5.2917 - val_mse: 334.0094\n",
      "Epoch 2/16\n",
      "3762662/3762662 - 352s - loss: 5.2474 - mse: 325.6170 - val_loss: 5.2407 - val_mse: 332.0394\n",
      "Epoch 3/16\n",
      "3762662/3762662 - 358s - loss: 5.2113 - mse: 323.2291 - val_loss: 5.2123 - val_mse: 328.4985\n",
      "Epoch 4/16\n",
      "3762662/3762662 - 361s - loss: 5.1882 - mse: 320.7349 - val_loss: 5.1933 - val_mse: 325.7516\n",
      "Epoch 5/16\n",
      "3762662/3762662 - 354s - loss: 5.1718 - mse: 320.0960 - val_loss: 5.1760 - val_mse: 324.9814\n",
      "Epoch 6/16\n",
      "3762662/3762662 - 356s - loss: 5.1586 - mse: 319.3404 - val_loss: 5.1629 - val_mse: 323.9946\n",
      "Epoch 7/16\n",
      "3762662/3762662 - 356s - loss: 5.1475 - mse: 318.7658 - val_loss: 5.1510 - val_mse: 324.1135\n",
      "Epoch 8/16\n",
      "3762662/3762662 - 360s - loss: 5.1381 - mse: 318.1583 - val_loss: 5.1400 - val_mse: 323.0582\n",
      "Epoch 9/16\n",
      "3762662/3762662 - 367s - loss: 5.1291 - mse: 317.7024 - val_loss: 5.1294 - val_mse: 322.7847\n",
      "Epoch 10/16\n",
      "3762662/3762662 - 360s - loss: 5.1211 - mse: 317.1236 - val_loss: 5.1224 - val_mse: 321.9101\n",
      "Epoch 11/16\n",
      "3762662/3762662 - 354s - loss: 5.1142 - mse: 316.7748 - val_loss: 5.1169 - val_mse: 321.7045\n",
      "Epoch 12/16\n",
      "3762662/3762662 - 354s - loss: 5.1078 - mse: 316.4736 - val_loss: 5.1101 - val_mse: 321.4796\n",
      "Epoch 13/16\n",
      "3762662/3762662 - 362s - loss: 5.1017 - mse: 316.0729 - val_loss: 5.1038 - val_mse: 321.2260\n",
      "Epoch 14/16\n",
      "3762662/3762662 - 382s - loss: 5.0963 - mse: 315.7619 - val_loss: 5.0981 - val_mse: 320.4265\n",
      "Epoch 15/16\n",
      "3762662/3762662 - 438s - loss: 5.0912 - mse: 315.5140 - val_loss: 5.0931 - val_mse: 320.1918\n",
      "Epoch 16/16\n",
      "3762662/3762662 - 345s - loss: 5.0864 - mse: 315.2652 - val_loss: 5.0883 - val_mse: 320.0682\n"
     ]
    }
   ],
   "source": [
    "train_model(6, 7, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
